@inproceedings{oldfield:lwfs,
        Abstract = {Today?s high-end massively parallel processing (MPP) machines have
        thousands to tens of thousands of processors, with next-generation
        systems planned to have in excess of one hundred thousand processors.
        For systems of such scale, efficient I/O is a significant challenge
        that cannot be solved using traditional approaches. In particular,
        general purpose parallel file systems that limit applications to
        standard interfaces and access policies do not scale and will likely
        be a performance bottleneck for many scientific applications.

        In this paper, we investigate the use of a ?lightweight? approach
        to I/O that requires the application or I/O-library developer to
        extend a core set of critical I/O functionality with the minimum
        set of features and services required by its target applications.
        We argue that this approach allows the development of I/O libraries
        that are both scalable and secure. We support our claims with preliminary
        results for a lightweight checkpoint operation on a development cluster
        at Sandia.},
        Address = {Barcelona, Spain},
        Author = {Ron A. Oldfield and Arthur B. Maccabe and Sarala Arunagiri and Todd Kordenbrock and Rolf Riesen and Lee Ward and Patrick Widener},
        Booktitle = cluster2006,
        Comment = {Also see extended version raoldfi:lwfs-tr.},
        Date-Modified = {2011-03-31 11:35:20 -0600},
        Doi = {10.1109/CLUSTR.2006.311853},
        File = {SAND2006-3057.pdf:http\://gaston.sandia.gov/cfupload/ccim_pubs_prod/SAND2006-3057.pdf:PDF},
        Institution = {Sandia National Laboratories},
        Keywords = {lightweight storage, checkpoint, scalable-io, LWFS, pario-bib},
        Month = sep,
        Owner = {raoldfi},
        Timestamp = {2006.05.15},
        Title = {Lightweight {I/O} for Scientific Applications},
        Url = {http://doi.ieeecomputersociety.org/10.1109/CLUSTR.2006.311853},
        Vitatype = {refConference},
        Year = {2006},
        Bdsk-Url-1 = {http://gaston.sandia.gov/cfupload/ccim_pubs_prod/SAND2006-3057.pdf}}

@inproceedings{lofstead:2009:adaptible,
        Address = {Rome, Italy},
        Author = {Jay Lofstead and Fang Zheng and Scott Klasky and Karsten Schwan},
        Booktitle = ipdps2009,
        Date-Added = {2009-10-14 13:52:24 -0600},
        Date-Modified = {2009-10-14 13:55:04 -0600},
        Keywords = {application programmer interface, pario-bib},
        Title = {Adaptable, metadata rich {IO} methods for portable high performance {IO}},
        Year = {2009}}

@inproceedings{lofstead:2012:txn,
 author= {Jay Lofstead and Jai Dayal and Karsten Schwan and Ron Oldfield},
 title = {D2T: Doubly Distributed Transactions for High Performance and Distributed Computing},
 abstract = {Current exascale computing projections suggest rather than a monolithic simulation running for the majority of the machine, a collection of components comprising the scientific discovery process will be employed in an online workflow. This move to an online workflow scenario requires knowledge that inter-step operations are completed and correct before the next phase begins. Further, dynamic load balancing or fault tolerance techniques may dynamically deploy or redeploy resources for optimal use of computing resources. These newly configured resources should only be used if they are successfully deployed.
Our D2T system offers a mechanism to support these kinds of operations by providing database-like transactions with distributed servers and clients. Ultimately, with adequate hardware support, full ACID compliance is possible for the transactions. To prove the viability of this approach, we show that the D2T protocol has less than 1.2 seconds of overhead using 4096 clients and 32 servers with good scaling characteristics using this initial prototype implementation.},
 year = {2012},
 month = {September},
 booktitle = {IEEE Cluster Conference},
 address = {Beijing, China},
}

@inproceedings{patil:2007:giga+,
  author    = {Swapnil Patil and
               Garth A. Gibson and
               Samuel Lang and
               Milo Polte},
  title     = {GIGA+: scalable directories for shared file systems},
  booktitle = {PDSW},
  year      = {2007},
  pages     = {26-29},
  ee        = {http://doi.acm.org/10.1145/1374596.1374604},
  crossref  = {DBLP:conf/sc/2007pdsw},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{carns:pvfs,
        Abstract = {As Linux clusters have matured as platforms for low-cost, high-performance
        parallel computing, software packages to provide many key services
        have emerged, especially in areas such as message passing and networking.
        One area devoid of support, however, has been parallel file systems,
        which are critical for high-performance I/O on such clusters. We
        have developed a parallel file system for Linux clusters, called
        the Parallel Virtual File System (PVFS). PVFS is intended both as
        a high-performance parallel file system that anyone can download
        and use and as a tool for pursuing further research in parallel I/O
        and parallel file systems for Linux clusters. \par In this paper,
        we describe the design and implementation of PVFS and present performance
        results on the Chiba City cluster at Argonne. We provide performance
        results for a workload of concurrent reads and writes for various
        numbers of compute nodes, I/O nodes, and I/O request sizes. We also
        present performance results for MPI-IO on PVFS, both for a concurrent
        read/write workload and for the BTIO benchmark. We compare the I/O
        performance when using a Myrinet network versus a fast-ethernet network
        for I/O-related communication in PVFS. We obtained read and write
        bandwidths as high as 700~Mbytes/sec with Myrinet and 225~Mbytes/sec
        with fast ethernet.},
        Address = {Atlanta, GA},
        Author = {Philip H. Carns and Walter B. {Ligon III} and Robert B. Ross and Rajeev Thakur},
        Booktitle = {Proceedings of the 4th Annual Linux Showcase and Conference},
        Comment = {won the Best Paper Award.},
        Keywords = {parallel I/O, parallel file system, cluster file system, Linux, pario-bib},
        Month = oct,
        Pages = {317--327},
        Publisher = {USENIX Association},
        Title = {{PVFS}: A Parallel File System for Linux Clusters},
        Toread = {5},
        Url = {http://www.mcs.anl.gov/~thakur/papers/pvfs.ps},
        Year = {2000},
        Bdsk-Url-1 = {http://www.mcs.anl.gov/~thakur/papers/pvfs.ps}}

@inproceedings{weil:ceph,
        Abstract = {We have developed Ceph, a distributed file system that provides excellent
        performance, reliability, and scalability. Ceph maximizes the separation
        between data and metadata management by replacing allocation tables
        with a pseudo-random data distribution function (CRUSH) designed
        for heterogeneous and dynamic clusters of unreliable object storage
        devices (OSDs). We leverage device intelligence by distributing data
        replication, failure detection and recovery to semi-autonomous OSDs
        running a specialized local object file system. A dynamic distributed
        metadata cluster provides extremely efficient metadata management
        and seamlessly adapts to a wide range of general purpose and scientific
        computing file system workloads. Performance measurements under a
        variety of workloads show that Ceph has excellent I/O performance
        and scalable metadata management, supporting more than 250,000 metadata
        operations per second.},
        Author = {Sage A. Weil and Scott A. Brandt and Ethan L. Miller and Darrell D. E. Long and Carlos Maltzahn},
        Booktitle = osdi2006,
        Date-Modified = {2009-01-06 17:33:31 -0700},
        Keywords = {distributed file systems, metadata scaling, object storage, distributed metadata, pario-bib},
        Organization = {University of California, Santa Cruz},
        Owner = {raoldfi},
        Pages = {307--320},
        Timestamp = {2007.03.27},
        Title = {Ceph: A Scalable, High-Performance Distributed File System},
        Url = {http://www.usenix.org/events/osdi06/tech/weil.html},
        Year = {2006},
        Bdsk-Url-1 = {http://www.usenix.org/events/osdi06/tech/weil.html}}

@inproceedings{lofstead:2010:io-variability,
        Author = {Jay Lofstead and Fang Zheng and Qing Liu and Scott Klasky and Ron Oldfield and Todd Kordenbrock and Karsten Schwan and Matthew Wolf},
        Booktitle = sc2010,
        Date-Added = {2010-06-21 17:16:42 -0600},
        Date-Modified = {2010-06-21 17:21:14 -0600},
        Keywords = {variability studies, pario-bib},
        Month = nov,
        Title = {Managing Variability in the {IO} Performance of Petascale Storage Systems},
        Year = {2010}}

@INPROCEEDINGS{zheng:2010:predata,
    author = {Fang Zheng and Hasan Abbasi and Ciprian Docan and Jay Lofstead and Scott Klasky and Qing Liu and Manish Parashar and Norbert Podhorszki and Karsten Schwan and Matthew Wolf},
    title = {{PreDatA }- Preparatory Data Analytics on {Peta-Scale} Machines},
    abstract = {Peta-scale scientific applications running on High
End Computing (HEC) platforms can generate large volumes
of data. For high performance storage and in order to be
useful to science end users, such data must be organized in
its layout, indexed, sorted, and otherwise manipulated for subsequent
data presentation, visualization, and detailed analysis.
In addition, scientists desire to gain insights into selected data
characteristics `hidden' or `latent' in these massive datasets
while data is being produced by simulations. PreDatA, short for
Preparatory Data Analytics, is an approach to preparing and
characterizing data while it is being produced by the large scale
simulations running on peta-scale machines. By dedicating
additional compute nodes on the machine as `staging' nodes
and by staging simulations' output data through these nodes,
PreDatA can exploit their computational power to perform
select data manipulations with lower latency than attainable
by first moving data into file systems and storage. Such intransit
manipulations are supported by the PreDatA middleware
through asynchronous data movement to reduce write
latency, application-specific operations on streaming data that
are able to discover latent data characteristics, and appropriate
data reorganization and metadata annotation to speed up
subsequent data access. PreDatA enhances the scalability and
exibility of the current I/O stack on HEC platforms and
is useful for data pre-processing, runtime data analysis and
inspection, as well as for data exchange between concurrently
running simulations.},
    booktitle = {In Proceedings of 24th IEEE International Parallel and Distributed Processing Symposium, April, Atlanta, Georgia},
    year = {2010}
}

@article{docan:2010:dataspaces,
author = {C. Docan and M. Parashar and S. Klasky},
title = {{DataSpaces}: An Interaction and Coordination Framework for Coupled Simulation Workflows},
abstract = {Emerging high-performance distributed computing environ-
ments are enabling new end-to-end formulations in science
and engineering that involve multiple interacting processes
and data-intensive application workflows. For example, cur-
rent fusion simulation efforts are exploring coupled models
and codes that simultaneously simulate separate application
processes, such as the core and the edge turbulence, and run
on different high performance computing resources. These
components need to interact, at runtime, with each other
and with services for data monitoring, data analysis and vi-
sualization, and data archiving. As a result, they require effi-
cient support for dynamic and flexible couplings and interac-
tions, which remains a challenge. This paper presents Data-
Spaces, a flexible interaction and coordination substrate that
addresses this challenge. DataSpaces essentially implements
a semantically specialized virtual shared space abstraction
that can be associatively accessed by all components and
services in the application workflow. It enables live data
to be extracted from running simulation components, in-
dexes this data online, and then allows it to be monitored,
queried and accessed by other components and services via
the space using semantically meaningful operators. The un-
derlying data transport is asynchronous, low-overhead and
largely memory-to-memory. The design, implementation,
and experimental evaluation of DataSpaces using a coupled
fusion simulation workflow is presented.},
journal = {HPDC '10: Proceedings of the 18th international symposium on High performance distributed computing},
year = {2010},
address = {Chicago, IL, USA},
}

@inproceedings{oldfield:2006:nessie,
Address = {Barcelona, Spain},
Author = {Ron A. Oldfield and Patrick Widener and Arthur B. Maccabe and Lee Ward and Todd Kordenbrock},
Booktitle = hiperio2006,
Month = sep,
Title = {Efficient Data-Movement for Lightweight {I/O}},
Year = {2006}
}

@inproceedings{Dayal:2014:flexpath,
 author = {Jai Dayal, Drew Bratcher, Hasan Abbasi, Greg Eisenhauer, Scott Klasky, Norbert Podhorszki, Karsten Schwan, Matthew Wolf},
 title = {{Flexpath: Type-Based Publish/Subscribe System for Large-scale Science Analytics}},
 booktitle = {Cluster, Cloud, and Grid},
 series = {CCGrid '14},
 year = {2014},
 location = {Chicago, Illinois},
 numpages = {10},
 publisher = {IEEE},
 keywords = {I/O, Publish Subscribe, code coupling, data redistribution, workflows}
}

@inproceedings{thapaliya:2014:io-cop,
  author    = {Sagar Thapaliya and
               Purushotham Bangalore and
               Jay F. Lofstead and
               Kathryn Mohror and
               Adam Moody},
  title     = {IO-Cop: Managing Concurrent Accesses to Shared Parallel File System},
  booktitle = {43rd International Conference on Parallel Processing Workshops, {ICPPW}
               2014, Minneapolis, MN, USA, September 9-12, 2014},
  pages     = {52--60},
  year      = {2014},
  crossref  = {DBLP:conf/icppw/2014},
  url       = {http://dx.doi.org/10.1109/ICPPW.2014.20},
  doi       = {10.1109/ICPPW.2014.20},
  timestamp = {Fri, 15 May 2015 15:29:43 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icppw/ThapaliyaBLMM14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/icppw/2014,
  title     = {43rd International Conference on Parallel Processing Workshops, {ICPPW}
               2014, Minneapolis, MN, USA, September 9-12, 2014},
  publisher = {{IEEE} Computer Society},
  year      = {2014},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7101393},
  isbn      = {978-1-4799-5615-9},
  timestamp = {Fri, 15 May 2015 15:24:43 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icppw/2014},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{dorier:2014:calciom,
  author    = {Matthieu Dorier and
               Gabriel Antoniu and
               Robert B. Ross and
               Dries Kimpe and
               Shadi Ibrahim},
  title     = {CALCioM: Mitigating {I/O} Interference in {HPC} Systems through Cross-Application
               Coordination},
  booktitle = {2014 {IEEE} 28th International Parallel and Distributed Processing
               Symposium, Phoenix, AZ, USA, May 19-23, 2014},
  pages     = {155--164},
  year      = {2014},
  crossref  = {DBLP:conf/ipps/2014},
  url       = {http://dx.doi.org/10.1109/IPDPS.2014.27},
  doi       = {10.1109/IPDPS.2014.27},
  timestamp = {Mon, 15 Sep 2014 08:47:01 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ipps/DorierARKI14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/ipps/2014,
  title     = {2014 {IEEE} 28th International Parallel and Distributed Processing
               Symposium, Phoenix, AZ, USA, May 19-23, 2014},
  publisher = {{IEEE}},
  year      = {2014},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6875427},
  isbn      = {978-1-4799-3799-8},
  timestamp = {Mon, 15 Sep 2014 08:29:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/ipps/2014},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Fitzpatrick:2004:memcached,
 author = {Fitzpatrick, Brad},
 title = {Distributed Caching with Memcached},
 journal = {Linux J.},
 issue_date = {August 2004},
 volume = {2004},
 number = {124},
 month = aug,
 year = {2004},
 issn = {1075-3583},
 pages = {5--},
 url = {http://dl.acm.org/citation.cfm?id=1012889.1012894},
 acmid = {1012894},
 publisher = {Belltown Media},
 address = {Houston, TX},
} 

@inproceedings{blaze:1992:hsm,
  author    = {Matt Blaze and
               Rafael Alonso},
  title     = {Dynamic Hierarchical Caching for Large-Scale Distributed File Systems},
  booktitle = {Proceedings of the 12th International Conference on Distributed Computing
               Systems, Yokohama, Japan, June 9-12, 1992},
  pages     = {521--528},
  year      = {1992},
  crossref  = {DBLP:conf/icdcs/1992},
  url       = {http://dx.doi.org/10.1109/ICDCS.1992.235001},
  doi       = {10.1109/ICDCS.1992.235001},
  timestamp = {Wed, 27 Nov 2013 16:54:39 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icdcs/BlazeA92},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@proceedings{DBLP:conf/icdcs/1992,
  title     = {Proceedings of the 12th International Conference on Distributed Computing
               Systems, Yokohama, Japan, June 9-12, 1992},
  publisher = {{IEEE} Computer Society},
  year      = {1992},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=419},
  isbn      = {0-8186-2865-0},
  timestamp = {Wed, 27 Nov 2013 16:54:39 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/icdcs/1992},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@techreport{sirocco,
author = {Matthew L. Curry and Lee Ward and Geoff Danielson},
title = {Motivation and Design of the Sirocco Storage System, version 1.0},
year = {2015},
institution = {Sandia National Laboratories},
address = {Albuquerque, New Mexico},
note = {\url{http://www.cs.sandia.gov/Scalable_IO/sirocco}},
}

