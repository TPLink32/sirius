\documentclass{sig-alt-gov2}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% \usepackage{subfig}
\usepackage{subfigure}
\usepackage[pdftex]{}
\usepackage{graphicx}
% \usepackage{todonotes}
\usepackage{listings}
\usepackage{comment}
\lstset {% general command to set parameter(s)
  language=C,
  basicstyle=\footnotesize,               % print whole listing small
  % keywordstyle=\color{black}\bfseries, % underlined bold black keywords
  % identifierstyle =\color{black},  % nothing happens
  % commentstyle=\color{black}\emph, % white comments
  % stringstyle=\ttfamily,          % typewriter type for strings
  % stringstyle=\color{black},       % typewriter type for strings
  tabsize=4,
  showtabs=false,
  showstringspaces=false}
% can't figure this one out for particles bandwidth
% \usepackage[caption,label]{subfig}

% \newcommand{\TITLE}{D\textsuperscript{\huge{2}}T: Doubly Distributed Transactions for High Performance and Distributed Computing}

\hyphenation{sub-trans-ac-tion}

\addtolength{\parskip}{-0.08in}

% in order for balance columns to work, it has to be before begin document...
% \balancecolumns

\begin{document}

% \conferenceinfo{PDSW'15,} {November 17, 2015, Austin, Texas}
% \CopyrightYear{2015}
% \crdata{978-1-4503-1103-8/11/11}
% \clubpenalty=10000
% \widowpenalty = 10000

\title{Restructuring Data for Exascale Scientific Productivity}

\numberofauthors{1}
\author{
  \alignauthor {
    Jay Lofstead\footnotemark[1]~~,
    Hasan Abbasi\footnotemark[3]~~,
    Mark Ainsworth\footnotemark[6]~\footnotemark[3]~~,
    Jong Choi\footnotemark[3]~~,
    Matthew Curry\footnotemark[1]~~,\\
    Scott Klasky\footnotemark[3]~\footnotemark[4]~\footnotemark[7]~~,
    Tahsin Kurc\footnotemark[9]~~,
    Qing Liu\footnotemark[3]~~,
    Jeremy Logan\footnotemark[4]~~,
    Carlos Maltzahn\footnotemark[2]~~,
    Manish Parashar\footnotemark[5]~~,\\
    Norbert Podhorzski\footnotemark[3]~~,
    Feiyi Wang\footnotemark[3]~~,
    Matthew Wolf\footnotemark[7]~~,
    C. S. Chang\footnotemark[8]~~,
    Michael Churchill\footnotemark[8]~~,
    Stephane Ethier\footnotemark[8]~~
  }\\
  \footnotemark[1]~~Sandia National Laboratories,
  \footnotemark[2]~~University of California Santa Cruz,\\
  \footnotemark[3]~~Oak Ridge National Laboratory,
  \footnotemark[4]~~U. Tenn. Knoxville,
  \footnotemark[5]~~Rutgers University,\\
  \footnotemark[6]~~Brown University,
  \footnotemark[7]~~Georgia Tech,
  \footnotemark[8]~~PPPL,
  \footnotemark[9]~~Stony Brook
}
\maketitle

\begin{abstract}

  Big data and the associated changes in the computing platform are key
  components for the advances from exascale systems for scientific discovery
  with computing. Data will increasingly drive the next generation of scientific
  discoveries, but limitations in the rate of progress for data storage and
  movement technologies is likely to hamper this revolution. Storage performance
  and capacity bottlenecks have the potential to greatly limit how much data can
  be output from exascale simulations, how often this data can be output, and
  how much knowledge can be derived from the data.

  New techniques in storage and I/O hardware is one direction towards addressing
  these issues. In this paper we present an alternate method for addressing the
  data deluge, by utilizing domain specific knowledge about the data to manage
  the organization, content and precision of the output data. The tradeoff
  between system level quality of service and domain specific quality of
  information can be used to circumvent the storage bottlenecks without
  exploding the cost or energy usage of storage systems.

  In this paper we present a lossless data partitioning technique that
  intelligently utilizes hierarchical storage in next-generation leadership
  computing systems. We show that lower accuracy partitions can be used to
  service the need for most common use cases, and additional accuracy can be
  added for more demanding scenarios. The performance benefits are .....

  
\end{abstract}

% \category{D.4}{Software}{Operating Systems}
% \category{D.4.7}{Operating Systems}{Organization and Design}[hierarchical design]

% \terms{Design, Performance}

\section{Introduction}


\begin{comment}
  As the exascale computing age emerges, data related issues are becoming
  critical factors that determine how and where we do computing. Popular
  approaches used by traditional I/O solution and storage libraries become
  increasingly bottlenecked due to their assumptions about data movement,
  re-organization, and storage. While, new technologies, such as ``burst
  buffers'', can help address some of the short-term performance issues, it is
  essential that we reexamine the underlying storage and I/O infrastructure to
  effectively support requirements and challenges at exascale and beyond.

  In this paper we present
  - different data needs different techniques
  - write overhead for the different refactoring/auditing approaches
  - derived quantities in viz after precision based refactoring and differences
  from full fidelity data sets.
  - linear auditing approaches for data refactoring and the effects on data
  sizes, io performance, and viz quality. Do every point, but split at 24 bits
  and 40 bits.
\end{comment}

\begin{comment}
  Be sure to address the following:\\
  1. Why should Chang or Chen care from the apps side.\\
  2. why should ken or hank care from the viz side\\
  3. why would Garth care from the storage side\\
\end{comment}

Big data and the associated changes in the computing platform are key
components for the advances from exascale systems for scientific discovery
with computing. Data will increasingly drive the next generation of scientific
discoveries, but limitations in the rate of progress for data storage and
movement technologies is likely to hamper this revolution. Storage performance
and capacity bottlenecks have the potential to greatly limit how much data can
be output from exascale simulations, how often this data can be output, and
how much knowledge can be derived from the data.

New techniques in storage and I/O hardware is one direction towards addressing
these issues. In this paper we present an alternate method for addressing the
data deluge, by utilizing domain specific knowledge about the data to manage
the organization, content and precision of the output data. The tradeoff
between system level quality of service and domain specific quality of
information can be used to circumvent the storage bottlenecks without
exploding the cost or energy usage of storage systems.


We have undertaken this effort as part of a larger rethinking of the role of
storage in high performance computing applications, the SIRIUS project. In
this paper we demonstrate the initial efforts towards creating precision
aware I/O techniques for different types of large scale data sets. Our
experiments show that selecting for output accuracy based on the intended
usage for data can yeild substantial performance benefits. Moreover, we show
that lossless data partitioning techniques can be utilize in lieu of lossy
compression methods to utilize the hierarchical storage structure of
next-generation leadership computing platforms. By partitioning data into
buckets of varying precision, many usecases can be met with smaller, less
accurate buckets. For other use cases where only the highest precision will
suffice, the additional cost can be paid as and when needed.


% \begin{figure}[htbp]
%   \vspace{-0.10in}
%   \centering
%   \includegraphics[width=\columnwidth]{images/arch-mapping}
%   \vspace{-0.20in}
%   \caption{Architecture and Component Mapping}
%   \label{fig:arch-mapping}
%   \vspace{-0.15in}
% \end{figure}

Contributions:\\
\begin{enumerate}
\item XGC1 Byte-split data (3/5)
  \begin{enumerate}
  \item decomposition/compression
  \item auditor
  \item data challenges preventing arbitrary use
  \end{enumerate}
\item XGC1/S3D Data Interpolation (timestep to timestep)
\end{enumerate}

Engineering:\\
\begin{enumerate}
\item ADIOS write transports
\item VisIt auditor reader
\item interfaces
\end{enumerate}

validation/experiments (XGC1 only for now, 1 \&1 2 buckets, all other factors):\\
\begin{enumerate}
\item Performance writing (A orig, B 2 buckets)
\item Performance reading (A orig, B 2 buckets, C bucket 1, D, bucket 2)
\item Visualization diff (3-d)
\item numeric evaluation diff (to get past criticism of 3-d hiding errors)
\item derived quantity
\item interpolation validation
\end{enumerate}

target: Lustre, DataSpaces\\
nodes: 10, 100, 1000 (Sith 35 compute + 5 storage)\\
data/node: 10M, 1G\\
vars: 10, 100\\
per var: 1, 2 buckets (4 if we have time)\\
timesteps: 2, 4, 8, 16\\
time between timesteps: 10s, 100s\\

XGC1:\\
1d array format\\
nparticles (N x 8)\\
fields (nv) (3-5 vars)\\

Pixie3D/S3D:\\
3d array: (1k x 1k x 1k)\\

Use ITER mesh input for XGC1

Viz diff

\{reduced, full, diff\} images, numerical eval of diff\\

Responsibilities:
\begin{itemize}
\item Jay - write draft/drive paper
\item everyone - experiment runs
\item Ivan - DataSpaces help
\item Tahsin/Jong - Use real XGC1 data in test harness
\item Tahsin/Jong - Big run for byte split example
\item Tahsin/Jong - small run for interpolation example
\item Hadan - Real S3D run
\item Hasan/Norbert - ADIOS writer
\item James/Dave - ADIOS reader/viz
\item Hasan/Ben - ADIOS interpolation pieces
\end{itemize}

COMPLETED WORK:\\
1. XGC1 ADIOS write to VisIt read, 3/5, 4/4, 2/6 split (4/4 converts the 4 double bytes to a proper float). Output is written to disk and read is directly. DataSpaces to do still. all scalars are written to both files.\\
2. Can we do this to Integers too?\\
3. XGC1 interpolation uses every 4th element only and did 3/5 split\\
4. Hasan to talk with Jeremy about how to make the interpolation work properly.
5. Doing off the particles for the first pass. Checking to see if they can use the fields instead.\\
6. Also measure the diff with the viz and the numerical eval for the 3/5 split\\
7. Make it neutral in terms of tech used to make double blind possible. Add in
the rest of the details once accepted\\


The rest of the paper is organized as follows. A brief overview of related work
is presented first in Section~\ref{sec:related}. Section~\ref{sec:end-user}
discusses the programmatic interface end users will see when interacting with
the storage array.
Section~\ref{sec:iof}
briefly discusses the motivation and proposal for the IO forwarding layer.
Section~\ref{sec:iod} describes the IO Dispatcher layer and the broad
functionality it offers.
Section~\ref{sec:daos} discusses how the DAOS
layer functions.
The VOSD layer is discussed in Section~\ref{sec:vosd}.
Next is an exploration of cross-cutting features like transactions and
metadata management in Section~\ref{sec:broader}.
A demonstration of the functionality is presented in
Section~\ref{sec:evaluation}.
The paper is concluded in
Section~\ref{sec:conclusion} with a summary of the broad issues covered in the
paper.

\section{Related Work}
\label{sec:related}

Many projects over the last couple of decades have sought to address some
challenging aspect of parallel file system design. The recent rise of ``Big
Data'' applications with different characteristic IO patterns have somewhat
complicated the picture. Extreme scale machines will be expected to handle both
the traditional simulation-related workloads as well as applications more
squarely in the Big Data arena. This will require some adjustments to the
underlying system for good performance for both scenarios.

The major previous work is really limited to full file systems rather than the
mountain of file system refinements made over the years. A selection of these
other file systems and some features that make it relatively unique are
described below.

Ceph~\cite{weil:ceph} is a distributed object store and file system. It offers
both a POSIX and object interface including features typically found in parallel
file systems.
Ceph's unique striping approach uses pseudo-random numbers with a
known seed eliminating the need for the metadata service to track where each
stripe in a parallel file is placed.
PVFS~\cite{carns:pvfs} offers optimizations to reduce metadata server load,
such as a single process opening a file and sharing the handle.
It has been
commercialized in recent years as OrangeFS.
Lustre~\cite{braam:lustre-arch} has become the de facto standard on most major
clusters offering scalable performance and fine-grained end-user and
programmatic control over how data is placed in the storage system.
GPFS~\cite{schmuck:gpfs} offers a hands-off approach for providing good
performance for scaling parallel IO tasks and is used extensively by its owner,
IBM.
Panasas~\cite{panasas:architecture} seeks to offer a dynamically adaptive
striping system that detects the need for additional stripes for performance
and adjusts the file layout as necessary.

Other file systems, like GoogleFS~\cite{ghemawat:googlefs} and
HDFS~\cite{Shvachko:2010:hdfs}, address distributed rather than parallel
computing and cannot be compared directly.
The primary difference between
distributed and parallel file systems is the ability of the file system to
store and retrieve data simultaneously from multiple clients, in parallel, and
treat the resulting collection of pieces as a single object.  Distributed file
systems rely on a single client creating a file, but distributing the set of
files across a wide array of storage devices.
The other, popular distributed
file system of note is NFS~\cite{powlowski:1994:nfs3} that has been used for
decades for enterprise file systems.
These other file systems are mainly of
interest in the context of the ACG features of FFSIO and will be discussed more
in Section~\ref{sec:acg}.

\section{Demonstration}
\label{sec:evaluation}


\section{Conclusions}
\label{sec:conclusion}

The Fast Forward Storage and IO Stack project has designed a good first pass at
addressing the requirements for an extreme scale data storage mechanism.  By
preferring a high level user API like HDF5 rather than using the POSIX
interface, more advanced functionality can be incorporated with less end-user
impact. The introduction of the IOD layer with buffering will absorb the
difference between the compute node IO demands and the available bandwidth in
the storage array. With DAOS supporting translating the container and object
model to the underlying storage options, different storage technologies can be
deployed over time.

With the overall stack design a prototype implementation complete, refinements,
such as fault detection and recovery, can be designed and tested.  These and
other activities for phase 2 will ultimately generate what is likely to be the
next generation storage stack for extreme scale platforms.

\section{Acknowledgements}

Sandia National Laboratories is a multi-program laboratory managed and operated
by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
Corporation, for the U.S. Department of Energy's National Nuclear Security
Administration under contract DE-AC04-94AL85000.

\bibliographystyle{abbrv}
\bibliography{paper}

\vfill\eject

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
