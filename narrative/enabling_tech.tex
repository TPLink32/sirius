\section{Enabling Technologies}
\label{sec:enabling-technologies}
Significant existing work will serve as the foundation upon which this project
will be performed.
%1
At the lowest level is the Lightweight File Systems ({\bf LWFS}) project~\cite{oldfield:lwfs}.
which had the  goal  to provide the simplest core services for
a storage system and having  added functionality  using auxiliary
services. 
%For example,
%by eliminating consistency checks from the core system, applications already
%managing consistency can eliminate the redundant check enhancing performance.
%
%The current LWFS project phase is investigating new ways for managing storage
%devices. This project, 
{\bf Sirocco} cam out of the LWFS project and offers a peer-to-peer style data storage device
mesh with data migrating based on use, required resilience, and available
resources. For example, when writing data initially into Sirocco, required
resilience characteristics are provided prompting Sirocco to ensure the data is
stored on appropriate storage device(s) that meet the resilience requirement.
When a particular device is full, Sirocco determines if it should migrate the
data to a new location or if it is a copy beyond the resilience requirement. If
this copy must still be protected, Sirocco will find a place of sufficient 
resilience to move the data to before deleting the local copy. Further, when
data is requested, a client can ask any Sirocco server and Sirocco will find
the data returning it to the client. If desired, the canonical data location
can be moved on client request to offer better performance for the client. In
additional features, a stored object can have multiple data forks that can
store different versions or aspects of the same data.  For this project, we
will build on this Sirocco foundation for data placement, migration, and the
version storage (via the fork mechanism). 

%The LWFS philosophy focuses on providing a universal core with additional
%semantics and services layered on top or run along side. For this project, the
%Sirocco API is much more complex than a standard POSIX call prompting layering
%a more user friendly interface on top. Placing a thin, optimized layer as part
%of the storage system will offer the kinds of functionality and semantics this
%project proposes.

  From the user perspective, something like the
{\bf ADIOS}~\cite{lofstead:2009:adaptible} API offers an API nearly as simple as POSIX, but with the
ability to change the actual data transport mechanism without changing the
source code. This affords writing to a single output API while shifting from
writing to a file system to operating on the data using in situ techniques, without
changing the source code. ADIOS has proven effective for writing to POSIX file
systems, data staging systems~\cite{zheng:2010:predata,docan:2010:dataspaces,oldfield:2006:nessie}, workflow
systems~\cite{Dayal:2014:flexpath}, and even through the WAN network 
Part of this project's goals seeks to understand where the dividing line
between the storage layer shared by all users and middleware being used by a
single or small number of users. We will utilize DataSpaces~\cite{docan:2010:dataspaces} to 
 further integrate the I/O (ADIOS) layer with the storage layer (Sirocco).
%
% 
%They offer a way to request data set portions
%from a one component by the next componet in a data flow. In DataSpaces case,
%the data transfer does not have to correspond to the data from a single
%process.  Instead, DataSpaces will determine where the data is and issue
%requests to pull the data from the source to the sink in an on demand process.

For this project, we plan to use the Sirocco data storage mechanism to manage
the actual data storage on all of the different tiers. We will add data management functionality as part of the storage
system layer to handle data discovery and metadata tasks. At the middleware
layer, we will investigate offering differing data compression, fidelity,
storage placement, and retrieval selection options.  Through this combination,
we will be able to deliver a complete end-to-end system. We will investigate
which functionality works best at the different layers for usability and
scalability. Furthermore we will  extend ADIOS~\cite{lofstead:2009:adaptible} with new APIs and novel
techniques to coordinate with the storage back-end. The key idea is offering
the ability to negotiate with the storage system to agree on the trade-off
between data quality and response time. 

We will also take advantage of the many innovations we discovered the Ceph~\cite{weil:osdi06} storage
system project. 
%
%  In the LWFS project, Sandia sought to demonstrate stripping
%down a file system to the bare minimum features required for any storage
%system. Then, additional functionality and semantics, such as POSIX-style
%directories and consistency requirements, were added in using auxiliary
%services separate from the stack or by layering providing a new interface to
%the storage system. The project's first phase demonstrated that using a core
%object storage system with integrated security providing authentication and
%authorization services, customized storage interfaces could be developed with
%exactly the overhead required for different applications. The second phase of
%LWFS, known as Sirocco~\cite{sirocco}, is investigating how to manage storage
%devices for efficient and scalable storage performance.  Sirocco has been in
%process for three years and has developed a usable storage infrastructure.
%
The Ceph project (ceph.com) started at UC Santa Cruz in 2004 and was initially
funded by DOE/NNSA involving LLNL, LANL, and Sandia, with the goal to create an
object-based parallel file system that addresses the well-known metadata
service bottleneck of creating and accessing 1000s of files comprising a
checkpoint~\cite{weil:osdi06}.  However, after the Ceph kernel client was
pulled into the Linux mainline in 2010, the focus of Ceph shifted to its
object-based storage subsystem, also called Reliable Autonomous Distributed
Object Store (RADOS), which now is the dominant storage system in OpenStack
production-level deployments and was recently selected as the new backend for
Flickr and Yahoo! Cloud Services. In this project we will leverage two of the
key innovations of Ceph: (1) Controlled Replication Under Scalable Hashing
(CRUSH)~\cite{weil:sc06} will help making resource identification and
predictable performance scalable, and (2) extensible storage object
classes~\cite{watkins:ucsctr15} will provide the main mechanisms of introducing
new APIs, metadata vocabulary, and quality of service into the storage stack.

