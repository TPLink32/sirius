\section{Introduction} 
\label{sec:introduction}

{\em The purpose of computing is insight, not numbers}~\cite{Hamming:book}.
Richard Hamming's remark, made over 50 years ago at the dawning of the age of
large scale scientific computation, is even more relevant today as we prepare
for scientific simulation at exascale. 
The central objective of this proposal, ``minimizing the time to knowledge'' from 
scientific computing, aligns perfectly with this sentiment. 
%Hamming was a pioneer in the fields of coding theory, scientific computing and 
%information theory who would instantly sympathize with the central objective of 
%this proposal--minimizing the time to knowledge.  

Avoiding impending bottlenecks in exascale scientific discovery will require
new research into managing and storing the large amounts of data that will be
produced by simulations and analyzed for months afterwards.
%
In this project we will demonstrate a cooperative approach for storing data
where the user and the storage system work together to achieve the best
possible utility and performance given data features, storage tier
characteristics, and current system state. We will investigate novel techniques
to facilitate efficient mapping of data objects, even partitioning individual
variables, from the user space onto multiple storage tiers and enable
application-guided data reductions/transformations to address capacity and
bandwidth bottlenecks, while constraining errors to be within user provided
bounds.
% 

We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes and expedite insights into critical
scientific processes, while demonstrating the validity of our approach in key
DOE domains. We will research techniques for creating and using a Scalable 
Storage Software Infrastructure, and integrating services from the middleware 
layer with Storage System capabilities to support novel strategies for distributing 
data both horizontally and vertically across the storage system.  Negotiation between
these layers will be provided as a fundamental service, allowing users to
easily save ``the best'' amount of data, which is automatically placed throughout the
entire storage stack.

The metric we are most interested in optimizing is time to knowledge.  Current
approaches to addressing the I/O bottleneck fall into two broad categories:
parallel file system approaches that optimize the throughput for an entire
system, and I/O middleware approaches that optimize the performance of a single
application. Both approaches have been successful to date, but are unlikely to
overcome the remaining obstacles in reaching exascale. 
Instead of trying to optimize throughput, we will seek to reduce the time to knowledge. 
This is the most significant metric for scientific applications where the desired outcome
is not storing data, but rather executing a knowledge creation and extraction processes 
on the data. 
Moreover, we aim to perform the optimization not for a single
application, but rather for workloads within a multi-user multi-application
system environment.

Our approach targets the expected characteristics of exascale storage hardware.
The storage layer will be partitioned into multiple heterogeneous tiers with
vastly different performance characteristics. The layers will be further
differentiated by the constraints on capacity and data lifetime within each
layer. Tape archival storage will still maintain data long term, but access to
this data will be orders of magnitude slower than the next layer and naively
accessing data from archival storage will greatly impact productivity.
%
Our approach in this proposal is to enable researchers to place more knowledge
into the SSIO system so that data can be recast from a sequence of bytes to
information, which the system can then place and optimize the time to gain
knowledge from exascale simulations.  We first motivate this by a problem our
team has been working on in the past six months, and then recast this problem
into a series of research questions and challenges which must be addressed in
order to optimize the SSIO system for exascale science.

The traditional approach to transforming results of scientific computation into
knowledge consists of storing the numerical output of a simulation on disk,
followed by a post-processing or analysis stage that might take place over
several ensuing weeks or months. Clearly, given the computation expected at the
exascale, this type of workflow is simply not viable.

%
\section{Scalable Storage Software Infrastructure}
Our focus in this proposal is in theme 2: Scalable Storage Software Infrastructure. We will focus on a state of the art
SSIO solution to solve problems which researchers are beginning to face and will make science on exascale systems
problematic without new insights and solutions.

Take for example the latest use case for the XGC1~\cite{chang2006integrated} simulation from C. S. Chang (PPPL), who is one of the largest users of Leadership Class Facilities (over 300M hours at ALCF, NERSC, and OLCF). They launched a series of simulations, planning to produce 100~PB of data over 10 days of total runtime on the Titan system. An expert team was assembled from the user group, the middleware I/O team to help with the application I/O (ADIOS), the local experts of the Lustre file-system and the tape archival system (HPSS). Due to physical resource limitations, the simulation was restricted to write about 10~PB initially. However, once the time to write and read, together with the financial cost to archive this much data were more fully explored, this plan was again significantly reduced. At each stage, the particular expert knowledge regarding the lustre file system and the archival tape storage were not known to the user team. As these particulars were realized, significant modifications in the plans were made. Eventually, the user was forced to make very hard decisions in reducing the desired 100~PB over a 10-day run to a mere 5~PB of data. A maximum of 2~PB would be on disk, and the rest would be archived.

This forced the team to carefully scrutinize the large amount of data to be generated and save only the very most important pieces. Further, they had to break this greatly reduced data into two categories: first data they thought they would need to access while the simulation was running or shortly thereafter, and second data they would need after the simulation was finished. The former category would be stored on disk, and the later would be archived. Additionally, they needed to figure out how to efficiently retrieve these different categories of data for post-processing, analysis and visualization.

Finally, the team had to develop new application specific data reduction techniques and encode them into the ADIOS middleware I/O layer. In order to do this, they had to implement a rudimentary discovery system to determine which data were located on disk, and which were on tape archive.  Additionally, some pieces of data needed to be moved to PPPL for immediate analysis.

This solution worked well for the first day of the run but after the second day significant problems occurred: I/O variability became prohibitively expensive. Although the team had reserved resources for the run, the file system was shared, and a 10X variability in I/O speeds was observed. What is needed for this problem is a system to monitor, predict and manage resource usage in a dynamically adaptable way. Such a system could write more data when the resource is idle, and less data when it is very busy.

However, the most significant problem occurs after the simulation run completes. Basic visualizations from the reducted data, just to get a high-level view of the simulation, can take significant time. While some timesteps might be on disk, others are archived on tape (where transfer could take weeks). Further, visualization and analysis of detailed views of the simulation are even more complicated.

The overarching idea of this proposal is that data from applications can be characterized by different
levels of importance and different levels of importance over time, which then can be used to guide the analysis and visualization later to access the most important data and deliver some level of knowledge without waiting for access to the whole original dataset. Information about the importance of the data at the application level, information about resource status and performance of the multi-level storage system, and flexible data reorganization and reduction operations at all levels can be utilized to provide a quality of service and allow users to gain insight in a timely manner.


To address these challenges we are proposing to combine knowledge
from a wide range of sources including ORNL applications, the ADIOS project,
the SNL Lightweight File Systems project and storage system knowledge, the metadata
management and storage systems knowledge of UCSC, and the middleware expertise
of Rutgers. We will integrate this diverse expertise to investigate how to make
a data, workflow, and application aware storage system.

%ADIOS was developed with the understanding that we must
%not only address the bottlenecks for current applications and
%hardware platforms but also provide a path forward for the next
%generation of applications and systems that would need to both
%maximize bandwidth to the storage system and also support
%transparently working around the storage system bandwidth
%limitations with new techniques and tools. To support the
%diverse operating modes of both using persistent storage and
%other data storage and processing technology, we made a
%great effort to provide a simplified interface to application
%developers, offering a simple, portable and scalable way for
%scientists to manage data that may need to be written, read
%or processed during simulation runs. This required abstracting
%away many decisions typically made in the application code
%so that they may be configured externally.
%In addition to this abstracted interface with external configuration
%options, common services were incorporated to afford
%optimizations beyond those for a single platform, such as
%buffering, aggregation, subfiling, and chunking with options
%to select each based on the data distribution characteristics
%of the application. A variety of asynchronous I/O techniques
%have been investigated and are being integrated with ADIOS.
%A recognition of application complexity has led to new techniques
%for testing I/O performance by extracting I/O patterns
%from applications and automatically generating benchmark
%codes. And finally, the march toward exascale has fueled the
%need to consider additional features such as compression and
%indexing to better cope with the expected tsunami of data.



We will examine the research challenges from the perspective of
%
(1) the user of the system who is attempting to run an exascale simulation,
%
(2) the Storage System and I/O layer which needs to negotiate between all of
the users, and finally
%
(3) the user of the system who is attempting to understand the data
produced by a set of simulations.

Our proposal will be responsive to theme 2 in the call, and we summarize them in the following table.
\begin{table}[htbp]
\vspace{2ex}
\begin{center}
\begin{tabular}{ | p {0.2cm} | p{5cm} | p{7.5cm} | p{1.6cm} |}\hline
\multicolumn{2}{|c|}{\bf Call area} & \multicolumn{1}{|c|}{\bf Relevant S2E2 work} & \multicolumn{1}{|c|}{\bf See} \\\hline\hline
%\multicolumn{2}{|c|}{Collaboration} & 
\multicolumn{2}{|p {5.5cm}|}{Approaches to improve the ability of SSIO software to support checkpoint/restart;} & 
General piece we are doing for this... & 
\S\ref{sec:?}, \S\ref{sec:??}, \\\hline
1 & Data abstractions & What will we do & \S\ref{sec:?}, \S\ref{sec:?}, \S\ref{sec:?} \\\hline
2 & Mapping science data models onto hierarchical storage & What we are doing & \S\ref{sec:?}, \S\ref{sec:?} \\\hline
3 & Mechanisms for data movement across the storage hierarchy & what are we doing. & \S\ref{sec:?} \\\hline
4 &  Extend the interaction of IO middleware with the resource management system & What we are doing & \S\ref{sec:?}, \S\ref{sec:?}  \\\hline
5 & Design and implement of IO middleware architectures  & What we are doing & \S\ref{sec:?}, \S\ref{sec:?}, \S\ref{sec:?} \\
\hline
\end{tabular}
\end{center}
\end{table}




\subsection{Writing Perspective}
\label{subsec:sim-perspective}
The simulation scientist ideally should be able to
obtain an estimate of how much time will be required to write their data,
and how much storage space they will be able to get during the lifetime of
their simulations. Furthermore, they would like the ability to get an assurance
of Quality of Service such that they can react appropriately when the
expected bandwidth, for example, is less than what they desire. Ideally, the user
would specify a set of rules with which the system can make autonomic
decisions to determine what should be done.
%

One of the key aspects of our approach is to allow the users to define the
utility of their data; in other-words, the system should provide a
mechanism such that, for each data object, a user can specify pieces of the object
which have more utility (more importance).
There are many research challenges faced with
this approach including: 1) How do we make this simple enough so that users can
do this, 2) Can we define a model to allow the system to make
autonomic decisions to save the data which has the highest utility, and 3) Can
we allow the back-end storage system to interact with this model so that
it can optimize not just a single application but across all of the
applications.
 %
  In our simple scenario, a user wants to write 1~PB of data every hour
for checkpoint/restart, and they wish to write 500~TB of data every 15
minutes to save data for analysis and visualization. At this stage, they will ask the system to
write the data, and they would like to get an estimated time from which they can
then figure out, through a series of rules, whether they want to write out all
of the data, postpone the write until a later time, or write a reduced amount
of data using in situ reduction techniques, along with some code
container that allows for regenerating the data with a specified level of
accuracy.  Furthermore, they realize that the analysis data will be saved on
either the parallel file system, long-term tape storage, or on the campaign storage,
which is a medium-term storage
area with lower bandwidth than the parallel file system.
For each piece of data, the user will specify the lifetime so
that the data with the highest utility is kept around in fast storage for the longest
period of time, while the data with the least utility may be the first to be migrated to
lower layers of the storage hierarchy. 

Since the data will be a collection of objects placed on the storage system,
some of the objects will be broken up into multiple pieces by ``plug-ins''
to the system which will allow data to be characterized first in terms of the most
important pieces, and then by subsequent levels of detail. We can think of this as
something similar to an Adaptive Mesh Refinement scheme which keeps track of
the places where there are features such as steep gradients, which contain large errors on the
coarser views, as opposed to other areas with smaller errors. These distinctions allow us to make decisions
about which sub-objects to save (all of the data, which will then go to
different layers) or just some of the sub-objects? The research questions
are numerous: How does the user specify their intentions? How does the user
prioritize the different sub-objects of the data? Where does the data go
into the storage hierarchy? Does the data get replicated in the lower tiers
of the hierarchy? When the user specifies the time they want the data saved,
how does this get into the storage system?

\subsection{Storage System Perspective}
\label{subsec:storage-perspective}
From the perspective of the storage system, it is necessary to manage the data from
a given user request amongst all of the requests from all users, and try to optimize the
entire system accordingly. Today the problem is that when a user writes there is a
tremendous amount of interference from other users on the system who may be
writing or reading at the same time. By requiring a certain quality
of service the {\bf  system must potentially lock out users with lower currency
they want to place in the reading or writing of their data}. The storage
system must also automatically move data from one tier of the storage to
another, without affecting the quality of service that was offered at request
time. The storage system must be able to organize
data amongst all of the tiers of storage in a known, consistent way.

One of the main research objectives of this project is to understand how the
storage system can interact with the middleware (which is application-aware)  so
that the data with the highest utility is kept on the fastest layers of the
storage system for as long as specified. We need to maintain 
QoS and therefore, be able to provide fuzzy estimates of the time it will take to write/read data from
the different layers to inform any migration/eviction decisions, an it
is also critical that the storage system can manage (discover and retrieve) the data across all of the
layers automatically, without user intervention. We will use learning techniques to automatically migrate data
sub-objects across the hierarchy as well as to eventually evict data which is
large and has very low utility.


\subsection{Reading  Perspective}
\label{subsec:reading-perspective}
From the perspective of the users who want to read in the data from their
simulations, and then operate on this data and possibly write data, we see
that in general these users will be running on much less compute resources
than the users who are running the simulation. One of the important aspects
of this class of user is that they want a reduction of latency and they
require a certain amount of quality of service as well. When someone is
doing interactive analysis and expects that the data they read in is about
10s, but it turns into 1,000s, the user typically tends to either wait
another day for doing their analysis or gets frustrated and reads much less
data than they really need to. Furthermore the user tends to read in just a
small sample of the data without much knowledge if they are missing
important pieces of the data.


%
%Exascale scientific discovery will be severely bottlenecked without
%sufficient new research into managing and storing the large amounts of data
%that will be produced during the simulation, and analyzed for months
%afterwards.
%%
%In this project we will demonstrate novel techniques to facilitate efficient
%mapping of data objects, even partitioning individual variables, from the
%user space onto multiple storage tiers, and enable application-guided data
%reductions/transformations to address capacity and bandwidth bottlenecks,
%while constraining the error to be within user provided bounds.
%%
%We will address the associated I/O and storage challenges in the context of
%current and emerging storage landscapes, and expedite insights into critical
%scientific processes, demonstrating the validity of our approach in key DOE
%domains. Our techniques will be to research novel techniques into a Scalable
%Storage Software Infrastructure by integrating services from the middle-ware
%layer which will talk to the applications, with the Storage system. The
%negotation between these layers will be a fundamental service which we will
%create in this project to ensure that users will be able to save `'the
%best'' amount of data in the different storage tiers.
%
%The metric we are most interested in optimizing is time to knowledge.
%Current approaches to addressing the I/O bottleneck fall into two broad
%categories. Parallel file system approaches that optimize the throughput for
%an entire system, and I/O middleware approaches that optimize the
%performance of a single application. Both approaches have seen success but
%are unlikely to overcome the major obstacles in reaching exascale. Instead
%of trying to optimize throughput, we will seek to reduce the time to
%knowledge. This is the most significant metric for scientific applications
%where the desired outcome is not storing data, but rather executing a
%knowledge extraction process on the data. Moreover, we aim to perform the
%optimization not for a single application, but rather for workload of a
%multi-user multi-application system environment.
%
%Our approach leverages the expected characteristics of exascale storage
%hardware. The storage layer will be partitioned into multiple heterogenous
%tiers with vastly different performance characteristics. This difference
%between layers will be further exacerbated by the constraints on capacity
%and data lifetime within a layer. Tape archival storage will still maintain
%data long term, but access to this data will be orders of magnitude slower
%than the next layer and naively accessing data from archival storage will
%greatly impact productivity.
%
%We will achieve reduced time to knowledge using a combination of techniques;
%\begin{enumerate}
%\item Data annotations specified at the application level to quantify the
%  relative important, utility and lifetime of data objects;
%\item Partitioning of data objects across the storage hierarchy utilizing
%  the additional knowledge embedded in the annotations;
%\item Evaluating the tradeoffs at runtime to guide data placement, movement,
%  and migration across storage layers using models, heuristics and
%  continuous learning;
%\item Utilize the additional knowledge available about the data to perform
%  application-aware data compression and I/O prioritization;
%\end{enumerate}
%
%We are aiming to spread an output across the vertical layers of the storage
%hierarchy simultaneously. When data should migrate to a higher tier, what
%happens to the existing version?
%
%Is there a canonical copy that is the originally written version?
%
%Is there annotation about this (I think so) attached to that part of the
%variable?
%
%At the tape layer, we see having a lossy compressed version just above the
%tape layer used as a directory/index and then the ``full'' resolution
%version on tape only retrieving once the user accepts the time/quality
%tradeoff. This is going to require some serious language to describe. If we
%have 100s PB of tape space, we'll need 1s PB of disk even at a 99\% data
%reduction. That is non-trivial.
%
%What happens when that data is retrieved from tape? Is a third copy made in
%an appropriately sized/quality level version for the tier it is requested to
%be pulled to? What if it is just the index version? Or the on tape version?
%Does it have a TTL because of the pain retrieving it from tape?
%
%Do we specify a target tier or just allow the system to place it in a place
%that makes sense? Given the potential space limitations, I think this is
%pretty critical because it could cause evictions or other insufficient space
%actions that are unintended. I can see us specifying that when you ask for
%data to be pulled at a particular quality level to a particular performance
%tier that it is a best effort with the use specifying if compromise or just
%failure is the result if the request cannot be fulfilled. Making this
%interaction make sense is going to require some reasonable thought and
%testing. We'll probably need to run it past apps people to get their
%feedback too as I don't think we are able to give a solid answer without
%some broad input.
%
%How does the migration to ``data lakes''/``campaign storage'' work? How does
%the migration to tape ultimately happen? I think we need to incorporate some
%explicit staging commands to move data both up and down the stack along with
%some variability in placement based on data features and current system
%state.
%
%Given the storage scarcity, particularly for NVM, I think we need to have a
%solid story here as part of the proposal. Yes, there are questions that have
%to be answered to build this still, but we need to have some pretty solid
%clarity where possible. I don't feel like I can explain it well enough at
%what I believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
