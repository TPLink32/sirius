\section{Introduction} 
\label{sec:introduction}

{\em The purpose of computing is insight, not numbers}~\cite{Hamming:book}.
Richard Hamming's remark, made over 50 years ago at the dawning of the age of
large scale scientific computation, is even more relevant today as we prepare
for scientific simulation at exascale. 
The central objective of this proposal, ``minimizing the time to knowledge''  
with respect to scientific computing, aligns perfectly with this sentiment. 
%Hamming was a pioneer in the fields of coding theory, scientific computing and 
%information theory who would instantly sympathize with the central objective of 
%this proposal--minimizing the time to knowledge.  

Avoiding impending bottlenecks in exascale scientific discovery will require
new research into managing, storing and retrieving the large volumes of data that are 
produced by simulations and analyzed for months afterwards.
%
In this project we will demonstrate a cooperative approach for storing and retrieving  data
where the user and the storage system work together to optimize performance with
respect to user requirements, current system state, and characteristics of the tiered-storage system.
Our investigation will include  novel techniques for the efficient mapping of data objects
from the user space across to a hierarchical tiered storage layer. These techniques will
include support for application-guided data reductions with user-defined error bounds
and transformations to optimize performance, capacity and minimize bandwidth bottlenecks.

% 
Our proposed research program is guided by the following overarching objectives:
\begin{itemize}
\item
{\em Address emerging storage architecture and technology landscape:}
We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes,
% and expedite insights into critical
%scientific processes, while demonstrating the validity of our approach in key
%DOE domains. 
%We will research techniques for creating and using a Scalable 
%Storage Software Infrastructure, and integrating services from the middleware 
%layer with Storage System capabilities to support 
and will develop novel strategies for distributing 
data both horizontally and vertically across the layers of these storage systems.  
Negotiation between these layers will be provided as a fundamental service, allowing users to
easily save ``the best''  data, which is automatically placed throughout the
entire storage stack.
\item
{\em Develop a science driven, system aware approach:}
%Our approach targets the expected characteristics of exascale storage hardware.
%The storage layer will be partitioned into multiple heterogeneous tiers with
%vastly different performance characteristics. The layers will be further
%differentiated by the constraints on capacity and data lifetime within each
%layer. Tape archival storage will still maintain data long term, but access to
%this data will be orders of magnitude slower than the next layer and naively
%accessing data from archival storage will greatly impact productivity.
%
Given that storage systems will be composed of multiple heterogeneous tiers with
vastly different performance characteristics and constraints on capacity and 
supported data lifetime, our approach will enable researchers to 
place more knowledge into the SSIO system so that data can be recast from 
a sequence of bytes to information rich data. This information rich data can then
be used by the system to guide the  placement within the tiered storage and will optimize the time
to gain knowledge and insight from exascale simulations.

%The traditional approach to transforming results of scientific computation into
%knowledge consists of storing the numerical output of a simulation on disk,
%followed by a post-processing or analysis stage that might take place over
%several ensuing weeks or months. Clearly, given the computation expected at the
%exascale, this type of workflow is simply not viable.

\item
{\em Emphasize impacts on science:}
%The metric we are most interested in optimizing is time to knowledge.  
%Current approaches to addressing the I/O bottleneck fall into two broad categories:
%parallel file system approaches that optimize the throughput for an entire
%system, and I/O middleware approaches that optimize the performance of a single
%application. Both approaches have been successful to date, but are unlikely to
%overcome the remaining obstacles in reaching exascale. 
Instead of trying to optimize throughput, we will seek to reduce the time to knowledge.
As extracting knowledge is the only metric that matters for scientific applications, we will
focus not only upon data storage, but upon efficient  retrieval of knowledge from their 
workflows.
Moreover, we aim to perform the optimization not only for a single
application, but rather for multiple workloads running on a multi-user, multi-application
system.
\end{itemize}

This proposal can be summarized as research into using the varying importance of simulation data over time to
ensure that the right portions of data, in the right quality  are available to scientists when they are needed.
Application-specific information about the importance of data will be used in conjunction with system and resource-specific information
to optimize  scientists experience in interacting with, and extracting knowledge and insight from their  data.

%The overarching idea of this proposal is that data from applications can be characterized by different
%%levels of importance and different 
%levels of importance over time, which then can be used to guide the analysis and visualization later to access the most important data and deliver some level of knowledge without waiting for access to the whole original dataset. Information about the importance of the data at the application level, information about resource status and performance of the multi-level storage system, and flexible data reorganization and reduction operations at all levels can be utilized to provide a quality of service and allow users to gain insight in a timely manner.

This proposal combines knowledge and expertise 
from a wide range of sources including many leadership class applications using the Adaptable Input/Output System (ADIOS) framework,
the Sandia National Laboratory (SNL) Lightweight File Systems (LWFS) project and storage system knowledge, the metadata
management and storage systems knowledge of UCSC, and the middleware expertise
of Rutgers. We will integrate this diverse expertise to investigate how to create
a data, workflow, and application aware storage and I/O system.
%This project brings together a team with strong expertise in I/O middleware (ORNL, Rutgers),
%file system (SNL, UCSC) and storage (UCSC), and connects and
%coordinates these key storage components in a seamless fashion.

%
%\section{Scalable Storage Software Infrastructure}
%Our focus in this proposal is in Theme 2: {\em Scalable Storage Software Infrastructure} 
%We will focus on a state of the art SSIO solution to solve problems which researchers are 
%beginning to face and will make science on exascale systems
%problematic without new insights and solutions.
Our proposal will be responsive to Theme 2: {\em Scalable Storage Software Infrastructure} of the call, as summarize in Table~\ref{tab:call}.

\begin{table}[htbp]
\small
\vspace{2ex}
\begin{center}
\begin{tabular}{ | p {0.2cm} | p{5cm} | p{7.5cm} | p{2.2cm} |}\hline
\multicolumn{2}{|c|}{\bf Call area} & \multicolumn{1}{|c|}{\bf Relevant S2E2 work} & \multicolumn{1}{|c|}{\bf See} \\\hline\hline
1 & Data abstractions & Create new APIs  and annotations for users to guide the system for placement  & \S\ref{sec:data-description} \\\hline
2 & Mapping science data models onto hierarchical storage &  Create the ability to re-organize, reduce and prioritize data chunks from an object& \S\ref{sec:data-refactor}, \S\ref{sec:data-description},  \S\ref{sec:managing-data-life}\\\hline
3 & Mechanisms for data movement across the storage hierarchy & what are we doing. & \S \ref{sec:managing-data-life},  \S\ref{sec:migration} \\\hline
4 &  Extend the interaction of IO middleware with the resource management system & We will manage data from the application  to the middleware to the storage system and allow them to have an autonomic interaction & \S\ref{sec:managing-data-life}, \S\ref{sec:qos}, \S\ref{sec:naming-discovery}  \\\hline
5 & Design and implement of IO middleware architectures  & We will create APIs to guide the placement of chunks of data across the SSIO and autonomically move data when necessary& \S\ref{sec:data-description}, \S\ref{sec:managing-data-life}, \S\ref{sec:qos}, \S\ref{sec:naming-discovery} \\ \hline
6 & Approaches to improve the ability of SSIO software to support checkpoint/restart & We will have utility functions defined for the lifetime of data, and allow CR data to be automatically placed in the fastest storage and then purged when it is no longer needed & \S \ref{sec:???}  \\
\hline
\end{tabular}
\end{center}
%\caption {WRITE A CAPTION FOR THIS TABLE}
\vskip -0.5cm
\caption {}
\label{tab:call}
\end{table}

%
\paragraph{An Illustrative Example}
The challenges addressed by this proposal are illustrated by a problem our team has been working on in the past six months. We use this motivating example to formulate a series of research questions and challenges that must be addressed in order to support exascale science.

The latest use case is from  the XGC1~\cite{chang2006integrated} simulation from C. S. Chang (PPPL), who is one of the largest users of Leadership Class Facilities (over 300M hours at DOE facilities). They launched a series of simulations, planning to produce 100~PB of data over 10 days of total runtime on the Titan system. An expert team was assembled from the user group, the  I/O, and storage  team to help with ensuring that the maximum amount of information would be
saved. Due to physical resource limitations, the simulation was restricted to write about 10~PB initially. However, once the time to write and read, together with the financial cost to archive this much data were fully explored, this plan was significantly reduced. At each stage, the particular expert knowledge regarding the file system and the archival storage were not known to the user team. As these particulars were realized, significant modifications in the plans were made. Eventually, they were forced to make very hard decisions in reducing the desired 100~PB over a 10-day run to only 5~PB of data. 

This forced the team to carefully scrutinize the large amount of data to be generated and save only the very most important pieces. Further, they had to break this greatly reduced data into two categories: first data they thought they would need to access while the simulation was running or shortly thereafter, and second data they would need after the whole campaign was finished. The former category would be stored on disk, and the later would be archived. Additionally, they needed to figure out how to efficiently retrieve these different categories of data for post-processing, analysis and visualization.

Finally, the team had to develop new application specific data reduction techniques and encode them into the ADIOS middleware I/O layer. In order to do this, they had to implement a rudimentary discovery system to determine which data were located on disk, and which were on tape archive.  

This solution worked well for the first day of the run but after the second day significant problems occurred: I/O variability became prohibitively expensive. Although the team had reserved resources for the run, the file system was shared, and a 10X variability in I/O speeds was observed. What is needed for this problem is a system to monitor, predict and manage resource usage in a dynamically adaptable way. Such a system could write more data when the resource is idle, and less data when it is very busy. However, the most significant problem occurs after the simulation run completes. Basic visualizations, just to get a high-level view of the simulation, can take significant time. While some timesteps might be on disk, others are archived on tape (where transfer could take weeks). Further, visualization and analysis of detailed views of the simulation are even more complicated.


\subsection{Overview of the proposed approach} 

%ADIOS was developed with the understanding that we must
%not only address the bottlenecks for current applications and
%hardware platforms but also provide a path forward for the next
%generation of applications and systems that would need to both
%maximize bandwidth to the storage system and also support
%transparently working around the storage system bandwidth
%limitations with new techniques and tools. To support the
%diverse operating modes of both using persistent storage and
%other data storage and processing technology, we made a
%great effort to provide a simplified interface to application
%developers, offering a simple, portable and scalable way for
%scientists to manage data that may need to be written, read
%or processed during simulation runs. This required abstracting
%away many decisions typically made in the application code
%so that they may be configured externally.
%In addition to this abstracted interface with external configuration
%options, common services were incorporated to afford
%optimizations beyond those for a single platform, such as
%buffering, aggregation, subfiling, and chunking with options
%to select each based on the data distribution characteristics
%of the application. A variety of asynchronous I/O techniques
%have been investigated and are being integrated with ADIOS.
%A recognition of application complexity has led to new techniques
%for testing I/O performance by extracting I/O patterns
%from applications and automatically generating benchmark
%codes. And finally, the march toward exascale has fueled the
%need to consider additional features such as compression and
%indexing to better cope with the expected tsunami of data.

We  examine the research challenges from the perspective of
%
(1) the scientist who runs the exascale simulation,
%
(2) the Storage System and I/O layer which needs to negotiate between all of
the users, and finally
%
(3) the user of the system who is attempting to understand the data
produced by a set of simulations.

\paragraph{Simulation Perspective}
\label{subsec:sim-perspective}
The simulation scientist ideally should be able to
obtain an estimate of how much time will be required to write the data,
and how much storage space is available during the simulations lifetime.
Further, they would like Quality of Service assurances so that reactions
to varying bandwidth can be handled appropriately. Ideally, a set of
user-specified rules would allow the system to make autonomic decisions dynamically.
%

Central to our approach is giving users the ability to define the importance, or utility of their data.
This includes the ability to assign varying levels of importance to individual data objects in their entirety, or specific
subsets and pieces.
Research challenges include 1) an expressive and easy to use interface, 2) a model for the system supporting
autonomic decision for saving data using the user-defined importance and utility of data, and 3) the interface between
the system and the back-end storage across both single, and multiple running applications.

 %
  In our  scenario we would like the scientist would then specify a set of rules that governed the actual behavior while the simulation is running.
If the system resources are available and the estimated write times are in order, the write can proceed.
Or, the write can be postponed to a later time-step, or data reduced via in situ techniques can be saved.
Data importance or utility will be used to place the various data on the proper SSIO layer.
The user specified lifetime for each data ensures that the data with the highest utility is preserved in fast storage
for the longest period of time, and the least utility are migrated to lower layers in the storage hierarchy.
%
%Since the data will be a collection of objects placed on the storage system,
%some of the objects will be broken up into multiple pieces by ``plug-ins''
%to the system which will allow data to be characterized first in terms of the most
%important pieces, and then by subsequent levels of detail. We can think of this as
%something similar to an Adaptive Mesh Refinement scheme which keeps track of
%the places where there are features such as steep gradients, which contain large errors on the
%coarser views, as opposed to other areas with smaller errors. These distinctions allow us to make decisions
%about which sub-objects to save (all of the data, which will then go to
%different layers) or just some of the sub-objects? The research questions
%are numerous: How does the user specify their intentions? How does the user
%prioritize the different sub-objects of the data? Where does the data go
%into the storage hierarchy? Does the data get replicated in the lower tiers
%of the hierarchy? When the user specifies the time they want the data saved,
%how does this get into the storage system?

\paragraph{Storage System Perspective}
\label{subsec:storage-perspective}
From the perspective of the storage system, it is necessary to manage the data from
a given user request amongst all of the requests from all users, and try to optimize the
entire system accordingly. Today the problem is that when a user writes there is a
tremendous amount of interference from other users on the system who may be
writing or reading at the same time, leading to unpredictable performance.
By requiring a certain quality of service the system could potentially lock out other users. 
The storage system must also automatically move data from one tier of the storage to
another, without affecting the quality of service that was offered at request
time. The storage system must be able to organize
data amongst all of the tiers of storage in a known, consistent way.

One of our  research objectives is to understand how the
storage system can interact with the middleware   so
that the data with the highest utility is kept on the fastest layers of the
storage system for as long as specified. We need to maintain 
QoS and therefore, be able to provide fuzzy estimates of the time it will take to write/read data from
the different layers to inform any migration/eviction decisions, an it
is also critical that the storage system can manage (discover and retrieve) the data across all of the
layers automatically, without user intervention. 
%We will use learning techniques to automatically migrate data
%sub-objects across the hierarchy as well as to eventually evict data which is
%large and has very low utility.


\paragraph{Reading  Perspective}
\label{subsec:reading-perspective}
From the perspective of the users who want to read in the data from their
simulations, and then operate on this data and possibly write data, we see
that in general these users will be running on much less compute resources
than the users who are running the simulation. One of the important aspects
of this class of user is that they want a reduction of latency and they
require a certain amount of quality of service as well. When someone is
doing interactive analysis and expects that the data they read in is about
10s, but it turns into 1,000s, the user typically tends to either wait
another day for doing their analysis or gets frustrated and reads much less
data than they really need to. Furthermore the user tends to read in just a
small sample of the data without much knowledge if they are missing
important pieces of the data.


%
%Exascale scientific discovery will be severely bottlenecked without
%sufficient new research into managing and storing the large amounts of data
%that will be produced during the simulation, and analyzed for months
%afterwards.
%%
%In this project we will demonstrate novel techniques to facilitate efficient
%mapping of data objects, even partitioning individual variables, from the
%user space onto multiple storage tiers, and enable application-guided data
%reductions/transformations to address capacity and bandwidth bottlenecks,
%while constraining the error to be within user provided bounds.
%%
%We will address the associated I/O and storage challenges in the context of
%current and emerging storage landscapes, and expedite insights into critical
%scientific processes, demonstrating the validity of our approach in key DOE
%domains. Our techniques will be to research novel techniques into a Scalable
%Storage Software Infrastructure by integrating services from the middle-ware
%layer which will talk to the applications, with the Storage system. The
%negotation between these layers will be a fundamental service which we will
%create in this project to ensure that users will be able to save `'the
%best'' amount of data in the different storage tiers.
%
%The metric we are most interested in optimizing is time to knowledge.
%Current approaches to addressing the I/O bottleneck fall into two broad
%categories. Parallel file system approaches that optimize the throughput for
%an entire system, and I/O middleware approaches that optimize the
%performance of a single application. Both approaches have seen success but
%are unlikely to overcome the major obstacles in reaching exascale. Instead
%of trying to optimize throughput, we will seek to reduce the time to
%knowledge. This is the most significant metric for scientific applications
%where the desired outcome is not storing data, but rather executing a
%knowledge extraction process on the data. Moreover, we aim to perform the
%optimization not for a single application, but rather for workload of a
%multi-user multi-application system environment.
%
%Our approach leverages the expected characteristics of exascale storage
%hardware. The storage layer will be partitioned into multiple heterogenous
%tiers with vastly different performance characteristics. This difference
%between layers will be further exacerbated by the constraints on capacity
%and data lifetime within a layer. Tape archival storage will still maintain
%data long term, but access to this data will be orders of magnitude slower
%than the next layer and naively accessing data from archival storage will
%greatly impact productivity.
%
%We will achieve reduced time to knowledge using a combination of techniques;
%\begin{enumerate}
%\item Data annotations specified at the application level to quantify the
%  relative important, utility and lifetime of data objects;
%\item Partitioning of data objects across the storage hierarchy utilizing
%  the additional knowledge embedded in the annotations;
%\item Evaluating the tradeoffs at runtime to guide data placement, movement,
%  and migration across storage layers using models, heuristics and
%  continuous learning;
%\item Utilize the additional knowledge available about the data to perform
%  application-aware data compression and I/O prioritization;
%\end{enumerate}
%
%We are aiming to spread an output across the vertical layers of the storage
%hierarchy simultaneously. When data should migrate to a higher tier, what
%happens to the existing version?
%
%Is there a canonical copy that is the originally written version?
%
%Is there annotation about this (I think so) attached to that part of the
%variable?
%
%At the tape layer, we see having a lossy compressed version just above the
%tape layer used as a directory/index and then the ``full'' resolution
%version on tape only retrieving once the user accepts the time/quality
%tradeoff. This is going to require some serious language to describe. If we
%have 100s PB of tape space, we'll need 1s PB of disk even at a 99\% data
%reduction. That is non-trivial.
%
%What happens when that data is retrieved from tape? Is a third copy made in
%an appropriately sized/quality level version for the tier it is requested to
%be pulled to? What if it is just the index version? Or the on tape version?
%Does it have a TTL because of the pain retrieving it from tape?
%
%Do we specify a target tier or just allow the system to place it in a place
%that makes sense? Given the potential space limitations, I think this is
%pretty critical because it could cause evictions or other insufficient space
%actions that are unintended. I can see us specifying that when you ask for
%data to be pulled at a particular quality level to a particular performance
%tier that it is a best effort with the use specifying if compromise or just
%failure is the result if the request cannot be fulfilled. Making this
%interaction make sense is going to require some reasonable thought and
%testing. We'll probably need to run it past apps people to get their
%feedback too as I don't think we are able to give a solid answer without
%some broad input.
%
%How does the migration to ``data lakes''/``campaign storage'' work? How does
%the migration to tape ultimately happen? I think we need to incorporate some
%explicit staging commands to move data both up and down the stack along with
%some variability in placement based on data features and current system
%state.
%
%Given the storage scarcity, particularly for NVM, I think we need to have a
%solid story here as part of the proposal. Yes, there are questions that have
%to be answered to build this still, but we need to have some pretty solid
%clarity where possible. I don't feel like I can explain it well enough at
%what I believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
