

\begin{itemize}

\item describe data size problems for getting scientific results out
\item describe annotating and differentiating data storage according to both
use and inherent data qualities (e.g., contains desireable features)
\item describe idea of using heavy compression for less interesting areas and less or no compression for more interesting areas.
\item describe inherent errors in scientific calculations
\item describe lossy compression within error bounds
\item describe how incorporating plug-ins for application aware data compression opportunities to aid data storage
\item describe Sirocco's current state and future designs as a solid foundation on which to work
\item describe how these new features can enhance a system like Sirocco to reduce data intensity
\item describe metadata challenges that we will also have to face.

\end{itemize}

every section should include:

Problem and background, one or more solution approaches, and investigation goals/questions.

Technical Areas
 
1. Application Intentions.\\
2. Metadata Management (middleware and storage system)\\
3. Pluggable infrastructure for data transformation (storage system)\\
4. Intelligent (autonomic, application-aware) data mapping to storage/memory hierarchies (middleware and storage system)\\
5. Predictable Performance and resolution tradeoffs\\
6. Data re-generation. (middleware \& storage)\\
7. Learning Motifs and data re-organization.\\

\section*{Introduction}

Exascale scientific discovery will be severely bottlenecked without
sufficient new research into managing and storing the large amounts of data
that will be produced during the simulation, and analyzed for months
afterwards.  
%
In this project we will demonstrate novel techniques to
facilitate efficient mapping of data objects, even partitioning individual
variables, from the user space onto multiple storage tiers, and enable
application-guided data reductions/transformations to address capacity and
bandwidth bottlenecks, while constraining the error to be within user
provided bounds. 
%
We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes, and expedite insights into critical
scientific processes, demonstrating the validity of our approach in key DOE
domains.  Our techniques will be to research novel techniques into a Scalable 
Storage Software Infrastructure by integrating services from the middle-ware layer which will
talk to the applications, with the Storage system. The negotation between these layers will
be a fundamental service which we will create in this project to ensure that users will be able
to save `'the best'' amount of data in the different storage tiers.

The metric we are most interested in optimizing is time to knowledge. Current
approaches to addressing the I/O bottleneck fall into two broad
categories. Parallel file system approaches that optimize the throughput for
an entire system, and I/O middleware approaches that optimize the
performance of a single application. Both approaches have seen success but
are unlikely to overcome the major obstacles in reaching exascale. Instead
of trying to optimize throughput, we will seek to reduce the time to
knowledge. This is the most significant metric for scientific applications
where the desired outcome is not storing data, but rather executing a
knowledge extraction process on the data. Moreover, we aim to perform the
optimization not for a single application, but rather for workload of a
multi-user multi-application system environment.

Our approach leverages the expected characteristics of exascale storage
hardware. The storage layer will be partitioned into multiple heterogenous
tiers with vastly different performance characteristics. This difference
between layers will be further exacerbated by the constraints on capacity
and data lifetime within a layer. Tape archival storage will still maintain
data long term, but access to this data will be orders of magnitude slower
than the next layer and naively accessing data from archival storage will
greatly impact productivity. 

We will achieve reduced time to knowledge using a combination of techniques;
\begin{enumerate}
\item Data annotations specified at the application level to quantify the
  relative important, utility and lifetime of data objects;
\item Partitioning of data objects across the storage hierarchy utilizing
  the additional knowledge embedded in the annotations;
\item Evaluating the tradeoffs at runtime to guide data placement, movement,
  and migration across storage layers using models, heuristics and continuous
  learning;
\item Utilize the additional knowledge available about the data to perform
  application-aware data compression and I/O prioritization;
\end{enumerate}

We are aiming to spread an output across the vertical layers of the storage
hierarchy simultaneously.  When data should migrate to a higher tier, what
happens to the existing version?

Is there a canonical copy that is the originally written version?

Is there annotation about this (I think so) attached to that part of the variable?

At the tape layer, we see having a lossy compressed version just above the tape
layer used as a directory/index and then the ``full'' resolution version on
tape only retrieving once the user accepts the time/quality tradeoff. This is
going to require some serious language to describe. If we have 100s PB of tape
space, we'll need 1s PB of disk even at a 99\% data reduction. That is
non-trivial.

What happens when that data is retrieved from tape? Is a third copy made in an
appropriately sized/quality level version for the tier it is requested to be
pulled to? What if it is just the index version? Or the on tape version? Does
it have a TTL because of the pain retrieving it from tape?

Do we specify a target tier or just allow the system to place it in a place
that makes sense? Given the potential space limitations, I think this is pretty
critical because it could cause evictions or other insufficient space actions
that are unintended. I can see us specifying that when you ask for data to be
pulled at a particular quality level to a particular performance tier that it
is a best effort with the use specifying if compromise or just failure is the
result if the request cannot be fulfilled.  Making this interaction make sense
is going to require some reasonable thought and testing. We'll probably need to
run it past apps people to get their feedback too as I don't think we are able
to give a solid answer without some broad input.

How does the migration to ``data lakes''/``campaign storage'' work? How does the
migration to tape ultimately happen? I think we need to incorporate some
explicit staging commands to move data both up and down the stack along with
some variability in placement based on data features and current system state. 

Given the storage scarcity, particularly for NVM, I think we need to have a
solid story here as part of the proposal. Yes, there are questions that have to
be answered to build this still, but we need to have some pretty solid clarity
where possible. I don't feel like I can explain it well enough at what I
believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
