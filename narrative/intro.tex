\section{Introduction} {\color{red} Scott, Hasan, Gary, Jay - 4 pages}
\label{sec:introduction}

Avoiding impending bottlenecks in exascale scientific discovery will require
new research into managing and storing the large amounts of data
that will be produced by simulations and analyzed for months
afterwards.
%
In this project we will demonstrate a cooperative approach for storing data
where the user and the storage system work together to achieve the best possible data
quality and performance given data features, storage tier characteristics,
and current system state. We will investigate novel techniques to facilitate efficient
mapping of data objects, even partitioning individual variables, from the user
space onto multiple storage tiers and enable application-guided data
reductions/transformations to address capacity and bandwidth bottlenecks while
constraining the error to be within user provided bounds.
% 
We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes and expedite insights into critical
scientific processes, while demonstrating the validity of our approach in key DOE
domains. We will research techniques for creating a Scalable
Storage Software Infrastructure, integrating services from the middleware
layer with Storage System features to support the
novel strategy of distributing data both horizontally and vertically across the storage
system. 
The negotiation between these layers will be provided as a fundamental service, 
allowing users to easily save ``the best'' amount of data, automatically placed
throughout the entire storage stack.

The metric we are most interested in optimizing is time to knowledge.
Current approaches to addressing the I/O bottleneck fall into two broad
categories: parallel file system approaches that optimize the throughput for
an entire system, and I/O middleware approaches that optimize the
performance of a single application. Both approaches have been successful to date, but
are unlikely to overcome the remaining obstacles in reaching exascale. Instead
of trying to optimize throughput, we will seek to reduce the time to
knowledge. This is the most significant metric for scientific applications
where the desired outcome is not storing data, but rather executing a
knowledge extraction process on the data. Moreover, we aim to perform the
optimization not for a single application, but rather for workloads within a
multi-user multi-application system environment.

Our approach targets the expected characteristics of exascale storage
hardware. The storage layer will be partitioned into multiple heterogenous
tiers with vastly different performance characteristics. The layers
will be further differentiated by the constraints on capacity
and data lifetime within each layer. Tape archival storage will still maintain
data long term, but access to this data will be orders of magnitude slower
than the next layer and naively accessing data from archival storage will
greatly impact productivity.
%
The overarching idea is that data from applications can be characterized by different
levels of importance and different level of importance over time. 
Codes such as the XGC1 simulation from C. S. Chang (PPPL), which is one of largest users of Leadership Class Facilities (over 300M hours at ANL, NERSC, and ORNL) has launched a series of simulations which need to write out 100 PB of data on the Titan system. Due to resource limitations, the simulation will only write about 10 PB, but
when our team considered not only the time it would take to write out 10 PB in 10 days of run, but also where we would store this, which had to be tape since it would take
over half of the file system, we decided that we couldn't write this amount of data. In other words we had to make decisions of which data to save from the original 100 PB.
The first thing that we had to discover is how much space we could have on the parallel file system at any given time, and how much time it would take to archive 1 PB of data,
which turned out to be 10 days, and we could only store 2 PB of data at once. This quickly led us to 1) decide which pieces of data to save from the large amount of data; in 
other words, we had to prioritize which were the most important pieces. We had to further breakdown the large data into two categories, data which we think we will touch after
the simulation is complete ( after the 10 days of wallclock time was finished), and what we would do with this data (the access pattern of the large data). We then had to 
discover application specific data reduction techniques, and encode that into our solution, on top of ADIOS. We then had to implement a crude discovery system to understand
which pieces of data were stored on the parallel file system and which pieces moved over to PPPL, and which pieces were on tape.  This solution worked well for one day
but after the second day more problems occurred, the variability of the time it took to write the data made the I/O prohibitively costly, because even though we had the
majority of the OLCF, the file system was shared and sometimes the writing time took less 100 seconds, and at other times it was over 1,000 seconds. We had no method
to monitor this time and then write less data when the time was going to be longer.   After the simulation is complete we also need to visualize this reduced data, and 
an additional problem appeared; just to get a first look at the visualization data seemed like it could take over a month because some of the timesteps were on tape and other
timesteps were on the file system. We had no method to just look at the data quickly and understand which regions of the simulation to focus on. 






To address these challenges we are proposing to combine knowledge
from a wide range of sources including ORNL applications, the ADIOS project,
the SNL Lightweight File Systems project and storage system knowledge, the metadata
management and storage systems knowledge of UCSC, and the middleware expertise
of Rutgers. We will integrate this diverse expertise to investigate how to make
a data, workflow, and application aware storage system.

%ADIOS was developed with the understanding that we must
%not only address the bottlenecks for current applications and
%hardware platforms but also provide a path forward for the next
%generation of applications and systems that would need to both
%maximize bandwidth to the storage system and also support
%transparently working around the storage system bandwidth
%limitations with new techniques and tools. To support the
%diverse operating modes of both using persistent storage and
%other data storage and processing technology, we made a
%great effort to provide a simplified interface to application
%developers, offering a simple, portable and scalable way for
%scientists to manage data that may need to be written, read
%or processed during simulation runs. This required abstracting
%away many decisions typically made in the application code
%so that they may be configured externally.
%In addition to this abstracted interface with external configuration
%options, common services were incorporated to afford
%optimizations beyond those for a single platform, such as
%buffering, aggregation, subfiling, and chunking with options
%to select each based on the data distribution characteristics
%of the application. A variety of asynchronous I/O techniques
%have been investigated and are being integrated with ADIOS.
%A recognition of application complexity has led to new techniques
%for testing I/O performance by extracting I/O patterns
%from applications and automatically generating benchmark
%codes. And finally, the march toward exascale has fueled the
%need to consider additional features such as compression and
%indexing to better cope with the expected tsunami of data.

In this project we will extend ADIOS~\cite{adios} with new APIs and novel techniques
to coordinate with the storage back-end. The key idea is offering the ability to
negotiate with the storage system to agree on the tradeoff between data quality
and response time. We will test our ideas on the next generation OLCF,
SNL/LANL, and NERSC systems.
%
We plan to leverage the many inovations we discovered in 
the Lightweight File Systems~\cite{lwfs} (LWFS) project and the Ceph storage system project (ceph.com).
In the LWFS project, Sandia sought to
demonstrate stripping down a file system to the bare minimum features required
for any storage system. Then, additional functionality and semantics, such as
POSIX-style directories and consistency requirements, were added in using
auxiliary services separate from the stack or by layering providing a new
interface to the storage system. The project's first phase demonstrated that
using a core object storage system with integrated security providing
authentication and authorization services, customized storage interfaces could
be developed with exactly the overhead required for different applications. The
second phase of LWFS, known as Sirocco~\cite{sirocco}, is investigating how to
manage storage devices for efficient and scalable storage performance.  Sirocco
has been in process for three years and has developed a usable storage infrastructure.

The Ceph project (ceph.com) started at UC Santa Cruz in 2004 and
was initially funded by DOE/NNSA involving LLNL, LANL, and Sandia,
with the goal to create an object-based parallel file system that
addresses the well-known metadata service bottleneck of creating
and accessing 1000s of files comprising a checkpoint~\cite{weil:osdi06}.
However, after the Ceph kernel client was pulled into the Linux
mainline in 2010, the focus of Ceph shifted to its object-based
storage subsystem, also called Reliable Autonomous Distributed
Object Store (RADOS), which now is the dominant storage system in
OpenStack production-level deployments and was recently selected
as the new backend for Flickr and Yahoo! Cloud Services. In this
project we will leverage two of the key innovations of Ceph: (1)
Controlled Replication Under Scalable Hashing (CRUSH)~\cite{weil:sc06}
will help making resource identification and predictable performance
scalable, and (2) extensible storage object classes~\cite{watkins:ucsctr15}
will provide the main mechanisms of introducing new APIs,
metadata vocabulary, and quality of service into the storage stack.


We will examine the research challenges from the perspective of
%
(1) the user of the system who is attempting to run an exascale simulation,
%
(2) the Storage System and I/O layer which needs to negotiate between all of
the users, and finally
%
(3) the user of the system who is attempting to understand the data
produced by a set of simulations.

\subsection{Writing Perspective}
\label{subsec:sim-perspective}
The simulation scientist ideally should be able to
obtain an estimate of how much time will be required to write their data,
and how much storage space they will be able to get during the lifetime of
their simulations. Furthermore, they would like the ability to get an assurance
of Quality of Service such that they can react appropriately when the
expected bandwidth, for example, is less than what they desire. Ideally, the user
would specify a set of rules with which the system can make autonomic
decisions to determine what should be done.
%

One of the key aspects of our approach is to allow the users to define the
utility of their data; in other-words, the system should provide a
mechanism such that, for each data object, a user can specify pieces of the object
which have more utility (more importance).
Clearly there are many research challenges faced with
this approach including: 1) How do we make this simple enough so that users can
do this, 2) Can we define a model to allow the system to make
autonomic decisions to save the data which has the highest utility, and 3) Can
we allow the back-end storage system to interact with this model so that
it can optimize not just a single application but across all of the
applications.
 %
  In our simple scenario, a user wants to write 1 PB of data every hour
for checkpoint/restart, and they wish to write 500TB of data every 15
minutes to save analysis and visualization results which have already been reduced
using in situ reduction techniques. At this stage, they will ask the system to
write their data, and they would like to get an estimated time which they can
then figure out, through a series of rules, whether they want to write out all
of the data, postpone the write until a later time, or write a reduced amount
of data, along with some code
container to allow them to regenerate the data with a specified level of
accuracy.  Furthermore, they realize that the analysis data will be saved on
either the parallel file system, long-term tape storage, or on the campaign storage,
which is a medium-term storage
area with lower bandwidth than the parallel file system.
For each piece of data, the user will specify the lifetime so
that the data with the highest utility is kept around for the longest
period of time, but the data which has the least utility might be migrated to
lower layers of the storage hierarchy more quickly. 

Since the data will be a collection of objects placed on the storage system,
some of the objects will be broken up into multiple pieces by ``plug-ins''
to the system which will allow data to be characterized first in terms of the most
important pieces, and then by subsequent levels of detail. We can think of this as
something similar to an Adaptive Mesh Refinement scheme which keeps track of
the places where there are features such as steep gradients, which contain large errors on the
coarser views, as opposed to other areas with smaller errors. These distinctions allow us to make decisions
about which sub-objects to save (all of the data, which will then go to
different layers) or just some of the sub-objects? The research questions
are numerous: How does the user specify their intentions? How does the user
prioritize the different sub-objects of the data? Where does the data go
into the storage hierarchy? Does the data get replicated in the lower tiers
of the hierarchy? When the user specifies the time they want the data saved,
how does this get into the storage system?

\subsection{Storage System Perspective}
\label{subsec:storage-perspective}
From the perspective of the storage system, it is necessary to manage the data from
a given user request amongst all of the requests from all users, and try to optimize the
entire system accordingly. Today the problem is that when a user writes there is a
tremendous amount of interference from other users on the system who may be
writing or reading at the same time. By requiring a certain quality
of service the system must potentially lock out users with lower currency
they want to place in the reading or writing of their data. The storage
system must also automatically move data from one tier of the storage to
another, without affecting the quality of service that was offered at request
time. The storage system must be able to organize
data amongst all of the tiers of storage in a known, consistent way.

One of the clear research objectives of this project is to understand how the
storage system can interact with the middleware (application-aware) piece so
that the data with the highest utility is kept on the fastest layers of the
storage system for as long as specified. Clearly we need to maintain 
QoS such that fuzzy estimates of the time it will take to write/read data from
the different layers informs any migration/eviction decisions, and clearly it
is critical that the storage system can manage the data across all of the
layers. We will use learning techniques to automatically migrate data
sub-objects across the hierarchy as well as to eventually evict data which is
large and has very low utility.


\subsection{Reading  Perspective}
\label{subsec:reading-perspective}
From the perspective of the users who want to read in the data from their
simulations, and then operate on this data and possibly write data, we see
that in general these users will be running on much less compute resources
than the users who are running the simulation. One of the important aspects
of this class of user is that they want a reduction of latency and they
require a certain amount of quality of service as well. When someone is
doing interactive analysis and expects that the data they read in is about
10s, but it turns into 1,000s, the user typically tends to either wait
another day for doing their analysis or gets frustrated and reads much less
data than they really need to. Furthermore the user tends to read in just a
small sample of the data without much knowledge if they are missing
important pieces of the data.
\subsection{Research Challenges to be address}
\label{subsec:challenges}
Our research into the storage system and middle ware layers must address
many of these challenges including
\begin{enumerate}
\item How do we describe the user intentions at the API layer and have this
  communicated down to the middle-ware and storage layers?
\item How do we allow users to define their user defined compression
  techniques and have this data read back from the storage system layers?
  Where do we execute the decompression techniques 
\item How do we evaluate the tradeoffs at runtime to guide data placement,
  including the movement of data across the network, across the different
  storage tiers? (data movement and quality tradeoff)
\item Can we use different forms of learning techniques as daemons on the
  system to perform data migration from one storage tier to another? For
  example, if we see that a user is looking at one time slice after another
  for a certain object in their dataset which is in the slower tiers of the
  hierarchy, do we automatically propane up the data which has not been
  requested in the hope that this will be requested data? (prediction and
  prefetching, user defined compression on movement)
\item Can we understand the true need of campaign storage and understand the
  possible impact of the campaign storage if we run a hadoop-like
  file-system instead of a lustre/gpfs file system? (additional layers)
\item Can we limit the amount of data duplication? Data space is going to be
  very constrained on the exascale system so we must ensure that minimal
  copies are made of data, and when data is duplicated, they are removed
  through a garbage collection routine. (space management)
\item Can we build a model to give us the time estimates during the reading
  and writing phases, so that rules can be in place from the user
  perspective to make adaptive decisions. For example, if the file system
  says that reading will take 3 months, users can they place in rules to
  then read in a subset of data, and they can understand which data can be
  read quickly and which pieces will take more time. (estimation)
\item Can we understand how to place code in the system which will allow
  data-regeneration to take place. (regeneration)
\item How does the storage system do a better job in managing request from
  all of the users on a LCF than today? We realize that today users who have
  a better middlware system can often lock other users from getting high
  performance when they are running. If we have the concept of currency
  which is eventually used in the same extent as node-hours, then users will
  have to be able to think about how much storage and how much bandwidth
  they can choose. One question that needs to be understood is if there are
  times when the system sees that there are very few storage system
  resources being used, then the lucky users can get the bandwidth cheaper
  than at times of heavy usage. How can we enforce this? (Fairness)
\item How do we manage all of the metdata not only from the principle
  objects, but from the sub-objects? How well will this scale when we have
  users who can potentially create billions of objects from their
  simulation? (scalability and discovery)
\end{enumerate}
%
%Exascale scientific discovery will be severely bottlenecked without
%sufficient new research into managing and storing the large amounts of data
%that will be produced during the simulation, and analyzed for months
%afterwards.
%%
%In this project we will demonstrate novel techniques to facilitate efficient
%mapping of data objects, even partitioning individual variables, from the
%user space onto multiple storage tiers, and enable application-guided data
%reductions/transformations to address capacity and bandwidth bottlenecks,
%while constraining the error to be within user provided bounds.
%%
%We will address the associated I/O and storage challenges in the context of
%current and emerging storage landscapes, and expedite insights into critical
%scientific processes, demonstrating the validity of our approach in key DOE
%domains. Our techniques will be to research novel techniques into a Scalable
%Storage Software Infrastructure by integrating services from the middle-ware
%layer which will talk to the applications, with the Storage system. The
%negotation between these layers will be a fundamental service which we will
%create in this project to ensure that users will be able to save `'the
%best'' amount of data in the different storage tiers.
%
%The metric we are most interested in optimizing is time to knowledge.
%Current approaches to addressing the I/O bottleneck fall into two broad
%categories. Parallel file system approaches that optimize the throughput for
%an entire system, and I/O middleware approaches that optimize the
%performance of a single application. Both approaches have seen success but
%are unlikely to overcome the major obstacles in reaching exascale. Instead
%of trying to optimize throughput, we will seek to reduce the time to
%knowledge. This is the most significant metric for scientific applications
%where the desired outcome is not storing data, but rather executing a
%knowledge extraction process on the data. Moreover, we aim to perform the
%optimization not for a single application, but rather for workload of a
%multi-user multi-application system environment.
%
%Our approach leverages the expected characteristics of exascale storage
%hardware. The storage layer will be partitioned into multiple heterogenous
%tiers with vastly different performance characteristics. This difference
%between layers will be further exacerbated by the constraints on capacity
%and data lifetime within a layer. Tape archival storage will still maintain
%data long term, but access to this data will be orders of magnitude slower
%than the next layer and naively accessing data from archival storage will
%greatly impact productivity.
%
%We will achieve reduced time to knowledge using a combination of techniques;
%\begin{enumerate}
%\item Data annotations specified at the application level to quantify the
%  relative important, utility and lifetime of data objects;
%\item Partitioning of data objects across the storage hierarchy utilizing
%  the additional knowledge embedded in the annotations;
%\item Evaluating the tradeoffs at runtime to guide data placement, movement,
%  and migration across storage layers using models, heuristics and
%  continuous learning;
%\item Utilize the additional knowledge available about the data to perform
%  application-aware data compression and I/O prioritization;
%\end{enumerate}
%
%We are aiming to spread an output across the vertical layers of the storage
%hierarchy simultaneously. When data should migrate to a higher tier, what
%happens to the existing version?
%
%Is there a canonical copy that is the originally written version?
%
%Is there annotation about this (I think so) attached to that part of the
%variable?
%
%At the tape layer, we see having a lossy compressed version just above the
%tape layer used as a directory/index and then the ``full'' resolution
%version on tape only retrieving once the user accepts the time/quality
%tradeoff. This is going to require some serious language to describe. If we
%have 100s PB of tape space, we'll need 1s PB of disk even at a 99\% data
%reduction. That is non-trivial.
%
%What happens when that data is retrieved from tape? Is a third copy made in
%an appropriately sized/quality level version for the tier it is requested to
%be pulled to? What if it is just the index version? Or the on tape version?
%Does it have a TTL because of the pain retrieving it from tape?
%
%Do we specify a target tier or just allow the system to place it in a place
%that makes sense? Given the potential space limitations, I think this is
%pretty critical because it could cause evictions or other insufficient space
%actions that are unintended. I can see us specifying that when you ask for
%data to be pulled at a particular quality level to a particular performance
%tier that it is a best effort with the use specifying if compromise or just
%failure is the result if the request cannot be fulfilled. Making this
%interaction make sense is going to require some reasonable thought and
%testing. We'll probably need to run it past apps people to get their
%feedback too as I don't think we are able to give a solid answer without
%some broad input.
%
%How does the migration to ``data lakes''/``campaign storage'' work? How does
%the migration to tape ultimately happen? I think we need to incorporate some
%explicit staging commands to move data both up and down the stack along with
%some variability in placement based on data features and current system
%state.
%
%Given the storage scarcity, particularly for NVM, I think we need to have a
%solid story here as part of the proposal. Yes, there are questions that have
%to be answered to build this still, but we need to have some pretty solid
%clarity where possible. I don't feel like I can explain it well enough at
%what I believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
