\section{Introduction} 
\label{sec:introduction}
{\bf {\color{red}Norbert  will integrate these pieces}}
{\em The purpose of computing is insight, not numbers}~\cite{Hamming:book}.
Richard Hamming's remark, made over 50 years ago at the dawning of the age of
large scale scientific computation, is even more relevant today as we prepare
for scientific simulation at the exascale. Hamming was a pioneer in the fields
of coding theory, scientific computing and information theory who would
instantly sympathize with the central objective of this proposal--minimizing
the time to knowledge.  

Avoiding impending bottlenecks in exascale scientific discovery will require
new research into managing and storing the large amounts of data that will be
produced by simulations and analyzed for months afterwards.
%
In this project we will demonstrate a cooperative approach for storing data
where the user and the storage system work together to achieve the best
possible data quality and performance given data features, storage tier
characteristics, and current system state. We will investigate novel techniques
to facilitate efficient mapping of data objects, even partitioning individual
variables, from the user space onto multiple storage tiers and enable
application-guided data reductions/transformations to address capacity and
bandwidth bottlenecks while constraining the error to be within user provided
bounds.
% 
We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes and expedite insights into critical
scientific processes, while demonstrating the validity of our approach in key
DOE domains. We will research techniques for creating a Scalable Storage
Software Infrastructure, integrating services from the middleware layer with
Storage System features to support the novel strategy of distributing data both
horizontally and vertically across the storage system.  The negotiation between
these layers will be provided as a fundamental service, allowing users to
easily save ``the best'' amount of data, automatically placed throughout the
entire storage stack.

The metric we are most interested in optimizing is time to knowledge.  Current
approaches to addressing the I/O bottleneck fall into two broad categories:
parallel file system approaches that optimize the throughput for an entire
system, and I/O middleware approaches that optimize the performance of a single
application. Both approaches have been successful to date, but are unlikely to
overcome the remaining obstacles in reaching exascale. Instead of trying to
optimize throughput, we will seek to reduce the time to knowledge. This is the
most significant metric for scientific applications where the desired outcome
is not storing data, but rather executing a knowledge extraction process on the
data. Moreover, we aim to perform the optimization not for a single
application, but rather for workloads within a multi-user multi-application
system environment.

Our approach targets the expected characteristics of exascale storage hardware.
The storage layer will be partitioned into multiple heterogeneous tiers with
vastly different performance characteristics. The layers will be further
differentiated by the constraints on capacity and data lifetime within each
layer. Tape archival storage will still maintain data long term, but access to
this data will be orders of magnitude slower than the next layer and naively
accessing data from archival storage will greatly impact productivity.
%
Our approach in this proposal is to enable researchers to place more knowledge
into the SSIO system so that data can be recast from a bunch of bytes to
information which the system can then place and optimize the time to gain
knowledge from exascale simulations.  We first motivate this by a problem our
team has been working on in the past six months, and then recast this problem
into a series of research questions and challenges which must be addressed in
order to optimize the SSIO system for exascale science.

The traditional approach to transforming results of scientific computation into
knowledge consists of storing the numerical output of a simulation on disk,
followed by a post-processing or analysis stage that might take place over
several ensuing weeks or months. The overarching objective is to gain insight,
or knowledge.  As already mentioned, the anticipated scale of computations
means that this workflow is simply no longer viable.
%
\section{Scalable Storage Software Infrastructure}
Our focus in this proposal is in theme 2: Scalable Storage Software Infrastructure. We will focus on a state of the art
SSIO solution to solve problems which researchers are beginning to face and will make science on exascale systems
problematic without new insights and solutions.

The overarching idea is that data from applications can be characterized by different
levels of importance and different level of importance over time. 
Codes such as the XGC1 simulation from C. S. Chang (PPPL), which is one of largest users of Leadership Class Facilities (over 300M hours at ANL, NERSC, and ORNL) has launched a series of simulations which need to write out 100 PB of data on the Titan system. Due to resource limitations, the simulation will only write about 10 PB, but
when our team considered not only the time it would take to write out 10 PB in 10 days of run, but also where we would store this, which had to be tape since it would take
over half of the file system, we decided that we couldn't write this amount of data. In other words we had to make decisions of which data to save from the original 100 PB.
The first thing that we had to discover is how much space we could have on the parallel file system at any given time, and how much time it would take to archive 1 PB of data,
which turned out to be 10 days, and we could only store 2 PB of data at once. This quickly led us to 1) decide which pieces of data to save from the large amount of data; in 
other words, we had to prioritize which were the most important pieces. We had to further breakdown the large data into two categories, data which we think we will touch after
the simulation is complete ( after the 10 days of wallclock time was finished), and what we would do with this data (the access pattern of the large data). We then had to 
discover application specific data reduction techniques, and encode that into our solution, on top of ADIOS. We then had to implement a crude discovery system to understand
which pieces of data were stored on the parallel file system and which pieces moved over to PPPL, and which pieces were on tape.  This solution worked well for one day
but after the second day more problems occurred, the variability of the time it took to write the data made the I/O prohibitively costly, because even though we had the
majority of the OLCF, the file system was shared and sometimes the writing time took less 100 seconds, and at other times it was over 1,000 seconds. We had no method
to monitor this time and then write less data when the time was going to be longer.   After the simulation is complete we also need to visualize this reduced data, and 
an additional problem appeared; just to get a first look at the visualization data seemed like it could take over a month because some of the timesteps were on tape and other
timesteps were on the file system. We had no method to just look at the data quickly and understand which regions of the simulation to focus on. 


To address these challenges we are proposing to combine knowledge
from a wide range of sources including ORNL applications, the ADIOS project,
the SNL Lightweight File Systems project and storage system knowledge, the metadata
management and storage systems knowledge of UCSC, and the middleware expertise
of Rutgers. We will integrate this diverse expertise to investigate how to make
a data, workflow, and application aware storage system.

%ADIOS was developed with the understanding that we must
%not only address the bottlenecks for current applications and
%hardware platforms but also provide a path forward for the next
%generation of applications and systems that would need to both
%maximize bandwidth to the storage system and also support
%transparently working around the storage system bandwidth
%limitations with new techniques and tools. To support the
%diverse operating modes of both using persistent storage and
%other data storage and processing technology, we made a
%great effort to provide a simplified interface to application
%developers, offering a simple, portable and scalable way for
%scientists to manage data that may need to be written, read
%or processed during simulation runs. This required abstracting
%away many decisions typically made in the application code
%so that they may be configured externally.
%In addition to this abstracted interface with external configuration
%options, common services were incorporated to afford
%optimizations beyond those for a single platform, such as
%buffering, aggregation, subfiling, and chunking with options
%to select each based on the data distribution characteristics
%of the application. A variety of asynchronous I/O techniques
%have been investigated and are being integrated with ADIOS.
%A recognition of application complexity has led to new techniques
%for testing I/O performance by extracting I/O patterns
%from applications and automatically generating benchmark
%codes. And finally, the march toward exascale has fueled the
%need to consider additional features such as compression and
%indexing to better cope with the expected tsunami of data.



We will examine the research challenges from the perspective of
%
(1) the user of the system who is attempting to run an exascale simulation,
%
(2) the Storage System and I/O layer which needs to negotiate between all of
the users, and finally
%
(3) the user of the system who is attempting to understand the data
produced by a set of simulations.

\subsection{Writing Perspective}
\label{subsec:sim-perspective}
The simulation scientist ideally should be able to
obtain an estimate of how much time will be required to write their data,
and how much storage space they will be able to get during the lifetime of
their simulations. Furthermore, they would like the ability to get an assurance
of Quality of Service such that they can react appropriately when the
expected bandwidth, for example, is less than what they desire. Ideally, the user
would specify a set of rules with which the system can make autonomic
decisions to determine what should be done.
%

One of the key aspects of our approach is to allow the users to define the
utility of their data; in other-words, the system should provide a
mechanism such that, for each data object, a user can specify pieces of the object
which have more utility (more importance).
Clearly there are many research challenges faced with
this approach including: 1) How do we make this simple enough so that users can
do this, 2) Can we define a model to allow the system to make
autonomic decisions to save the data which has the highest utility, and 3) Can
we allow the back-end storage system to interact with this model so that
it can optimize not just a single application but across all of the
applications.
 %
  In our simple scenario, a user wants to write 1 PB of data every hour
for checkpoint/restart, and they wish to write 500TB of data every 15
minutes to save analysis and visualization results which have already been reduced
using in situ reduction techniques. At this stage, they will ask the system to
write their data, and they would like to get an estimated time which they can
then figure out, through a series of rules, whether they want to write out all
of the data, postpone the write until a later time, or write a reduced amount
of data, along with some code
container to allow them to regenerate the data with a specified level of
accuracy.  Furthermore, they realize that the analysis data will be saved on
either the parallel file system, long-term tape storage, or on the campaign storage,
which is a medium-term storage
area with lower bandwidth than the parallel file system.
For each piece of data, the user will specify the lifetime so
that the data with the highest utility is kept around for the longest
period of time, but the data which has the least utility might be migrated to
lower layers of the storage hierarchy more quickly. 

Since the data will be a collection of objects placed on the storage system,
some of the objects will be broken up into multiple pieces by ``plug-ins''
to the system which will allow data to be characterized first in terms of the most
important pieces, and then by subsequent levels of detail. We can think of this as
something similar to an Adaptive Mesh Refinement scheme which keeps track of
the places where there are features such as steep gradients, which contain large errors on the
coarser views, as opposed to other areas with smaller errors. These distinctions allow us to make decisions
about which sub-objects to save (all of the data, which will then go to
different layers) or just some of the sub-objects? The research questions
are numerous: How does the user specify their intentions? How does the user
prioritize the different sub-objects of the data? Where does the data go
into the storage hierarchy? Does the data get replicated in the lower tiers
of the hierarchy? When the user specifies the time they want the data saved,
how does this get into the storage system?

\subsection{Storage System Perspective}
\label{subsec:storage-perspective}
From the perspective of the storage system, it is necessary to manage the data from
a given user request amongst all of the requests from all users, and try to optimize the
entire system accordingly. Today the problem is that when a user writes there is a
tremendous amount of interference from other users on the system who may be
writing or reading at the same time. By requiring a certain quality
of service the system must potentially lock out users with lower currency
they want to place in the reading or writing of their data. The storage
system must also automatically move data from one tier of the storage to
another, without affecting the quality of service that was offered at request
time. The storage system must be able to organize
data amongst all of the tiers of storage in a known, consistent way.

One of the clear research objectives of this project is to understand how the
storage system can interact with the middleware (application-aware) piece so
that the data with the highest utility is kept on the fastest layers of the
storage system for as long as specified. Clearly we need to maintain 
QoS such that fuzzy estimates of the time it will take to write/read data from
the different layers informs any migration/eviction decisions, and clearly it
is critical that the storage system can manage the data across all of the
layers. We will use learning techniques to automatically migrate data
sub-objects across the hierarchy as well as to eventually evict data which is
large and has very low utility.


\subsection{Reading  Perspective}
\label{subsec:reading-perspective}
From the perspective of the users who want to read in the data from their
simulations, and then operate on this data and possibly write data, we see
that in general these users will be running on much less compute resources
than the users who are running the simulation. One of the important aspects
of this class of user is that they want a reduction of latency and they
require a certain amount of quality of service as well. When someone is
doing interactive analysis and expects that the data they read in is about
10s, but it turns into 1,000s, the user typically tends to either wait
another day for doing their analysis or gets frustrated and reads much less
data than they really need to. Furthermore the user tends to read in just a
small sample of the data without much knowledge if they are missing
important pieces of the data.

Our proposal will be responsive to theme 2 in the call, and we summarize them in the following table.
\begin{table}[htbp]
\vspace{2ex}
\begin{center}
\begin{tabular}{ | p {0.2cm} | p{5cm} | p{7.5cm} | p{1.6cm} |}\hline
\multicolumn{2}{|c|}{\bf Call area} & \multicolumn{1}{|c|}{\bf Relevant S2E2 work} & \multicolumn{1}{|c|}{\bf See} \\\hline\hline
%\multicolumn{2}{|c|}{Collaboration} & 
\multicolumn{2}{|p {2.4cm}|}{Approaches to improve the ability of SSIO software to support checkpoint/restart;} & 
General piece we are doing for this... & 
\S\ref{sec:?}, \S\ref{sec:??}, \\\hline
1 & Data \hspace{1cm} abstractions &What will we do& \S\ref{sec:?}, \S\ref{sec:?}, \S\ref{sec:?} \\\hline
2 & Mapping science data models onto hierarchical storage &What we are doing & \S\ref{sec:?}, \S\ref{sec:?} \\\hline
3 & Mechanisms for data movement across the storage hierarchy  & what are we doing. & \S\ref{sec:?} \\\hline
4 &  Extend the interaction of IO middleware with the resource management system & What we are doing & \S\ref{sec:?}, \S\ref{sec:?}  \\\hline
5 & Design and implement of IO middleware architectures  &What we are doing & \S\ref{sec:?}, \S\ref{sec:?}, \S\ref{sec:?} \\
\hline
\end{tabular}
\end{center}
\end{table}



%
%Exascale scientific discovery will be severely bottlenecked without
%sufficient new research into managing and storing the large amounts of data
%that will be produced during the simulation, and analyzed for months
%afterwards.
%%
%In this project we will demonstrate novel techniques to facilitate efficient
%mapping of data objects, even partitioning individual variables, from the
%user space onto multiple storage tiers, and enable application-guided data
%reductions/transformations to address capacity and bandwidth bottlenecks,
%while constraining the error to be within user provided bounds.
%%
%We will address the associated I/O and storage challenges in the context of
%current and emerging storage landscapes, and expedite insights into critical
%scientific processes, demonstrating the validity of our approach in key DOE
%domains. Our techniques will be to research novel techniques into a Scalable
%Storage Software Infrastructure by integrating services from the middle-ware
%layer which will talk to the applications, with the Storage system. The
%negotation between these layers will be a fundamental service which we will
%create in this project to ensure that users will be able to save `'the
%best'' amount of data in the different storage tiers.
%
%The metric we are most interested in optimizing is time to knowledge.
%Current approaches to addressing the I/O bottleneck fall into two broad
%categories. Parallel file system approaches that optimize the throughput for
%an entire system, and I/O middleware approaches that optimize the
%performance of a single application. Both approaches have seen success but
%are unlikely to overcome the major obstacles in reaching exascale. Instead
%of trying to optimize throughput, we will seek to reduce the time to
%knowledge. This is the most significant metric for scientific applications
%where the desired outcome is not storing data, but rather executing a
%knowledge extraction process on the data. Moreover, we aim to perform the
%optimization not for a single application, but rather for workload of a
%multi-user multi-application system environment.
%
%Our approach leverages the expected characteristics of exascale storage
%hardware. The storage layer will be partitioned into multiple heterogenous
%tiers with vastly different performance characteristics. This difference
%between layers will be further exacerbated by the constraints on capacity
%and data lifetime within a layer. Tape archival storage will still maintain
%data long term, but access to this data will be orders of magnitude slower
%than the next layer and naively accessing data from archival storage will
%greatly impact productivity.
%
%We will achieve reduced time to knowledge using a combination of techniques;
%\begin{enumerate}
%\item Data annotations specified at the application level to quantify the
%  relative important, utility and lifetime of data objects;
%\item Partitioning of data objects across the storage hierarchy utilizing
%  the additional knowledge embedded in the annotations;
%\item Evaluating the tradeoffs at runtime to guide data placement, movement,
%  and migration across storage layers using models, heuristics and
%  continuous learning;
%\item Utilize the additional knowledge available about the data to perform
%  application-aware data compression and I/O prioritization;
%\end{enumerate}
%
%We are aiming to spread an output across the vertical layers of the storage
%hierarchy simultaneously. When data should migrate to a higher tier, what
%happens to the existing version?
%
%Is there a canonical copy that is the originally written version?
%
%Is there annotation about this (I think so) attached to that part of the
%variable?
%
%At the tape layer, we see having a lossy compressed version just above the
%tape layer used as a directory/index and then the ``full'' resolution
%version on tape only retrieving once the user accepts the time/quality
%tradeoff. This is going to require some serious language to describe. If we
%have 100s PB of tape space, we'll need 1s PB of disk even at a 99\% data
%reduction. That is non-trivial.
%
%What happens when that data is retrieved from tape? Is a third copy made in
%an appropriately sized/quality level version for the tier it is requested to
%be pulled to? What if it is just the index version? Or the on tape version?
%Does it have a TTL because of the pain retrieving it from tape?
%
%Do we specify a target tier or just allow the system to place it in a place
%that makes sense? Given the potential space limitations, I think this is
%pretty critical because it could cause evictions or other insufficient space
%actions that are unintended. I can see us specifying that when you ask for
%data to be pulled at a particular quality level to a particular performance
%tier that it is a best effort with the use specifying if compromise or just
%failure is the result if the request cannot be fulfilled. Making this
%interaction make sense is going to require some reasonable thought and
%testing. We'll probably need to run it past apps people to get their
%feedback too as I don't think we are able to give a solid answer without
%some broad input.
%
%How does the migration to ``data lakes''/``campaign storage'' work? How does
%the migration to tape ultimately happen? I think we need to incorporate some
%explicit staging commands to move data both up and down the stack along with
%some variability in placement based on data features and current system
%state.
%
%Given the storage scarcity, particularly for NVM, I think we need to have a
%solid story here as part of the proposal. Yes, there are questions that have
%to be answered to build this still, but we need to have some pretty solid
%clarity where possible. I don't feel like I can explain it well enough at
%what I believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
