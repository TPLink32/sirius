\section{Introduction} 
\label{sec:introduction}

{\em The purpose of computing is insight, not numbers}~\cite{Hamming:book}.
Richard Hamming's remark, made over 50 years ago at the dawning of the age of
large scale scientific computation, is even more relevant today as we prepare
for scientific simulation at exascale. 
The central objective of this proposal, ``minimizing the time to insight''  
with respect to scientific computing, aligns perfectly with this sentiment. 
%Hamming was a pioneer in the fields of coding theory, scientific computing and 
%information theory who would instantly sympathize with the central objective of 
%this proposal--minimizing the time to knowledge.  

Avoiding impending bottlenecks in exascale scientific discovery will require
new research into managing, storing and retrieving the large volumes of data that are 
produced by simulations and analyzed for months afterwards.
%
In this project we will demonstrate a cooperative approach for storing and retrieving  data
where the user and the storage system work together to optimize performance with
respect to user requirements, current system state, and characteristics of the tiered-storage system.
Our investigation will include  novel techniques for the efficient mapping of data objects
from the user space to a hierarchical tiered storage layer. These techniques will
include support for application-guided data reductions with user-defined error bounds
and transformations to optimize performance, capacity and minimize bandwidth bottlenecks.

% 
Our proposed research program is guided by the following overarching objectives:
\begin{tightItemize}
\item
{\em Address emerging storage architecture and technology landscape:}
We will address the associated I/O and storage challenges in the context of
current and emerging storage landscapes,
% and expedite insights into critical
%scientific processes, while demonstrating the validity of our approach in key
%DOE domains. 
%We will research techniques for creating and using a Scalable 
%Storage Software Infrastructure, and integrating services from the middleware 
%layer with Storage System capabilities to support 
and will develop novel strategies for distributing 
data both horizontally and vertically across the layers of these storage systems.  
Negotiation between these layers will be provided as a fundamental service, allowing users to
easily save ``the best''  data, which is automatically and appropriately placed within the
 storage stack.
\item
{\em Develop a science driven, system aware approach:}
%Our approach targets the expected characteristics of exascale storage hardware.
%The storage layer will be partitioned into multiple heterogeneous tiers with
%vastly different performance characteristics. The layers will be further
%differentiated by the constraints on capacity and data lifetime within each
%layer. Tape archival storage will still maintain data long term, but access to
%this data will be orders of magnitude slower than the next layer and naively
%accessing data from archival storage will greatly impact productivity.
%
Given that storage systems will be composed of multiple heterogeneous tiers with
vastly different performance characteristics and constraints on capacity and 
supported data lifetime, our approach will enable researchers to 
place more knowledge into the Storage Systems and Input/Output (SSIO) system 
so that data can be recast from a sequence of bytes to information rich data. This 
information rich data can then be used by the system to guide the  placement within 
the tiered storage and will optimize the time to gain insight from 
exascale simulations.

%The traditional approach to transforming results of scientific computation into
%knowledge consists of storing the numerical output of a simulation on disk,
%followed by a post-processing or analysis stage that might take place over
%several ensuing weeks or months. Clearly, given the computation expected at the
%exascale, this type of workflow is simply not viable.

\item
{\em Emphasize impacts on science:}
%The metric we are most interested in optimizing is time to knowledge.  
%Current approaches to addressing the I/O bottleneck fall into two broad categories:
%parallel file system approaches that optimize the throughput for an entire
%system, and I/O middleware approaches that optimize the performance of a single
%application. Both approaches have been successful to date, but are unlikely to
%overcome the remaining obstacles in reaching exascale. 
Instead of trying to optimize throughput, we will seek to reduce the time to insight.
As extracting knowledge is the only metric that matters for scientific applications, we will
focus not only on data storage, but also on efficient retrieval of knowledge from their 
workflows.
Moreover, we aim to perform the optimization not only for a single
application, but rather for multiple workloads running on a multi-user, multi-application
system.
\end{tightItemize}

%%% Manish: I am not sure I fully understand the first part of the paragraph
%%% below.
The proposed work explores how application-specific information about the
importance of data will be used in conjunction with system and
resource-specific information to optimize the scientists' experience in
interacting with, and extracting knowledge and insight from their data.
%
%The overarching idea of this proposal is that data from applications can be characterized by different
%%levels of importance and different 
%levels of importance over time, which then can be used to guide the
% analysis and visualization later to access the most important data and
% deliver some level of knowledge without waiting for access to the whole
% original dataset. Information about the importance of the data at the
% application level, information about resource status and performance of
% the multi-level storage system, and flexible data reorganization and
% reduction operations at all levels can be utilized to provide a quality of
% service and allow users to gain insight in a timely manner.
The project combines knowledge and expertise 
from a wide range of sources including many leadership class applications using the Adaptable Input/Output System (ADIOS) framework,
the Sandia National Laboratory (SNL) Lightweight File Systems (LWFS) project and storage system knowledge, the metadata
management and storage systems knowledge at UCSC, and the middleware expertise
at Rutgers. We will integrate this diverse expertise to investigate how to create
a data, workflow, and application aware storage and I/O system, which we refer to as
{\bf SIRIUS}.
%This project brings together a team with strong expertise in I/O middleware (ORNL, Rutgers),
%file system (SNL, UCSC) and storage (UCSC), and connects and
%coordinates these key storage components in a seamless fashion.
%
%\section{Scalable Storage Software Infrastructure}
%Our focus in this proposal is in Theme 2: {\em Scalable Storage Software Infrastructure} 
%We will focus on a state of the art SSIO solution to solve problems which researchers are 
%beginning to face and will make science on exascale systems
%problematic without new insights and solutions.
Our proposal will be responsive to Theme 2: {\em Scalable Storage Software Infrastructure} of the call, as summarized in Table~\ref{tab:call}.

%
\paragraph{An Illustrative Example}
We use an illustrative fusion simulation use case that our team as been working on to highlight the challenges addressed in this proposal, and to 
%The challenges addressed by this proposal are illustrated by a problem our team has been working on in the past six months. We use this motivating example to 
formulate a series of SSIO research questions that must be addressed in order to support exascale science.

The motivating use case is a series of simulations of the
ITER~\cite{dietz1996iter} fusion experiments using the
XGC1~\cite{chang2006integrated} application. XGC1 is one of the largest
applications used at the Department of Energy's (DOE's) Leadership Class
Facilities, with an allocation of over 300M hours. 
% % simulations
% % simulations planne
% %that C. S. Chang (PPPL), 
% one of the largest users of Leadership Class Facilities (LCFs) (over 300M hours at DOE facilities), planned to launch.
These simulations were expected to produce 100~PB of data over 10 days of
total runtime on the Titan system at the Oak Ridge Leadership Computing
Facility (OLCF), and required a team of experts including
the user group and I/O and storage personnel to help ensure that the maximum
amount of information would be saved. Due to physical resource limitations,
the simulation was immediately restricted to write about 10~PB. However,
when the time to write and read, together with the financial cost to archive
this volume of data was fully explored, the data size goal was further
reduced to only 5~PB over the 10 day run. 

This forced a careful scrutiny of the large amount of data to be generated
and restriction of the output to only the most important pieces. Further,
this reduced data set had to be divided into two categories: first, the data
that would be accessed while the simulation was running or shortly
thereafter, and second, the data that would be needed after the whole
campaign was finished. The former category of data would be stored on disk,
and the latter would be archived. Additionally, the entire team needed to
figure out how to efficiently retrieve these different categories of data
for post-processing, analysis and visualization.

To accomplish this, the application and storage teams developed new
application specific data reduction techniques and added them to the ADIOS
I/O middleware layer. A rudimentary discovery system was also created to
track what data was on disk and what was on the tape archive. Initially this
solution was sufficient, but eventually a new problem was introduced:
Although the team had reserved resources for the run, the entire file system
was shared \cite{shipman2012next} resulting in very high I/O variability
\cite{lofstead:2010:io-variability,liu_hotstorage}. Addressing this variability, caused by
contention on shared resources, is an ongoing concern. 

The problems faced by this user workflow motivate our research here. The use
case demonstrated the three tiers of solutions that are required -
application level knowledge of data and how it will be used, middleware
management of data and resources, and storage system level scheduling of
resources. It also provided us with new insights: First, the users and developers
of the application data can, without a great loss of encoded knowledge,
reduce the size of data that needs to be stored. Second, data storage must
be managed with input from the users to correctly decide the target of
storage operations. Third, shared resources must be managed in a way that
provides sufficient performance to all applications without encumbering the
applications with high variability in performance. 
These insights drive our proposed research approach. 

% Finally, the team had to develop new application specific data reduction
% techniques and encode them into the ADIOS middleware I/O layer. In order to
% do this, they  implemented a rudimentary discovery system to determine
% which data was located on disk, and which was on the tape archive. This
% solution worked well  initially but after the second day significant
% problems occurred: I/O variability became prohibitive. Although
% the team had reserved resources for the run, the file system was shared, and
% a 10X variability in I/O speeds was observed.



% What is needed here is a system to monitor, predict and manage resource
% usage in a dynamically adaptable way. Such a system can write more data when
% the resource was idle, and less data when it is busy. However, the most
% significant problem occurs after the simulation run completes. Basic
% visualizations, to get a high-level view of the simulation, can take
% significant time due to data access latencies. While some timesteps might be
% on disk, others are archived on tape (where transfers could take weeks).
% Detailed visualization and analysis of the simulation data are even more
% complicated.



\subsection{Overview of the proposed approach} 

\begin{table}[htbp]
\small
\vspace{2ex}
\begin{center}
\caption {Relevance to SSIO}
\label{tab:call}
\begin{tabular}{ | p {0.2cm} | p{5cm} | p{7.5cm} | p{2.2cm} |}\hline
\multicolumn{2}{|c|}{\bf Call area} & \multicolumn{1}{|c|}{\bf Relevant SIRIUS work} & \multicolumn{1}{|c|}{\bf Proposed research} \\\hline\hline
1 & Data abstractions & Create new APIs  and annotations to guide data placement across the storage system & \S\ref{sec:data-description} \\\hline
2 & Mapping science data models onto hierarchical storage &  Enable science-driven mapping, re-organization, reduction and prioritization of data chunks derive from application objects & \S\ref{sec:data-refactor}, \S\ref{sec:data-description}, \S\ref{sec:managing-data-life}\\\hline
3 & Mechanisms for data movement across the storage hierarchy & Enable science-driven data placement and migration based on policies& \S\ref{sec:managing-data-life}, \S\ref{sec:migration} \\\hline
4 &  Extend the interaction of middleware with the storage system & Manage data placement and migration between the application, middleware and storage system using an autonomic cross-layer approach & \S\ref{sec:managing-data-life}, \S\ref{sec:qos}, \S\ref{sec:naming-discovery}  \\\hline
5 & Design and implement of I/O middleware architectures  & Create APIs to guide the placement of chunks of data across the SSIO and autonomically move data& \S\ref{sec:data-description}, \S\ref{sec:managing-data-life}, \S\ref{sec:qos}, \S\ref{sec:naming-discovery} \\ \hline
6 &Approaches to improve the ability of SSIO software to support checkpoint/restart &  We will have utility functions defined for the lifetime of data, and allow CR data to be automatically placed in the fastest storage and then later purged & \S\ref{sec:data-description}, \S\ref{sec:managing-data-life}, \S\ref{sec:qos}, \S\ref{sec:data-description} \\ \hline
\end{tabular}
\end{center}
\vskip -0.5cm
\end{table}

%ADIOS was developed with the understanding that we must
%not only address the bottlenecks for current applications and
%hardware platforms but also provide a path forward for the next
%generation of applications and systems that would need to both
%maximize bandwidth to the storage system and also support
%transparently working around the storage system bandwidth
%limitations with new techniques and tools. To support the
%diverse operating modes of both using persistent storage and
%other data storage and processing technology, we made a
%great effort to provide a simplified interface to application
%developers, offering a simple, portable and scalable way for
%scientists to manage data that may need to be written, read
%or processed during simulation runs. This required abstracting
%away many decisions typically made in the application code
%so that they may be configured externally.
%In addition to this abstracted interface with external configuration
%options, common services were incorporated to afford
%optimizations beyond those for a single platform, such as
%buffering, aggregation, subfiling, and chunking with options
%to select each based on the data distribution characteristics
%of the application. A variety of asynchronous I/O techniques
%have been investigated and are being integrated with ADIOS.
%A recognition of application complexity has led to new techniques
%for testing I/O performance by extracting I/O patterns
%from applications and automatically generating benchmark
%codes. And finally, the march toward exascale has fueled the
%need to consider additional features such as compression and
%indexing to better cope with the expected tsunami of data.
%
%Overall this project deals with I/O and storage problems
%for both checkpoint/restart (C/R) and post-processing.
This proposal aims to address core I/O and storage challenges faced by 
extreme scale simulation workflows for typical I/O patterns including 
checkpoint/restart (C/R), in-situ analysis, and post-processing.
%
Furthermore, we examine these research challenges from the perspective of:
%
(1) scientists who run exascale simulations,
%
(2) the Storage System and I/O layer, which needs to negotiate between all of
the users, and finally
%
(3) users of the system who attempt to understand the data
produced by a set of simulations.

\paragraph{Simulation Perspective}
\label{subsec:sim-perspective}
A simulation scientist should ideally be able to
obtain an estimate of how much time will be required to write data,
and how much storage space is available during the simulations lifetime.
Further, they would like Quality of Service (QoS) assurances, and that
issues such as varying bandwidth can be handled appropriately and 
automatically. For example, a set of user-specified rules would allow 
the system to make autonomic decisions at runtime
(\S\ref{sec:managing-data-life}, \S\ref{sec:qos}).
%
Our approach is based on giving users the ability to define the utility of
their data (\S\ref{sec:data-description}). This includes the ability to assign
varying levels of importance to individual data objects in their entirety,
or to specific subsets and pieces. Key research challenges include: (1)
developing an expressive and easy to use a program interface, (2) formulating
a system model that can support autonomic decision, for example, for saving
data based on user-defined importance and utility of
data (\S\ref{sec:managing-data-life}), and (3) defining the interface between
the system and the back-end storage for both single and multiple running
applications (\S\ref{sec:qos}).

The scientist will be able to specify a set of rules that
would guide I/O and data management at runtime. For example, if system
resources are available and the estimated write times acceptable, the write
can proceed, but if this is not the case, the write may be postponed to a
later time-step, or the data reduced using an in-situ techniques before it
is saved. Data ``importance or utility'' can be used to place various data
at proper SSIO layers -- data with the highest utility (e.g., based on user
defined importance or the frequency of access) is maintained in faster
storage for long periods of time, while data with lower utility can be
migrated to lower layers in the storage hierarchy.

%
%Since the data will be a collection of objects placed on the storage system,
%some of the objects will be broken up into multiple pieces by ``plug-ins''
%to the system which will allow data to be characterized first in terms of the most
%important pieces, and then by subsequent levels of detail. We can think of this as
%something similar to an Adaptive Mesh Refinement scheme which keeps track of
%the places where there are features such as steep gradients, which contain large errors on the
%coarser views, as opposed to other areas with smaller errors. These distinctions allow us to make decisions
%about which sub-objects to save (all of the data, which will then go to
%different layers) or just some of the sub-objects? The research questions
%are numerous: How does the user specify their intentions? How does the user
%prioritize the different sub-objects of the data? Where does the data go
%into the storage hierarchy? Does the data get replicated in the lower tiers
%of the hierarchy? When the user specifies the time they want the data saved,
%how does this get into the storage system?

\paragraph{Storage System Perspective}
\label{subsec:storage-perspective}
From the perspective of the storage system, it is necessary to manage a
single I/O request from a user in the context of all requests from all
users, and try to optimize the entire system accordingly. This entails
providing quality of service to the user to minimize
resource contention (\S\ref{sec:qos}), distribute the data across all of the tiers of storage
in a known consistent manner (\S\ref{sec:init-plac-data}), and automatically move data from one part of
the storage hierarchy to another (\S\ref{sec:migration}). 

One of our research objectives is to understand how the
storage system can interact with the middleware   so
that the data with the highest utility is kept on the fastest layers of the
storage system for as long as specified. We need to maintain 
QoS and therefore, be able to provide fuzzy estimates of the time it will take to write/read data from
the different layers to inform any migration/eviction decisions, and it
is also critical that the storage system can manage (discover and retrieve) the data across all of the
layers automatically, without user intervention. 
Our approach here is that the storage system needs to define bounds of time estimates
(min time, max time), and as our system evolves, we want to allow plug-ins which can help
reduce the range of the bounds.
Overall, our goal is to enable predictable performance acorss the storage system, and we will develop mechanisms to enable this
(\S\ref{sec:naming-discovery}).

%We will use learning techniques to automatically migrate data
%sub-objects across the hierarchy as well as to eventually evict data which is
%large and has very low utility.


\paragraph{Reading  Perspective}
\label{subsec:reading-perspective}

From the reading perspective, the data must be accessible in a fast and
predictable manner for all read access patterns, with different access
patterns having their own requirements for latency, throughput and
consistency. Here the user must be able to describe the read patterns and
query for performance estimations (\S\ref{sec:data-description}), be able
to quickly access the most important data (\S\ref{sec:data-refactor}) and recieve
performance gurantees (\S\ref{sec:qos}) on the time required to complete
the read request. The challenge here is to address the variety of read
patterns that drive knowledge discover. 

For example, consider the case of Checkpoint/Restart (C/R) which needs a lot
of data at high throughput to complete the restart in case of application
failure, with latency being less important due to the batch nature of the
request, and the data must be of sufficient quality to support an
application restart without significant errors. 
This is in contrast to interactive analysis, where the most important metric
is the latency to start the analysis process. The quality here is more
variable and based greatly on the expectations of the user, but performance
predictability is important due to the interactive nature of the process. 

Our research objective here is to explore the interface that reading code
can utilize to discover the important properites of data, the expectations
about the time to access the data, and make decisions on the quality of data
required to complete the task. 

%
%Exascale scientific discovery will be severely bottlenecked without
%sufficient new research into managing and storing the large amounts of data
%that will be produced during the simulation, and analyzed for months
%afterwards.
%%
%In this project we will demonstrate novel techniques to facilitate efficient
%mapping of data objects, even partitioning individual variables, from the
%user space onto multiple storage tiers, and enable application-guided data
%reductions/transformations to address capacity and bandwidth bottlenecks,
%while constraining the error to be within user provided bounds.
%%
%We will address the associated I/O and storage challenges in the context of
%current and emerging storage landscapes, and expedite insights into critical
%scientific processes, demonstrating the validity of our approach in key DOE
%domains. Our techniques will be to research novel techniques into a Scalable
%Storage Software Infrastructure by integrating services from the middle-ware
%layer which will talk to the applications, with the Storage system. The
%negotation between these layers will be a fundamental service which we will
%create in this project to ensure that users will be able to save `'the
%best'' amount of data in the different storage tiers.
%
%The metric we are most interested in optimizing is time to knowledge.
%Current approaches to addressing the I/O bottleneck fall into two broad
%categories. Parallel file system approaches that optimize the throughput for
%an entire system, and I/O middleware approaches that optimize the
%performance of a single application. Both approaches have seen success but
%are unlikely to overcome the major obstacles in reaching exascale. Instead
%of trying to optimize throughput, we will seek to reduce the time to
%knowledge. This is the most significant metric for scientific applications
%where the desired outcome is not storing data, but rather executing a
%knowledge extraction process on the data. Moreover, we aim to perform the
%optimization not for a single application, but rather for workload of a
%multi-user multi-application system environment.
%
%Our approach leverages the expected characteristics of exascale storage
%hardware. The storage layer will be partitioned into multiple heterogenous
%tiers with vastly different performance characteristics. This difference
%between layers will be further exacerbated by the constraints on capacity
%and data lifetime within a layer. Tape archival storage will still maintain
%data long term, but access to this data will be orders of magnitude slower
%than the next layer and naively accessing data from archival storage will
%greatly impact productivity.
%
%We will achieve reduced time to knowledge using a combination of techniques;
%\begin{enumerate}
%\item Data annotations specified at the application level to quantify the
%  relative important, utility and lifetime of data objects;
%\item Partitioning of data objects across the storage hierarchy utilizing
%  the additional knowledge embedded in the annotations;
%\item Evaluating the tradeoffs at runtime to guide data placement, movement,
%  and migration across storage layers using models, heuristics and
%  continuous learning;
%\item Utilize the additional knowledge available about the data to perform
%  application-aware data compression and I/O prioritization;
%\end{enumerate}
%
%We are aiming to spread an output across the vertical layers of the storage
%hierarchy simultaneously. When data should migrate to a higher tier, what
%happens to the existing version?
%
%Is there a canonical copy that is the originally written version?
%
%Is there annotation about this (I think so) attached to that part of the
%variable?
%
%At the tape layer, we see having a lossy compressed version just above the
%tape layer used as a directory/index and then the ``full'' resolution
%version on tape only retrieving once the user accepts the time/quality
%tradeoff. This is going to require some serious language to describe. If we
%have 100s PB of tape space, we'll need 1s PB of disk even at a 99\% data
%reduction. That is non-trivial.
%
%What happens when that data is retrieved from tape? Is a third copy made in
%an appropriately sized/quality level version for the tier it is requested to
%be pulled to? What if it is just the index version? Or the on tape version?
%Does it have a TTL because of the pain retrieving it from tape?
%
%Do we specify a target tier or just allow the system to place it in a place
%that makes sense? Given the potential space limitations, I think this is
%pretty critical because it could cause evictions or other insufficient space
%actions that are unintended. I can see us specifying that when you ask for
%data to be pulled at a particular quality level to a particular performance
%tier that it is a best effort with the use specifying if compromise or just
%failure is the result if the request cannot be fulfilled. Making this
%interaction make sense is going to require some reasonable thought and
%testing. We'll probably need to run it past apps people to get their
%feedback too as I don't think we are able to give a solid answer without
%some broad input.
%
%How does the migration to ``data lakes''/``campaign storage'' work? How does
%the migration to tape ultimately happen? I think we need to incorporate some
%explicit staging commands to move data both up and down the stack along with
%some variability in placement based on data features and current system
%state.
%
%Given the storage scarcity, particularly for NVM, I think we need to have a
%solid story here as part of the proposal. Yes, there are questions that have
%to be answered to build this still, but we need to have some pretty solid
%clarity where possible. I don't feel like I can explain it well enough at
%what I believe to be a proper clarity level at this point.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
