\subsection{Data Refactoring}



It is useful to classify scientific data into two basic categories: structured
data (in the sense that it satisfies a known or relatively simple model) or
unstructured data (in which information content follows no obvious or explicit
model).  Although scientific data generally contains random components (due to
finite precision, measurement and calibration effects etc.), scientific data is
never purely random. Broadly speaking, the path from data to knowledge consists
of extracting underlying models or patterns from the datasets, and interpreting
the resulting models. 

Ideally, the scientist would perform the analysis stage {\em in situ} to
extract the relevant information and, in the unlikely event of this being
possible, would effectively circumvent the large data issue completely.  The
problem of course is that this is unlikely to be possible since, by their
nature, large scale simulations aim to discover new information that is often
hidden as higher order effects amongst the data deluge. In particular, this
means if data thinning or truncation is applied then one is in danger of
effectively throwing out the baby with the bath water. One cannot store the
entire data set in easily accessible source, due to sheer size, yet one cannot
reduce it prior to archival without risking losing the information one is
trying to extract.  

Viewed in this way, the problem would appear intractable. However, the above
discussion fails to recognize that much of the data is redundant in the
information theoretic sense.  That is to say, the amount of information
contained in the data is often significantly less than the amount of data.  The
difficulty stems from the fact that one does not know about this redundancy in
advance. 


We propose a fresh approach to tackle the data overload problem head-on through
using preprocessors in which information is not simply stored in the form of
raw data, but rather in the form of appropriate parameterized algorithms. A
priori information on the nature of the data is used to choose the basic
algorithm while the actual values of the data are used to tune the parameters
used in the reconstruction procedures. We shall refer to this paradigm as an
Auditor.

Scientific data is sometimes described as essentially incompressible owing to
noise and other random effects present in the data, stemming from the numerical
algorithm, software implementation, system specific behavior etc. The reasoning
behind the description lies in the mathematical fact that it is impossible to
achieve lossless compression on completely random data sets \cite{Gray:book}. Even
recent state of the art lossless compression approaches \cite{GomezCappello} are only
able to obtain 15\% improvement in lossless compression ratio while reducing
the time spent in compression of floating point data by one half in some cases.
Nevertheless, these limitations apply only if one ignores two key features:
Firstly, randomness in scientific data generally occurs only below some
threshold related to the accuracy of the measurement apparatus or the numerical
algorithm. Secondly, lossy compression is perfectly acceptable provided the
loss level falls below this threshold.

Data compression broadly consists of three basic steps: (1) Pre-processing to
extract as much structure as feasible. This step is variously referred to as
filtering, preconditioning, and pre- processing but in all cases, the objective
is to represent the data in a form that becomes more amenable to compression.
(2) Removal of redundancy such as omission of duplicates etc.  Finally, (3) the
resulting data is compressed using entropy coding. The level of compression
that can be achieved in the final step is limited by the Shannon information
content of the data \cite{Gray:book}, and many standard tools are available for
efficient and perfect entropy coding. The potential for compression through
removal of redundancy is generally minor. The main scope for achieving
significant compression (a compression rate of two or better say) therefore
lies in the pre-processing step.

Research in the construction of pre-processing techniques is still in its
infancy, especially in context of exascale simulations. Techniques that have
been advocated range from simple difference filtering, bit-sorting, prefix
transformation through to simply reordering the data.  What these, and other
catchall methods, fail to exploit is a priori information on the nature and
provenance of the data.

As we have already observed, the incorporation of a priori information of this
type is vital if one is to obtain significant data reduction. The use of an
Auditor as a pre-processor exploits the wide (and increasing) cost gap between
CPU operations versus storage-based operations. The preprocessor would ideally
be employed in situ and is used in a similar fashion to pre-processors in
standard compression approaches such as FPC \cite{BurtscherFPC} in which an XOR
operation against the previous data is used as a pre-processor. The fact that
even such a crude pre- processor is able to deliver reasonable results augurs
well: Even a relatively crude Auditor can be expected to significantly
out-perform the XOR and other similar approaches.





