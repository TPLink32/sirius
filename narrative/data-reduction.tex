\subsection{Data Refactoring} \label{sec:data-refactor} 

The classical workflow whereby the entire dataset is written to storage for
later analysis is no longer viable owing to the sheer amount of data being
generated. In future, it will be vital to take advantage of a priori
information when writing and reading from the SSIO layer in order to gain
performance and predictability, and to prioritize I/O so that the data that
is most useful for end-user can be written in the time available. 

In the case of C. S. Chang described earlier, in commmon with most other
applications scientists with whom we collaborate, information is provided on
which data should be sent over to our SSIO layer in order that the best
possible information is available the subsequent analysis. The decision as how
this should be accomplished generally involves refactoring (re-organizing and
reducing the data). 

In this section we describe some common methods to reduce and re-organize data.
There are many possibilities and the best choice will generally vary from case
to case but, once settled, will not change from run to run. It will be
important understand when the time and resources to required to identify and
perform the ``best methods'' outweighs the eventual gains. Another critical
research question concerns the quantification and control of the loss of
information resulting from refactoring the data and using a reduced 
dataset. 

A basic issue allied to refactoring is to understand how much information is
actually present in a dataset and whether a refactoring based on a reduced
order representation might prove effective. It is useful to classify scientific
data into two basic categories: structured data (in the sense that it satisfies
a known or relatively simple model) or unstructured data (in which information
content follows no obvious or explicit model).  Although scientific data
generally contains random components (due to finite precision, measurement and
calibration effects etc.), useful scientific data is never purely random.
Broadly speaking, the path from data to knowledge consists of extracting
underlying models or patterns from the datasets, and interpreting the resulting
models. 

Ideally, the scientist would perform the entire analysis stage {\em in situ} to
extract the relevant information and, in the unlikely event of this being
possible, would effectively circumvent the large data issue completely.  The
problem of course is that this is unlikely to be possible since, by their
nature, large scale simulations aim to discover new information that is often
hidden in the form of higher order effects amongst the data deluge. In
particular, this means if data thinning or truncation is applied willy nilly
then one is in danger of effectively throwing out the baby with the bath water.
One cannot store the entire data set in an easily accessible source, due to
sheer size, yet one cannot reduce it prior to archival without risking losing
the information one is trying to extract.  Viewed in this way, the problem
would appear intractable. However, the above discussion fails to recognize that
much of the data is redundant in the information theoretic sense.  That is to
say, the amount of information contained in the data is often significantly
less than the amount of data.  The difficulty stems from the fact that one does
not know about this redundancy in advance \emph{without the benefit of 
priori knowledge}. This knowledge can often be provided by the user or can 
be acquired through experience of dealing with differing runs of the same
code. 

Application knowledge means one can sometimes achieve drastically superior
reduction in the total data compared with what one might achieve otherwise.
However, in the absence of such knowledge, we must equip the SSIO layers with
generic data reduction techniques and re-organization techniques.
Nevertheless, certain basic information on the semantics of the data is needed 
and must be supported by the overall infrastructure. 

Another important consideration is knowledge regarding typical access patterns
for data in a given application. Scientific applications often read and analyze
data in a structured fashion and failure to respect this pattern, or to exploit
it through prefetching, can significantly increase the overall execution time
of the application. Accurate prefetching can only be achieved with access
pattern analysis. This requires an SSIO layer which can prefetch data in a
asynchronous fashion to take the bytes on the file.

There are many methods to re-organize the data, and to reduce the data and in
this section we will list many different known methods.  We then list our
different approaches to address the many research challenges using many well
known methods, and a new method that we have been working on, known as an
``auditor'' which can be used ``in situ'' with the calculation during the data
generation, and then used to dramatically reduce the impact on the SSIO layer
for both reading and writing.


\paragraph{State of the art:} {\bf Data Reduction Techniques} Data compression
broadly consists of three basic steps: (1) Pre-processing to extract as much
structure as feasible. This step is variously referred to as filtering,
preconditioning, and pre- processing but in all cases, the objective is to
represent the data in a form that becomes more amenable to compression.  (2)
Removal of redundancy such as omission of duplicates etc.  Finally, (3) the
resulting data is compressed using entropy coding. The level of compression
that can be achieved in the final step is limited by the Shannon information
content of the data \cite{Gray:book}, and many standard tools are available for
efficient and perfect entropy coding. The potential for compression through
removal of redundancy is generally minor. The main scope for achieving
significant compression (a compression rate of two or better say) therefore
lies in the pre-processing step.

Research in the construction of pre-processing techniques is still in its
infancy, especially in context of exascale simulations. Techniques that have
been advocated range from simple difference filtering, bit-sorting, prefix
transformation through to simply reordering the data.  What these, and other
catchall methods, fail to exploit is a priori information on the nature and
provenance of the data.

As we have already observed, the incorporation of a priori information of this
type is vital if one is to obtain significant data reduction. The use of an
Auditor as a pre-processor exploits the wide (and increasing) cost gap between
CPU operations versus storage-based operations. The preprocessor would ideally
be employed in situ and is used in a similar fashion to pre-processors in
standard compression approaches such as FPC \cite{BurtscherFPC} in which an XOR
operation against the previous data is used as a pre-processor.

Our previous
work~\cite{lakshminarasimhan2011compressing,lakshminarasimhan2011compressing,%
gong2012multi,jenkins2012byte,gong2013parlo,boyuka2014transparent,%
tang2014improving}
has focused on many of these well known techniques to not only reduce the data
but also re-organize the data. Previously we have examined:


\begin{itemize} 
	
\item Precision based re-organization. This is where the most significant bytes
	of the data are all group together from each object. This data
	generally will have a higher utility then the data with the least
	significant bytes.  This process has the following three steps which
	need to be efficiently implemented when data gets placed to the storage
	system and from the storage system.  First, the data needs to be
	re-arranged which involves memory operations and needs to be done
	in-situ. This operation involves no communication.  Data needs to be
	copied, so the memory requirements are increased. At most this requires
	two copies an individual dataset in memory, and we will investigate
	techniques to allocate and deallocate this memory if the user will
	specify that the data will be overwritten after it is written to the
	storage system. This is often the case for many of the data quantities
	written form the simulation, but there are many cases where, for
	example, we want to write all of the particles from a Particle In Cell
	(PIC) simulation.  Since the particles will be used later in the
	calculation, we need to duplicate the storage. Our observation with
	working with the XGC1, GTC, Warp, and PiconGPU simulations (all
	leadership class simulations) is that we can temporary increase the
	storage of the particles, and then release them since the temporary
	arrays used in the calculations are often freed when a PIC iteration is
	finished. 
%
\item Frequency-based re-organization.  Another common technique to classify
	the importance of data is to first re-organize the data in frequency
	space and then specify that as the frequency increases the importance
	of data decreases.   Common technique which is used for streaming data,
	and data reduction techniques such as JPEG-2000~\cite{jpeg2000} is to
	code streams which have regions of interest that offer several
	mechanisms to support spatial random access or region of interest
	access at varying degrees of granularity. It is possible to store
	different parts of the same data using different quality.  This allows
	us to place the lowest frequency pieces in the fastest storage, and the
	highest frequency either on the slowest storage tiers, or if the data
	sizes are prohibitively  costly, not even write out the highest
	frequency. This binning of data in frequency space can take as an input
	(storage bandwidth knowledge, storage size limitations, and user
	intentions of how long the majority of the data will be frequently
	accessed.  In order to fully take advantage of frequency based
	re-organization we have devised a scheme where the data pre-conditioner
	first sorts the data in bins of 1024 words, and then we maintain the
	sort order, and then we used wavelets and spline fits to then
	re-organize the values.  This re-organization allows us to understand
	that the resultant data is smoother now, and can be reduced in each one
	of the bins.  \end{itemize}

\paragraph{Proposed research approach:} We will be looking at a multi-throng
approach to data-refactoring. One approach allows the user to specify
information about their data to indicate if the data is spatially varying,
space-and time varying, just time varying. This allows us to store information
in NVRAM on each node (when and where necessary) to later preform compression
in both space and time . We will examine the different techniques for
compression and re-organization along with examine techniques to use an
error-estimator (gradients) to create a multi-resolution view of the data which
can then be sampled, and placed in different quantiles of importance. These
will then be mapped over into the different layers of the storage hierarchy.
We will also investigate technique which can use ``plug-ins'' for detection of
features and store higher resolution data around the feature and lower
resolution values which are further away from the features. This
re-organization must occur without increasing the total amount of storage than
what the users intended. We will also investigate the time it takes to
re-organize the data and use the multi-layer approach to staging to understand
where the data gets reduced and where it gets re-organized. Open questions are:
1) do we move the data over to other nodes to then reduce and re-organize, 2)
do we first reduce the data and then later re-organize this data on different
nodes?  3) do we use different cores on a node to do data-refactoring.
Similarly we will examine the cost of reading the data. It is clear that if we
only read in the high-importance data it should improve the overall time to
move the data from the SSIO layer to the application, but what happens when
users want all of the data? There is an obvious penalty of having to take data
in a hierarchical layout and then place this back to a single-layer view.
Additional research questions which must be addressed: Will some types of data
(PIC data, finite element data, ...) be be conducive  for data-refactoring than
others? How much semantic-domain knowledge will we need to get a dramatic
improvement of the storage system over not doing anything special? 

We will measure these methods using the fusion XGC simulation and the
corresponding analysis routines which they use for their largest simulations.
We will also work with Jeroen Tromp's SPECFM3D code, and the fusion GTC code
from Z. Lin. In all cases we will measure their impact from their current
simulations, and their projected simulations and analytics on exascale
machines. Since all three of these codes are part of the OLCF CAAR~\cite{CAAR}
project for early access, there is strong belief that the simulations will be
close to exascale simulations and we will be able to run these on prototypes of
our SSIO layer to examine the full impact of data-refactoring.

\paragraph{High Impact Challenges:} Since the greatest form of data reduction
on the SSIO layer would be to only write the code, and the input data, and then
recalculate all of the results, we want to understand how to write a minimal
set of information, and then place an auditor code in a docker container so
that data can be re-generated with an approximation of the calculation, but
only valid over a limited amount of time.This new approach to tackle the data
overload by using preprocessors in which information is not simply stored in
the form of raw data, but rather in the form of appropriate parameterized
algorithms. A priori information on the nature of the data is used to choose
the basic algorithm while the actual values of the data are used to tune the
parameters used in the reconstruction procedures. We shall refer to this
paradigm as an Auditor.

We will examine the use of several auditors for the simulations which we will
work with. One approach is to use a fluid-code to audit a kinetic code, which
can be a good approximation over short periods of time. Once there is a large
deviation (specified by the user) from the auditor to the original calculation,
the data will be saved. This is a local operation, since the deviations will
grow differently in space and in time. 

