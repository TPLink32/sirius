\subsection{Data Refactoring} \label{sec:data-refactor} 

The classical workflow where the entire dataset is written to storage for
later analysis is no longer viable simply because the amount of generated data is too large.
In the future, it will be vital to take advantage of a priori
information when 1) writing and reading from the SSIO layer in order to gain
performance and predictability, and to prioritize I/O so that the data that
is most useful for end-user can be written in the time available, and 2)
performing {\em in situ} operations and analysis before storing the information.

In the case of C. S. Chang described earlier, and common with most other
applications scientists with whom we collaborate, information is provided on
which data should be sent over to the SSIO layer so that the best, and most
important information is available for subsequent analysis.
This is generally accomplished using refactoring, which includes re-organization and reductions of the data.

There are many possibilities for refactoring and the best choice will generally
vary from case to case but, once settled, will typically not change from run to run. It
will be important to understand when the time and resources required to
identify and perform the ``best methods'' outweighs any eventual gains. Another
critical research question concerns the quantification and control of the loss
of information resulting from refactoring the data and using a reduced dataset. 

A basic issue allied to refactoring is to understand how much information is
actually present in a dataset and whether a refactoring based on a reduced
order representation might prove effective. It is useful to classify scientific
data into two basic categories: structured data (in the sense that it satisfies
a known or relatively simple model), or unstructured data (in which information
content follows no obvious or explicit model).  Although scientific data
generally contains random components (due to finite precision, measurement and
calibration effects etc.), useful scientific data is never purely random.
Broadly speaking, the path from data to knowledge consists of extracting
underlying models or patterns from the datasets, and interpreting the resulting
models. 

Ideally,  scientists would like to perform the entire analysis stage {\em in situ}
to extract the relevant information and in so doing would effectively
circumvent the large data issue completely.  The catch, of course, is that this
is unlikely to be possible since, by their nature, large scale simulations aim
to discover new information that is often hidden in the form of higher order
effects amongst the data deluge. In particular, this means if data thinning or
truncation is applied haphazardly then one is in danger of effectively throwing
out the information one is seeking. It would seem that one cannot store the
entire data set in an easily accessible source, due to sheer size, yet one
cannot reduce it prior to archival without risking losing the information one
is trying to extract.  Viewed in this way, the problem would appear
intractable. However, the above discussion fails to recognize that much of the
data is redundant in an information theoretic sense.  That is to say, the
amount of information contained in the dataset is often significantly less than
the amount of data.  The difficulty stems from the fact that one does not know
about this redundancy in advance \emph{without the benefit of priori
knowledge}. This knowledge can often be provided by the user or can be acquired
through experience of dealing with differing runs of the same code. 

Deep application knowledge means one can sometimes achieve drastically superior
reduction in the total data compared with what one might achieve otherwise.
However, even in the absence of such high level knowledge, we must equip the
SSIO layers with generic data reduction techniques and re-organization
techniques. For instance, certain basic information on the semantics of the
data is needed and must be supported by the overall infrastructure. Likewise,
information regarding typical access patterns for data in a given application
should be placed in the SSIO layers. Scientific applications often read and
analyze data in a structured fashion and failure to respect this pattern, or to
exploit it through prefetching, can significantly increase the overall
execution time of the application. Accurate prefetching can only be achieved
with access pattern analysis. This requires an SSIO layer which is capable of
prefetching data in an asynchronous fashion to take the bytes on the file.

\paragraph{State of the art:} {\bf Data Reduction Techniques.} Data compression
broadly consists of three basic steps: (1) Pre-processing to extract as much
structure as feasible. This step is variously referred to as filtering,
preconditioning, and preprocessing. But in all cases, the objective is to
represent the data in a form that becomes more amenable to compression.  (2)
Removal of redundancy such as omission of duplicates, etc.  Finally, (3) the
resulting data is compressed using entropy coding. The level of compression
that can be achieved in the final step is limited by the Shannon information
content of the data \cite{Gray:book}, and many standard tools are available for
efficient and perfect entropy coding. The potential for compression through
removal of redundancy is generally minor. The main scope for achieving
significant compression (e.g. a compression rate of two) therefore
lies in the pre-processing step.

Research in the construction of pre-processing techniques is still in its
infancy, especially in context of exascale simulations. Techniques that have
been advocated range from simple difference filtering, bit-sorting, prefix
transformation through to simply reordering the data.  What these, and other
catch-all methods fail to exploit is a priori information on the nature and
provenance of the data. 

Our previous
work~\cite{lakshminarasimhan2011compressing,%
gong2012multi,jenkins2012byte,gong2013parlo,boyuka2014transparent,%
tang2014improving}
has focused on several alternative techniques to not only reduce the data
but also re-organize the data. Previously we have examined:


\begin{itemize} 
	
\item Precision based re-organization. This is where the most significant bytes
	of the data are all grouped together from each object. This data
	generally will have a higher utility then the data with the least
	significant bytes.  This process has the following three steps which
	need to be efficiently implemented when data gets placed to the storage
	system and from the storage system.  First, the data needs to be
	re-arranged which involves memory operations and needs to be done
	in situ. This operation involves no communication.  Data needs to be
	copied, so the memory requirements are increased. At most this requires
	two copies of an individual dataset in memory, and we will investigate
	techniques to allocate and deallocate this memory if the user will
	specify that the data will be overwritten after it is written to the
	storage system. This is often the case for many of the  quantities
	written from the simulation, but there are many cases where, for
	example, we want to write all of the particles from a Particle In Cell
	(PIC) simulation.  Since the particles will be used later in the
	calculation, we need to duplicate the storage. Our observation with
	working with the XGC1, GTC, and PiconGPU simulations 
	 is that we can temporary increase the
	storage of the particles, and then release them since the temporary
	arrays used in the calculations are often freed when a PIC iteration is
	finished. 
%
\item Frequency-based re-organization.  Another common technique to classify
	the importance of data is to first re-organize the data in frequency
	space and then specify that as the frequency increases the importance
	of data decreases.   Common techniques  used for streaming data,
	and data reduction techniques such as JPEG-2000~\cite{jpeg2000} is to
	code streams which have regions of interest that offer several
	mechanisms to support spatial random access or region of interest
	access at varying degrees of granularity. It is possible to store
	different parts of the same data using different quality.  This allows
	us to place the lowest frequency chunks in the fastest storage, and the
	highest frequency chunks  on either  the slowest storage tiers, or if the data
	sizes are prohibitively  costly, not even write out  these pieces. 
%	This binning of data in frequency space can take as an input
%	(storage  {\color{red}ending )? CHECK-tsr} bandwidth knowledge, storage size limitations, and user
%	intentions of how long the majority of the data will be frequently
%	accessed. 
	 In order to fully take advantage of frequency based
	re-organization we have devised a scheme where the data pre-conditioner
	first sorts the data in bins of 1024 words, then we maintain the
	sort order, and then we used wavelets and spline fits to then
	re-organize the values.  This re-organization allows us to understand
	that the resultant data is smoother now, and can be reduced in each one
	of the bins.  \end{itemize}

\paragraph{Proposed research approach:} It is now widely recognized that any
opportunities for {\em in situ} analysis should be exploited as far as
possible. What is less well understood, is how one should incorporate a priori
information and how one can identify possibilities for {\em in situ} analysis.
A preprocessor would be employed in situ and used in a similar fashion to
pre-processors in standard compression approaches such as FPC
\cite{BurtscherFPC} in which an XOR operation against the previous data is used
as a pre-processor.  We shall exploit a priori information in the preprocessing
stage through the use of relatively computationally intensive preprocessors
(compared with FPC) that seek to extract structure in the data {\em in situ}.
This approach takes advantage of the increasingly wide cost gap between CPU
operations versus storage-based operations. 
We will assemble a suite of general techniques for refactoring of data that
will be embedded in the system and can be selected either by the user or by the
application itself. The techniques will range from crude catch-all methods such
as those employed by general compression routines such as FPC, through to
multi-resolution analysis using wavelets, including the methods on which we
have worked (cited above).  These will then be mapped over into the different
layers of the storage hierarchy.  We will also cater for user supplied
``plug-ins'' for detection of features and store higher resolution data around
the feature and lower resolution values which are further away from the
features. The plug-ins could be used to override the general embedded
techniques and would provide a mechanism for the transfer of a priori knowledge
from the user to the hierarchy without imposing a burden on either. 

We will also research the time it takes to re-organize the data and use the
multi-layer approach to staging to understand where the data gets reduced and
where it gets re-organized. Open questions are: 1) do we move the data over to
other nodes to then reduce and re-organize, 2) do we first reduce the data and
then later re-organize this data on different nodes?, or 3) do we use different
cores on a node to do data-refactoring.  Similarly we will examine the cost of
reading the data. It is clear that if we only read in the high-importance data
it should improve the overall time to move the data from the SSIO layer to the
application, but what happens when users want all of the data? There is an
obvious penalty of having to take data in a hierarchical layout and then place
this back to a single-layer view.  Additional research questions which must be
addressed: Will some types of data (e.g. PIC data) be more
conducive  for data-refactoring than others? How much semantic-domain knowledge
will we need to get a dramatic improvement of the storage system over not doing
anything special? 

We will measure these methods using the fusion XGC simulation and the
corresponding analysis routines which they use for their largest simulations.
We will also work with the SPECFM3D~\cite{SPECFEM3D} code, and the
fusion GTC~\cite{klasky2003grid} code.  In all cases we will measure their
impact from their current simulations, and their projected simulations and
analytics on exascale machines. Since all three of these codes are part of the
OLCF CAAR~\cite{CAAR} project for early access, there is strong belief that the
simulations will be close to exascale simulations and we will be able to run
these on prototypes of our SSIO layer to examine the full impact of
data-refactoring.

\paragraph{High Impact Challenges:} Maximal data reduction on the SSIO layer
would result from storing only the code and the input parameters, and simply
recalculating all of the results. This extreme is not feasible in practice
owing to the size of the computation involved, but sheds light on the path to
achieving a step change in data reduction. We shall investigate the possibility
of writing a minimal set of information, and placing an ``auditor'' code in a
docker container so that data can be re-generated with a {\em local}
approximation of the full simulation. This is an entirely new approach to
tackle the data overload by using preprocessors in which information is not
simply stored in the form of raw data, but rather in the form of appropriate
parameterized algorithms. An ``auditor'' is a code which mimics the exascale simulation, 
but typically can run using less degrees of freedom. This means that the code is a good
local (in space and time) approximation of the exascale simulation and will be much less
expensive to execute (so that it can be run in situ), and follows the behavior of the simulation
for short periods of time. This allows us to ``audit'' the simulation and when the differences 
become large in a local region of space, we write out the results of the data in this local region.
The hope is that this will be done only locally and that with a good ``auditor''  this will not happen
too frequently. This then allows us to use the ``auditor'' to reproduce results during the analysis 
process using an inexpensive computation and put less pressure on the SSIO layer.
%
% 
%   A priori information on the nature of the data is
%used to choose the basic algorithm while the actual values of the data are used
%to tune the parameters used in the reconstruction procedures. We shall refer to
%this paradigm as an Auditor.  We will examine the use of several auditors for
%the simulations which we will work with. One approach is to use a fluid-code to
%audit a kinetic code, which can be a good approximation over short periods of
%time. 

