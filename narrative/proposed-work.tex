\section{Proposed Research and Methods}
\label{subsec:challenges}
<<<<<<< HEAD
\label{sec:proposed}

\begin{enumerate}
\item Re-factoring of the data
  \begin{itemize}
  \item What kind of reorganization would work when?
  \item Would it take too much time and resources + memory and how can we
    manage the cost of the reorganization
  \item Will the users loose too much value from the data during refactoring
    and how can we control/bound this loss of value
  \end{itemize}
\item Quality of Service
  \begin{itemize}
  \item Can we make any gurantees in a scalable decentralized system?
  \item What quality of service metrics can be make gurantees on in a scalable
    way?
  \item What kind of error rates will we have in our estimation and what error
    rates are acceptable?
  \item How much will the qos scheduler, admission control, manager cost in
    terms of resources and time
  \item How do we make the scheduler scalable with the size of the system?
  \end{itemize}
\item Discovery and naming
  \begin{itemize}
  \item Which data block and what quality/type do we respond to a query
    with?
  \item How will we scale a fully distributed storage system's metadata
    management
  \item Can we accelerate the discovery process with the addition of an
    application/user level metadata store that tracks where data was
    originally written and how was it structured?
  \item Can we provide time estimates for read/write calls in the presence of
    an unbounded discovery system? If not how do we bound the discovery time?
  \item Time cost of discovery vs time cost of actually reading the data
    (latency vs bandwidth essentially)
  \end{itemize}
\item Migration
  \begin{itemize}
  \item Can we do scalable migration without making the discovery process
    unbounded
  \item What are the parameters and input for migration
  \item Can we use the additional application level knowledge of data to purge
    portions of the data from storage without making the data too much less
    valuable?
  \item With so many objects and no centralized directory how do we know what
    is actually in the storage to be purged?
  \item How do purge in a way that doesn't require a central authority,
    doesn't interfere with other I/O and isn't bottlenecked by some sort of
    global consistency
  \end{itemize}
\end{enumerate}


=======
{\bf {\color{red}Norbert will clean these pieces}}
>>>>>>> d16c2739bab5a833eb4d1de8805a12f25b751fc2
Our research into the storage system and middle ware layers must address
many of these challenges including
% \begin{enumerate}
% \item How do we describe the user intentions at the API layer and have this
%   communicated down to the middle-ware and storage layers?
% \item How do we allow users to define their user defined compression
%   techniques and have this data read back from the storage system layers?
%   Where do we execute the decompression techniques 
% \item How do we evaluate the tradeoffs at runtime to guide data placement,
%   including the movement of data across the network, across the different
%   storage tiers? (data movement and quality tradeoff)
% \item Can we use different forms of learning techniques as daemons on the
%   system to perform data migration from one storage tier to another? For
%   example, if we see that a user is looking at one time slice after another
%   for a certain object in their dataset which is in the slower tiers of the
%   hierarchy, do we automatically propane up the data which has not been
%   requested in the hope that this will be requested data? (prediction and
%   prefetching, user defined compression on movement)
% \item Can we understand the true need of campaign storage and understand the
%   possible impact of the campaign storage if we run a hadoop-like
%   file-system instead of a lustre/gpfs file system? (additional layers)
% \item Can we limit the amount of data duplication? Data space is going to be
%   very constrained on the exascale system so we must ensure that minimal
%   copies are made of data, and when data is duplicated, they are removed
%   through a garbage collection routine. (space management)
% \item Can we build a model to give us the time estimates during the reading
%   and writing phases, so that rules can be in place from the user
%   perspective to make adaptive decisions. For example, if the file system
%   says that reading will take 3 months, users can they place in rules to
%   then read in a subset of data, and they can understand which data can be
%   read quickly and which pieces will take more time. (estimation)
% \item Can we understand how to place code in the system which will allow
%   data-regeneration to take place. (regeneration)
% \item How does the storage system do a better job in managing request from
%   all of the users on a LCF than today? We realize that today users who have
%   a better middlware system can often lock other users from getting high
%   performance when they are running. If we have the concept of currency
%   which is eventually used in the same extent as node-hours, then users will
%   have to be able to think about how much storage and how much bandwidth
%   they can choose. One question that needs to be understood is if there are
%   times when the system sees that there are very few storage system
%   resources being used, then the lucky users can get the bandwidth cheaper
%   than at times of heavy usage. How can we enforce this? (Fairness)
% \item How do we manage all of the metdata not only from the principle
%   objects, but from the sub-objects? How well will this scale when we have
%   users who can potentially create billions of objects from their
%   simulation? (scalability and discovery)

\paragraph{Deliverables and artifacts}

\begin{enumerate}
\item Add plug-in architecture to Sirocco to support selective data compression
\item Add specific tier destinations command to Sirocco (assuming that the caching mechansism cannot achieve the desired effects).
\item Develop profiling system to determine how a data set is used during preparation runs prior to a capability run to determine how to optimize data placement for capability run analytics.

\end{enumerate}

{\bf \color{red}we need to condense these challenges into a few basic principles - 4 at most - SAK}

We propose to overcome these challenges by pioneering a new {\bf knowledge-centric approach} to a dynamic storage system
and I/O layer which takes into account user prioritization and utility to optimize the SSIO layer. The approach is based on 
four underlying principles:

\underline{Principle 1: A knowledge-centric system} , let user knowledge  to define data policies. talk about this pieces...

\underline{Principle 2: A SSIO system which integrates system knowledge together with application knowledge} to understand
how to organize data in the deep storage hierarchy

\underline{Principle 3: Predictable Performance in the SSIO layer} so that intelligent choices can be made by both the user and the system

\input{narrative/data-description} % includes data-desc, apis, and qos-app.   

% \subsection{Application Interface}

%\input{narrative/data-desc}

{\bf {\color{red}Scott will integrate these pieces}}
\input{narrative/data-reduction} % includes data-reorg and the api specs move to the above section -
\input{narrative/data-reorg}


%\input{narrative/qos-app}
{\bf {\color{red}Hasan will integrate these pieces, Manish after Hasan}}
\input{narrative/placement}
\input{narrative/migration}
\input{narrative/eviction-metrics}

%\input{narrative/policies}
%\input{narrative/apis}

% \subsection{Storage System}

{\bf {\color{red}Gary will integrate these pieces}}
\input{narrative/resource-mgmt}
\input{narrative/qos-storage}
%\input{narrative/discovery}
{\bf {\color{red}Jay  will integrate these pieces}}
\input{narrative/naming-service}
% \input{narrative/time-estimation}
\input{narrative/infrastructure}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
