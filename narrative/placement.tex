\subsection{Data Placement and Movement}
\newcommand{\Sir}{Sirocco}

%key points based on discussion with Carlos
%1. Allocation tables and simple search won't be able to scale
%2. Carlos suggests looking at a determinstic function to decide on
%placement of data into the storage hierarchy. Instead of storing the
%metadata that describes where data is stored (which server/tier) we can
%simple associate a function for each application. 
%3. Research into what would be an approrpriate function that allows even
%splitting of data across many servers/tiers
%4. Similar to our approach for defining utility over time
%5. Migration can also use a function. We shouldn't distinguish migration
%from data movement and placement too much
%6. We need metrics that can decide how the data moves up and down in the
%hierarchy (so we can enable pre-fetching)
%7. Carlos suggested that instead of hints we call them service level
%objectives (and that leads into quality of service)
The application workflows targeted in this proposal generate very large
amounts of data, which needs to be processed and analyzed before
potential insights from the simulations can be realized. 
In-situ and in-transit data staging approaches can alleviate some of the
burden on storage, leveraging available compute and memory resources on high
end systems to process the data close to where it is generated. 
However, given current architectural trends, emerging systems will have a
vastly great gulf between the computing capability (the capability to
generate data) and the storage capability (the capability to store the
generated data). Moreoever, the complexity and heterogeniety of the
architecture will introduce new tiers in the storage system. Instead of
being a flat, roughly homogenous space in which data can be stored, the new
storage hierarchy will be multi-tiered and greatly heterogeneous. 

We proposed to advance the state of the art in the three major parts of the
data lifecycle. First, we will explore the optimizations and techniques to
scalably manage data when it is generated by the application. Second, we
will study the management of resources and data at rest. And finally, we will
investiage how data is consumed, either by being read and processed (for
example within analytics) or by being purged from the system. 

Towards this end, we will utilize the faster layer of the
storage hierarhcies, such as DRAM, NVRAM and remote NVRAM, as the target for
the initial write from an application. To mitigate the space concerns, we
will use the techniques described in Section~\ref{sec:refactoring} to
identify and partition the segments of data with the highest utility for the
application and use case. The rest of the data will be stored on the
parallel file system, as is done today. We will leverage the supported data
mirgration techniques in \Sir to determine the most appropriate location to
move the data once its in stable storage. This migration will be guided by a
combination of the utility function defined by the application and the
resources avialable to the storage system. Knowing the specific structural
and semantic information (including relationships between data segments)
about the data will enable migration to partition the data further to
optimize access to data for reader. 
We will also explore policies that combine the knowledge of data, its
utility to users, the cost of maintaining data in long term storage and
other metrics to decide how to purge irrelevant data from the storage
system. 

\subsubsection{Data Placement}
\label{sec:data-placement}

Our past work on data
staging\cite{tongipdps15,qiansc15}\cite{docan2012dataspaces}\cite{abbasi2010datastager}
in HPC systems, coupled with the experience in developing high performance
I/O transports for ADIOS \cite{liu2014hello,lofstead2008flexible} has
provided us with insights on how to manage the complex I/O requirements for
exascale applications, particularly in the face of the aforementioned
multi-tier storage hierarchy. A key requirements is application-driven
%
runtime mechanisms for dynamically managing data placement across the layers
of the distributed storage hierarchy,
%
coordinating data movement 
%
and data sharing between the components of the application workflow so as to
maximize its utility to the application and reduce access costs. Our
approach to data placement will leverage the increased set of knowledge
about the data available to the middleware and storage system, potentially
allowing for greater optimizations as compared to other storage solutions. 
The key components of our research include

\paragraph{Application Hints:}
As described in Section~\ref{sec:refactor} one of the main components of our
proposed storage system is the ability to refactor and reduce data as it is
generated, and to reorgnized and regenerate the data as it is accessed. We
carry this principle into the placement and movement of data by allowing
applications to define hints and policies that guide \textbf{what} data is
placed \textbf{where}. We will explore the use of application hints in two
distinct areas. Firstly, we will study the challenges and tradeoffs of either
augmenting the I/O interface with hints or allowing the additional of an
external specification that defines the use case. Our experience with
developing modern I/O interfaces has shown that both techniques have value,
and we will investigate the set of hints that are embedded in the
application code vs. those that are described within an non-compiled
specification. Secondly, we will study how hints can guide the initial data
placement as data is handed off from application to storage ( during a
write) and from storage to application (during the read). In both cases we
will study what minimal set of annotations and hints can allow the storage
system to minimize data movement and optimize the resources consumed by I/O. 

\paragraph{Data Utility:} We will formulate the concept of utility (from an
application perspective) that is assigned data objects to quantify the
benefits of placement and/or movement actions to application. These
utilities may be based on the importance of the data objects, the frequency
with which they are accessed, etc., and are used during runtime data
management. For example, data objects will higher utility value may be
placed closer (e.g., in faster memory) to the accessing application.
Similarly, when it is time to evict data objects from a higher level storage
layer, the decision about which object to evict may take the utility into
account.

\paragraph{Autonomic in-transit data management:} 
Efficiently placing data objects
vertically across multiple memory hierarchy layers and storage levels as
well as horizontally across different storage nodes requires balancing
competing objectives. In this project we will explore autonomic data
management strategies that can evaluate utility/cost tradeoffs and
appropriately place/move data objects at runtime. The autonomic approach
will leverage both user hints and information gathered at runtime (e.g.,
runtime data access history, network topology, etc.) to place data object.

\paragraph{ Runtime tracking and estimation:}
Autonomic data placement and/or
movement will leverage the information gathered at runtime together with
predictive analysis requirements. This information may include runtime
spatial-temporal data access patterns, physical network topology
information, and etc., which will be tracked and gathered at runtime, and
then be used to estimate and predict the behavior of coupled applications in
the rest of simulation-time. A key research aspect here will be to
anticipate accesses, for example using learning techniques or using
application knowledge, and use this information to prefetch data objects.
For example, we will identify and catalogue key phases and access patterns
in the target workflows and then use it to characterize patterns at runtime
and trigger appropriate place/movement actions.

\paragraph{State of the art:} NAND flash-based devices are being
increasingly used in HPC storage systems at different levels and for a
variety of purposes. Several studies such as \cite{multitier}, \cite{sc10li}
have investigated using compute node local SSDs as storage buffers, for
example, to temporarily cache checkpoint data to support recovery from
failures. Research efforts have also explored using deep memory devices for
data analysis. Active Flash \cite{activeflash} proposes in-situ scientific
data analysis by directly executing data analysis tasks on emerging storage
devices. Minerva \cite{minerva} extends the conventional SSD architecture
using a FPGA-based storage controller to offload data or I/O intensive
application code to the SSD to accelerate data analysis. However, these
solutions cannot deal with coupled application workflows, where the data
exchange and access patterns can be both complex and dynamic. Furthermore,
some of these solutions require hardware modifications or special access
priorities to the HPC systems, which can limit their use to specific
resources.

\subsubsection{Migration}
\label{sec:migration}

One important research question in this proposal is whether and how a liberated
Hierarchical Storage Manager designed for online use, such as Sirocco, might be
employed in supporting the higher level concepts discussed previously.
Conceptually, in this effort, data is accessed by the middleware IO libraries
at various tiers in the lower-level storage hierarchy. While Sirocco does
autonomously group the distributed set of media into like groups and manage
movement of data between these groups the various motivations to do so include
both the traditional ones found in classical HSM (Hierarchical Storage
Management) as well as the novel.

Sirocco groups like media into aggregated volumes on a service node, and like
service nodes with like media into pseudo-tiers. It accomplishes both using
attributes such as latency to first byte, bandwidth to and from the media, and
some function representing the resilience capabilities of the attached media.
In addition to these static attributes, when grouping servers into
pseudo-tiers, Sirocco also may employ more dynamic attributes such as CPU load
and the rate-of-change of the media use.

Grouping into pseudo-tiers is not a formal, nor rigid, concept within Sirocco,
at present. In classic HSM the tiering is often a formally expressed concept,
embodied both in the architecture and the implementation. In Sirocco, though,
it is not directly expressed. Instead, a client or server that attempts to make
a decision relative to migration and staging uses the attributes discussed
above to constrain the choice of candidates. For instance, a server acting as
an off-node writeback cache for a compute node might only have access to
volatile dynamic RAM for use as storage media and a client might deposit data
that is marked for persistent media as a resilience constraint. At some point,
then, such a server must copy or move the data to another Sirocco server with
media matching, or exceeding, the resilience constraint. At that point in time,
all servers managing appropriate persistent media are candidates but
second-order attributes, such as latency and bandwidth of a candidate's managed
media would tend to dominate the ranking. Thus, the concept of a ``tier''
becomes an artifact of decision rather than a formal concept.

Further complication arises when a Sirocco server is forced to make choices for
capacity reasons. The Sirocco architecture dictates that a server function as a
victim cache. As media managed by a server fills the server will begin to
manage for capacity. In order to do so, currently, it need only verify or make
a copy of the data on another server that meets the resiliency constraint of
the held data prior to removing the local copy and in the current
implementation the server makes no attempt either to remember it ever had the
data or to notify others about what it is doing.  others that it has removed
the local copy or remember that it has 

Taken together then, the above at least complicates the higher level research
questions discussed in this proposal, potentially even renders some
unachievable since a few involve determinism. We are forced, then, to consider
either or both mitigating management policies within and between Sirocco
servers as well as explicitly allowing clients to participate in capacity
management decisions, for a time at least, on the relevant Sirocco servers.

Our research intent involves short term and long term activities. In the short
term we will modify the relevant clients to provide hit and miss related data
and associated activities and explore an enhancement to our storage servers
that would provide some, hopefully tightly, bounded estimate about time to
first byte for a potential read operation. In the long term we hope that an
augmented client-server API providing usage hints will allow us to modify local
server policy algorithms to more effectively manage it's media and communicate
to other servers holding copies of the relevant data how they might best manage
theirs. In the event that this approach proves ineffective at meeting the
overall approach described in this proposal we will explore modifying Sirocco
servers to provide notifications about locations of the relevant data as they
migrate and stage as well as supporting client-directed pinning of location,
for a period at least. Client-directed pinning could introduce potentially
serious negative impacts to Sirocco's overarching goals though, since the
fundamental design of Sirocco assumes much about opaque and unrestricted data
movement, so this is contemplated only as a last resort.

\subsubsection{Purging}
\label{sec:purging}


% The title might have to change...

\paragraph{Background:} There is a spectrum of approaches on how
to leverage a memory and storage hierarchy: On one end of the
spectrum is to copy or move data from one tier to another, the
closer the tier to the hierarchy's top, the faster the access to
the data it contains. On the other end of the spectrum is to use
higher tiers exclusively for auxiliary data that to some extent
represents the actual data on the lowest tier. Examples of auxiliary
data are views, indices, lossy compressions, lower resolution data,
and summary data (sometimes also known as metadata). There are a
number of drivers that push approaches to one or the other extreme:
(1) if a high tier has sufficient room for an application's working
set, approaches that move the working set into that tier will perform
well; Conversely, if a high tier very rarely captures working sets,
it is better to fill it with auxiliary data that speeds up access
to the working set captured by a lower tier; working sets only
reduce capacity misses but the concept can be generalized to the
likelihood of having the right data in place, i.e. proactively
moving data to high tiers accessed in predictable patterns reduces
compulsory misses as well; (2) if the application generates parallel
read/write requests and strong consistency semantics are important,
approaches that use auxiliary data to speed up access to persistent
shared data might yield better performance than approaches that
require coherence overhead to keep all copies on higher tiers
consistent; (3) if the application's access patterns are mismatched
with low tier access characteristics, a combination of approaches
that use auxiliary data and move data between tiers in order to
convert access patterns into a better match will perform well.

% Any other drivers?

\paragraph{Approach:} This spectrum enables interesting strategies
to manage space pressure on high tiers: instead of just evicting
pieces of data (blocks, pages, objects, files), one can free space
by ``thinning'' data so that accesses within more limited views or
lower resolutions can still occur without misses while requests
outside of views or higher resolutions can leverage the information
stored in auxiliary data. Conversely, data on a high tier can be
``enriched'' (e.g. turned into a higher resolution) if there is
value in doing so.

\paragraph{Related Work:} Mention Stanford's Legion system.

\paragraph{Challenge:} The decision on when to thin and when to
enrich data on a particular level depends on \emph{access patterns}.
Approaches that leveragee access patterns in application at national
labs has shown to greatly improve performance and reduce the amount
of overhead required for data management~\cite{he:hpdc13}. The
challenge is for applications to communicate these access patterns
to the I/O stack layers that manages tiers outside of the address
space. In the proposed work we will develop an abstraction for
\emph{locality} that allow applications or runtime profilers to
describe locality properties of access patterns, and implement
services that leverage locality properties to dynamically determine
the value of thinning or enriching data at a particular tier.
Consider the example of a road navigation system: the locality
properties of each navigation client are based on physical constraints
of speed, direction, likely resolution of map, and the fact that
clients almost never go offroad.

\paragraph{Preliminary work and results:} Our recent work \cite{tongipdps15}
explored a two-tiered staging method that spans both DRAM and solid state
disks (SSD). It allows us to support both code coupling and data management
for data intensive simulation workflows in a loosely coupled manner. In
addition, our application-aware adaptive data placement has demonstrated the
effectiveness of using user provided access information as ``hints'', and
runtime data access pattern history to improve data access efficiency and
overall end-to-end execution. Besides, additional data placement
optimization \cite{qiansc15} that leverages physical network topology along
with data access patterns has also demonstrated lower data access costs and
good scalability. Our work have been deployed and proven with real
applications, e.g., coupled combustion simulations (DNS-LES) that are part
of the ExaCT Exascale Co-design Center and coupled Fusion simulations
(XGC0-XGCa) that are part of the EPSI SciDAC on current high-end systems
such Titan at ORNL.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
