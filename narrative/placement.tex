\subsection{Managing Data in the Life Cycle}
\label{sec:managing-data-life}

\newcommand{\Sir}{Sirocco} %So I don't get the spelling wrong!

%key points based on discussion with Carlos
%1. Allocation tables and simple search won't be able to scale
%2. Carlos suggests looking at a determinstic function to decide on
%placement of data into the storage hierarchy. Instead of storing the
%metadata that describes where data is stored (which server/tier) we can
%simple associate a function for each application. 
%3. Research into what would be an approrpriate function that allows even
%splitting of data across many servers/tiers
%4. Similar to our approach for defining utility over time
%5. Migration can also use a function. We shouldn't distinguish migration
%from data movement and placement too much
%6. We need metrics that can decide how the data moves up and down in the
%hierarchy (so we can enable pre-fetching)
%7. Carlos suggested that instead of hints we call them service level
%objectives (and that leads into quality of service)

The application workflows targeted in this proposal generate very large
amounts of data, which needs to be processed and analyzed before
potential insights from the simulations can be realized. 
The overall data life cycle consists of four phases. Data starts
when it is generated by a simulation or through a data acquisition system,
in the case of experimental and observational data (EOD). 
In our target heterogenous multi-tier storage platform, the system must
decide where to place the output data at this time, and how to partition and
reorganize the data to optimize this initial placement. The key optimization
metrics during this phase is the percieved I/O overhead for the application.

In the second phase of the data life cycle, the system takes over the
management of the data and performs the necessary steps to meet service
level objectives (SLOs). These steps include migration, reorganization, and
reductions for the data. During this phase the system optimizes for resource
utilization, interference with running applications, and availability and
resiliency objectives. Data can be moved into archival storage (tape or cold
storage) during this phase. 

In the third phase the data is consumed for knowledge discovery (for example
read into analytics to produce new scientific insights). It must be noted
that the knowledge discovery process does not remove the data from the
system - the data remains available until it is purged. During this phase
the storage system optimizes for read performance and data availability. 

In the final phase the data is purged from the system altogether. The
decision to purge is made by the system based on operational metrics from
system administrators. This phase optimizes the space available in the
system to prevent out of space concerns for applications. 

Our proposed research explores the management of this entire life cycle
given our thesis that more knowledge, and more specific knowledge about the
data, can allow the management of data to be optimized with reduced cost.
Thus, we will explore the following high level research questions: (1) What methods can
be utilized in the initial placement of data that do not impose expensive
costs on discovery, consumption and migration? (2) How can the placement of
data be optimized for the multi-tiered hierarchy expected in exascale
systems? (3) How can we migrate data throughout the layers of storage
without making the discovery process unbounded? (4) How does the additional
knowledge aid in preparing the data for reading in a timely manner? (5) How
do we make the decision on purging using this additional knowledge? 

%Next we provide details on the challenges and proposed research through each
% phase of the data life cycle.

% We address three main concerns for the data life-cycle in this work. First,
% we will {\it explore the initial placement} of data during the output
% process. Second, we will study techniques that can {\it enhance migration}
% without making the eventual lookup of data unbounded. And finally, we will
% study who to {\it manage each tier} in a scalable fashion. %FIXME 


% \subsubsection{Initial Placement of Data}
% \label{sec:init-plac-data}

% The generation of data from simulations or EOD sources will require the
% system to make initial placement decisions. These decisions will leverage
% the description and refactoring of data mentioned earlier, and also the
% tiered design of the storage system. The volume and velocity of data at
% exascale can be alleviated partially by utilizing in-situ and in-transit
% data staging to process the data before the storage layer. Eventually,
% however, the data, processed and transformed, has to be sent to storage. 

% % In-situ and in-transit data staging approaches can alleviate some of the
% % burden on storage, leveraging available compute and memory resources on high
% % end systems to process the data close to where it is generated. 
% % However, given current architectural trends, emerging systems will have a
% % vastly great gulf between the computing capability (the capability to
% % generate data) and the storage capability (the capability to store the
% % generated data). Moreover, the complexity and heterogeneity of the
% % architecture will introduce new tiers in the storage system. Instead of
% % being a flat, roughly homogeneous space in which data can be stored, the new
% % storage hierarchy will be multi-tiered and greatly heterogeneous. 
% %

% For the initial placement of data, we will utilize the faster layers of the storage
% hierarchy, such as DRAM and NVRAM as the target for the initial write from
% an application as much as possible. These targets can be on-node (such as in
% the design of the Summit system at ORNL) or on separate nodes (as in the
% Trinity system at LANL). Regardless, the space limitations might require
% some of the data to be directly transferred to the parallel file system.
% Partitioning of data to accomplish this has been described in
% ~\S~\ref{sec:data-refactor}. 


% To mitigate the space concerns, we will use the techniques
% described in \S~\ref{sec:data-refactor} to identify and partition the
% segments of data with the highest utility for the application and use case.
% The rest of the data will be stored on the lower layers of the file system,
% such as the parallel file system, as is done today. We will leverage the
% supported data migration techniques in \Sir~to determine the most
% appropriate location to move the data once its in stable storage. This
% migration will be guided by a combination of the utility function defined by
% the application and the resources available to the storage system. Knowing
% the specific structural and semantic information (including relationships
% between data segments) about the data will enable migration to partition the
% data further to optimize access to data for reader. For example, in
% Fig.~\ref{fig:ssio-bucket}, P3 is initially placed in Campaign storage,
% since it's size and utility allow our system to place data there, but as the
% utility of data allows this chunk to be migrated to the Long Term, and
% slower, storage layer to allow other data chunks from other users to take
% advantage of this space. Similar, P2 migrates from the Parallel FS to
% Campaign storage to clear room for other users. We will utilize this
% migration and eviction strategy for Checkpoint/Restart (C/R) data as well,
% where data can be initially placed on NVRAM and then purged after the next
% (or next two) C/R files are written to NVRAM.

% We will also explore policies that combine the knowledge of data, its
% utility to users, the cost of maintaining data in long term storage and
% other metrics to decide how to purge irrelevant data from the storage
% system. 

% Our research will ultimately address the following questions:\\
% 1) Can we do scalable migration without making the discovery process
%     unbounded;\\
% 2) What are the parameters and input for migration;
% 3)  Can we use the additional application level knowledge of data to purge
%     portions of the data from storage without making the data too much less
%     valuable?;\\
% 3)  With so many objects and no centralized directory how do we know what
%     is actually in the storage to be purged?;\\
% 4)  How do purge in a way that doesn't require a central authority,
%     doesn't interfere with other I/O and isn't bottlenecked by some sort of
%     global consistency?

\subsubsection{Initial Placement of Data}
\label{sec:init-plac-data}

\paragraph{Background:}
When an application outputs data the storage and middleware layer needs to
decide \textit{what} data is placed \textit{where} amongst the storage
resource available to the system. This decision has far reaching impact
throughout the life time of the data. Past work on data
staging\cite{tongipdps15,qiansc15}\cite{docan2012dataspaces}\cite{abbasi2010datastager}
in HPC systems has shown that different output techniques targetting
different layers of the storage hierarchy can have a significant impact on
the overhead observed by the application for I/O operations. The key
requirements here are application-driven runtime mechanisms for dynamically, and
deterministically, 
managing data placement across the layers of the distributed storage
hierarchy, coordinating data movement and data sharing between the
components of the application workflow so as to maximize its utility to the
application and reduce access costs. Our approach to data placement will
leverage the increased set of knowledge about the data available to the
middleware and storage system, potentially allowing for greater
optimizations as compared to other storage solutions.

\paragraph{Application Hints:}
As described in \S\ref{sec:data-refactor} one of the main components of our
proposed storage system is the ability to refactor and reduce data as it is
generated, and to reorgnized and regenerate the data as it is accessed. We
carry this principle into the placement and movement of data by allowing
applications to define hints and policies that guide \textbf{what} data is
placed \textbf{where}. We will explore the use of application hints in two
distinct areas. Firstly, we will study the challenges and tradeoffs of either
augmenting the I/O interface with hints or allowing the additional of an
external specification that defines the use case. Our experience with
developing modern I/O interfaces has shown that both techniques have value,
and we will investigate the set of hints that are embedded in the
application code vs. those that are described within an non-compiled
specification. Secondly, we will study how hints can guide the initial data
placement as data is handed off from application to storage ( during a
write) and from storage to application (during the read). In both cases we
will study what minimal set of annotations and hints can allow the storage
system to minimize data movement and optimize the resources consumed by I/O. 

\paragraph{Data Utility:} We will formulate the concept of utility (from an
application perspective) that is assigned data objects to quantify the
benefits of placement and/or movement actions to application. These
utilities may be based on the importance of the data objects, the frequency
with which they are accessed, etc., and are used during runtime data
management. For example, data objects will higher utility value may be
placed closer (e.g., in faster memory) to the accessing application.
Similarly, when it is time to evict data objects from a higher level storage
layer, the decision about which object to evict may take the utility into
account. We will also explore the balancing of the data across the memory
hierarchy autonomically. The autonomic approach will leverage both user
hints and information gathered at runtime (e.g., runtime data access
history, network topology, etc.) to place data object.

\paragraph{ Runtime tracking and estimation:}
Autonomic data placement and/or
movement will leverage the information gathered at runtime together with
predictive analysis requirements. This information may include runtime
spatial-temporal data access patterns, physical network topology
information, and etc., which will be tracked and gathered at runtime, and
then be used to estimate and predict the behavior of coupled applications in
the rest of simulation-time. A key research aspect here will be to
anticipate accesses, for example using learning techniques or using
application knowledge, and use this information to prefetch data objects.
For example, we will identify and catalogue key phases and access patterns
in the target workflows and then use it to characterize patterns at runtime
and trigger appropriate place/movement actions.

% \paragraph{State of the art:} NAND flash-based devices are being
% increasingly used in HPC storage systems at different levels and for a
% variety of purposes. Several studies such as \cite{multitier}, \cite{sc10li}
% have investigated using compute node local SSDs as storage buffers, for
% example, to temporarily cache checkpoint data to support recovery from
% failures. Research efforts have also explored using deep memory devices for
% data analysis. Active Flash \cite{activeflash} proposes in-situ scientific
% data analysis by directly executing data analysis tasks on emerging storage
% devices. Minerva \cite{minerva} extends the conventional SSD architecture
% using a FPGA-based storage controller to offload data or I/O intensive
% application code to the SSD to accelerate data analysis. However, these
% solutions cannot deal with coupled application workflows, where the data
% exchange and access patterns can be both complex and dynamic. Furthermore,
% some of these solutions require hardware modifications or special access
% priorities to the HPC systems, which can limit their use to specific
% resources.
\subsubsection{Migration}
\label{sec:migration}

\paragraph{Background:} 
Once the system has completed the initial data placement with minimal
overhead to the application, it is up to the storage system to manage the
migration of data to other storage servers and across the hierarchy. An
important reserach aspect here is how a liberated Hierarchical Storage
Manager (HSM) designed for online use, such as Sirocco, might be employed in
providing support for the migration of data. Sirocco autonomously groups the
distributed set of media and service nodes into like groups and manages the
movement of data between these groups. The grouping is managed using
attributes such as latency to first byte, bandwidth to and from the media, and
some function representing the resilience capabilities of the attached media.
In addition to these static attributes, when grouping servers into
pseudo-tiers, Sirocco also may employ more dynamic attributes such as CPU load
and the rate-of-change of the media use. 

Grouping into pseudo-tiers is not a formal, nor rigid, concept within
Sirocco, at present. In classic HSM the tiering is often a formally
expressed concept, embodied both in the architecture and the implementation.
In Sirocco, though, it is not directly expressed. Instead, a client or
server that attempts to make a decision relative to migration and staging
uses these attributes to constrain the choice of candidates. For instance, a
server acting as an off-node writeback cache for a compute node might only
have access to volatile dynamic RAM for use as storage media and a client
might deposit data that is marked for persistent media as a resilience
constraint. At some point, then, such a server must copy or move the data to
another Sirocco server with media matching, or exceeding, the resilience
constraint. At that point in time, all servers managing appropriate
persistent media are candidates but second-order attributes, such as latency
and bandwidth of a candidate's managed media would tend to dominate the
ranking. Thus, the concept of a ``tier'' becomes an artifact of decision
rather than a formal concept.

Give these design decisions, our research in this project will study how
migration can serve to manage storage resident data within the constraints
defined by the application and the user. 

\paragraph{Migration Policies:}
The policies that govern the migration of data across groups and across
tiers have similar challenges as initial data placement. The migration
policy must encapsulate, at a high level, the constraints defined by the
application, but must also manage resources on each server for capacity and
load reasons. We will explore policy formulations that allow \Sir to move
data within a tier to continue to meet service objectives. Another component
of migration is the temporal function that determines the utility of data
through its life time on storage. If they utility decays over time, for
example, the initial constraints of data (such as resilience requirements)
will be modified and allow more leeway in how the system handles migration.
However, as data is widely distributed the scalability of this approach must
be investigated. 
We will explore how the utility function can be used to modify migration
parameters in a scalable manner, without requiring a centralized authority
to make decisions. 

\paragraph{Bounded estimates for lookup:}
One challenge for storage systems with continous decentralized migration is
the timeliness of lookup when applications issue read calls. This is due to
how no central directory exists for where data is at anytime. There are
solutions to this problem that utilize lookup tables which are updated
whenever a server initiates a migration process. However, these solutions
are not scalable. In order to have predictable performance the lookup
process must complete in a bounded time. Another solution here is to use a
distributed hashing function to determine exactly where data is placed,
reducing lookup to an $O(1)$ operation. We explore the discovery of data in
\S~\ref{sec:naming-discovery}. To support this discovery process, we will
investiagte how the migration of data can be performed in a way that enables
bounded estimates about time to first byte for a potential read operation.

% We will explore the use of a  hashing function
% that will leverage determinism to initiate the lookup process. If at most
% $k$ migration steps can happen 
% Our research intent involves short term and long term activities. In the short
% term we will modify the relevant clients to provide hit and miss related data
% and associated activities and explore an enhancement to our storage servers
% that would provide some, hopefully tightly, bounded estimate about time to
% first byte for a potential read operation. 


% Further complication arises when a Sirocco server is forced to make choices for
% capacity reasons. The Sirocco architecture dictates that a server function as a
% victim cache. As media managed by a server fills the server will begin to
% manage for capacity. In order to do so, currently, it need only verify or make
% a copy of the data on another server that meets the resiliency constraint of
% the held data prior to removing the local copy and in the current
% implementation the server makes no attempt either to remember it ever had the
% data or to notify others about what it is doing.  others that it has removed
% the local copy or remember that it has 

% Taken together then, the above at least complicates the higher level research
% questions discussed in this proposal, potentially even renders some
% unachievable since a few involve determinism. We are forced, then, to consider
% either or both mitigating management policies within and between Sirocco
% servers as well as explicitly allowing clients to participate in capacity
% management decisions, for a time at least, on the relevant Sirocco servers.


% \paragraph{Migration Policies:}
% In the long term we hope that an
% augmented client-server API providing usage hints will allow us to modify local
% server policy algorithms to more effectively manage it's media and communicate
% to other servers holding copies of the relevant data how they might best manage
% theirs. In the event that this approach proves ineffective at meeting the
% overall approach described in this proposal we will explore modifying Sirocco
% servers to provide notifications about locations of the relevant data as they
% migrate and stage as well as supporting client-directed pinning of location,
% for a period at least. Client-directed pinning could introduce potentially
% serious negative impacts to Sirocco's overarching goals though, since the
% fundamental design of Sirocco assumes much about opaque and unrestricted data
% movement, so this is contemplated only as a last resort.

\subsubsection{Making data avilable on multiple tiers}
\label{sec:manag-data-mult}

\paragraph{Background:} Providing data to users for knowledge discovery is
one of the primary purposes of our storage system design. There is
There is a
spectrum of approaches on how
this can be accomplished: On one end the storage system can aim to maintain
full resolution data in the highest tiers of the storage hierarchy. This
approach is, obviously, limited due to the disparity between size of data
and available space in the higher tiers. Another approach is to utilize the
techniques described in \ref{sec:data-refactor} to create auxiliary views of
the data that, with user defined error, represent the data, and place these
smaller data sets on the higher tiers for faster access. 

There are a
number of drivers that push approaches to one or the other extreme:
(1) if a high tier has sufficient room for an application's working
set, approaches that move the working set into that tier will perform
well; Conversely, if a high tier very rarely captures working sets,
it is better to fill it with auxiliary data that speeds up access
to the working set captured by a lower tier; working sets only
reduce capacity misses but the concept can be generalized to the
likelihood of having the right data in place, i.e. proactively
moving data to high tiers accessed in predictable patterns reduces
compulsory misses as well; (2) if the application generates parallel
read/write requests and strong consistency semantics are important,
approaches that use auxiliary data to speed up access to persistent
shared data might yield better performance than approaches that
require coherence overhead to keep all copies on higher tiers
consistent; (3) if the application's access patterns are mismatched
with low tier access characteristics, a combination of approaches
that use auxiliary data and move data between tiers in order to
convert access patterns into a better match will perform well.

% Any other drivers?
\paragraph{Thinning of Data:} 
This spectrum enables interesting strategies
to manage space pressure on high tiers: instead of just evicting
pieces of data (blocks, pages, objects, files), one can free space
by ``thinning'' data so that accesses within more limited views or
lower resolutions can still occur without misses while requests
outside of views or higher resolutions can leverage the information
stored in auxiliary data. Conversely, data on a high tier can be
``enriched'' (e.g. turned into a higher resolution) if there is
value in doing so. We will explore how the storage system can perform these
operations to meet the service level objectives for each use case, while
optimizing the overall utility from the system perpective. 

\paragraph{Locality Properties:} The decision on when to thin and when to
enrich data on a particular level depends on \emph{access patterns}.
Approaches that leverage access patterns in applications at leadership facilities
 have shown to greatly improve performance and reduce the amount
of overhead required for data management~\cite{he:hpdc13}. The
challenge is for applications to communicate these access patterns
to the I/O stack layers that manages tiers outside of the address
space. In the proposed work we will develop an abstraction for
\emph{locality} that allow applications or runtime profilers to
describe locality properties of access patterns, and implement
services that leverage locality properties to dynamically determine
the value of thinning or enriching data at a particular tier.
Consider the example of a road navigation system: the locality
properties of each navigation client are based on physical constraints
of speed, direction, likely resolution of map, and the fact that
clients almost never go offroad.

\subsubsection{Purging data from the system}
\label{sec:purging-data}
(only outline since I have to go)
1. Purging is important from the perspective of preventing uncontrolled
growth of data on a system
2. Purging is hard because no one wants to lose data and we need to explore
the policies that govern it
3. If we have a good utility function and a good way to refactor the data we
could purge the ``less important'' pieces and maintain a high level of
knowledge without requiring very high amount of storage
3. Purging needs to happen in a decentralized way but global utility has to
be maximized - thats a pretty hard problem 




% \subsubsection{Preliminary work and results} 
% \label{sec:prework}

% Our recent work \cite{tongipdps15}
% explored a two-tiered staging method that spans both DRAM and solid state
% disks (SSD). It allows us to support both code coupling and data management
% for data intensive simulation workflows in a loosely coupled manner. In
% addition, our application-aware adaptive data placement has demonstrated the
% effectiveness of using user provided access information as ``hints'', and
% runtime data access pattern history to improve data access efficiency and
% overall end-to-end execution. Besides, additional data placement
% optimization \cite{qiansc15} that leverages physical network topology along
% with data access patterns has also demonstrated lower data access costs and
% good scalability. Our work have been deployed and proven with real
% applications, e.g., coupled combustion simulations (DNS-LES) that are part
% of the ExaCT Exascale Co-design Center and coupled Fusion simulations
% (XGC0-XGCa) that are part of the EPSI SciDAC on current high-end systems
% such Titan at ORNL. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
