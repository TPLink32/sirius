\subsubsection{Data Placement and Movement}

The application workflows targeted in this proposal generate very large amounts of data, and this data must be processed and analyzed before potential insights from the simulations can be realized. In-situ/in-transit data processing using in-memory data-staging approaches have been effectively used to address data-related challenges and enable simulations workflows on current high end systems -- the key idea is to use compute and storage resources on the high-end system to process the data close to where it is generated. However, given current architectural trends, indicate that emerging systems will have increasing numbers of cores per node and correspondingly decreasing amounts of DDRAM memory per core as well as decreasing memory bandwidth. Architectural trends also indicate deeper memory hierarchies and multilevel storage structures. As a results, to address the increases data volumes and data processing requirements requires that these levels of storage be effectively utilized and exposes as a uniform staging abstraction that can be used by the application workflows.  A key requirements is application-driven runtime mechanisms for dynamically managing data placement across the layers of the distributed storage hierarchy, coordinating data movement and data sharing between the components of the application workflow so as to maximize its utility to the application and reduce access costs. 

\paragraph{State of the art:} NAND flash-based devices are being increasingly used in HPC storage systems at different levels and for a variety of purposes. Several studies such as \cite{multitier}, \cite{sc10li} have investigated using compute node local SSDs as storage buffers, for example, to temporarily cache checkpoint data to support recovery from failures. Research efforts have also explored using deep memory devices for data analysis. Active Flash \cite{activeflash} proposes in-situ scientific data analysis by directly executing data analysis tasks on emerging storage devices. Minerva \cite{minerva} extends the conventional SSD architecture using a FPGA-based storage controller to offload data or I/O intensive application code to the SSD to accelerate data analysis. However, these solutions cannot deal with coupled application workflows, where the data exchange and access patterns can be both complex and dynamic. Furthermore, some of these solutions require hardware modifications or special access priorities to the HPC systems, which can limit their use to specific resources.

\paragraph{Proposed research approach:} In this project we will utilizes deep memory hierarchies and multilevel storage structures to support staging-based in-transit data management and processing for data-intensive scientific workflows at extreme scale. Specifically, we will address the coordination and data access/sharing requirements of application workflows, as well as the  online data management requirements such as for example, in-transit online data analysis,  check-pointing,  data reduction. We will also deal with the complexities associated with data placement of data and movement in the multiple layers and components of the storage hierarchy. The key components of our research include: 

{\em Application hints:}  Application hints can provide additional information to the runtime based on the users' knowledge of the application workflow as well as on past experiences, for example, possible data access patterns, nature of regions of interest, and domain specific information. This application level information will be added to the specification of the workflow and can provide advisory inputs to efficiently help the runtime make smarter decisions about the placement and/or movement of data objects and the scheduling and execution of tasks, as well as to effectively manage trade-offs between for example, power consumption and time-to-solution. 

{\em Data utility:} We will formulate the concept of utility (from an application perspective) that is assigned data objects to quantify the benefits of placement and/or movement actions to application. These utilities may be based on the importance of the data objects, the frequency with which they are accessed, etc., and are used during runtime data management. For example, data objects will higher utility value may be placed closer (e.g., in faster memory) to the accessing application. Similarly, when it is time to evict data objects from a higher level storage layer, the decision about which object to evict may take the utility into account. 

{\em Autonomic in-transit data management:} Efficiently placing data objects vertically across multiple memory hierarchy layers and storage levels as well as horizontally across different storage nodes requires balancing competing objectives. In this project we will explore autonomic data management strategies that can  evaluate utility/cost tradeoffs and appropriately place/move data objects at runtime. The autonomic approach will leverage both user hints and information gathered at  runtime (e.g., runtime data access history, network topology, etc.) to place data object. 

{\em Runtime tracking and estimation:}
Autonomic data placement and/or movement will leverage the information gathered at runtime together with predictive analysis requirements. This information may include runtime spatial-temporal data access patterns, physical network topology information, and etc., which will be tracked and gathered at runtime, and then be used to estimate and predict the behavior of coupled applications in the rest of simulation-time. A key research aspect here will be to anticipate accesses, for example using learning techniques or using application knowledge, and use this information to prefetch data objects. For example, we will identify and catalogue key phases and access patterns in the target workflows and then use it to characterize patterns at runtime and trigger appropriate place/movement actions. 

\paragraph{Preliminary work and results:} Our recent work \cite{tongipdps15} explored a two-tiered staging method that spans both DRAM and solid state disks (SSD).  It allows us to support both code coupling and data management for data intensive simulation workflows in a loosely coupled manner. In addition, our application-aware adaptive data placement has demonstrated the effectiveness of using user provided access information as ``hints'', and runtime data access pattern history to improve data access efficiency and overall end-to-end execution. Besides, additional data placement optimization \cite{qiansc15} that leverages physical network topology along with data access patterns has also demonstrated lower data access costs and good scalability. Our work have been deployed and proven with real applications, e.g., coupled combustion simulations (DNS-LES) that are part of the ExaCT Exascale Co-design Center and coupled Fusion simulations (XGC0-XGCa) that are part of the EPSI SciDAC on current high-end systems such Titan at ORNL. 