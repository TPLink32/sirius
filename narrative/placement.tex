\subsection{Managing Data in the Life Cycle}
\label{sec:managing-data-life}

\newcommand{\Sir}{Sirocco} %So I don't get the spelling wrong!

%key points based on discussion with Carlos
%1. Allocation tables and simple search won't be able to scale
%2. Carlos suggests looking at a determinstic function to decide on
%placement of data into the storage hierarchy. Instead of storing the
%metadata that describes where data is stored (which server/tier) we can
%simple associate a function for each application. 
%3. Research into what would be an approrpriate function that allows even
%splitting of data across many servers/tiers
%4. Similar to our approach for defining utility over time
%5. Migration can also use a function. We shouldn't distinguish migration
%from data movement and placement too much
%6. We need metrics that can decide how the data moves up and down in the
%hierarchy (so we can enable pre-fetching)
%7. Carlos suggested that instead of hints we call them service level
%objectives (and that leads into quality of service)

The application workflows targeted in this proposal generate very large
amounts of data, which needs to be processed and analyzed before
potential insights from the simulations can be realized. 
The overall data life cycle consists of three phases. Data starts
when it is generated by a simulation or through a data acquisition system,
in the case of experimental and observational data (EOD). 
In our target heterogenous multi-tier storage platform, the system must
decide where to place the output data at this time, and how to partition and
reorganize the data to optimize this initial placement. The key optimization
metrics during this phase is the percieved I/O overhead for the application.

In the second phase of the data life cycle, the system takes over the
management of the data and performs the necessary steps to meet service
level objectives (SLOs). These steps include migration, reorganization, and
reductions for the data. During this phase the system optimizes for resource
utilization, interference with running applications, and availability and
resiliency objectives. Data can be moved into archival storage (tape or cold
storage) during this phase. 

In the third phase the data is consumed for knowledge discovery (for example
read into analytics to produce new scientific insights). It must be noted
that the knowledge discovery process does not remove the data from the
system - the data remains available until it is purged. During this phase
the storage system optimizes for read performance and data availability. 

% In the final phase the data is purged from the system altogether. The
% decision to purge is made by the system based on operational metrics from
% system administrators. This phase optimizes the space available in the
% system to prevent out of space concerns for applications. 

Our proposed research explores the management of this entire life cycle
given our thesis that more knowledge, and more specific knowledge about the
data, can allow the management of data to be optimized with reduced cost.
Thus, we will explore the following high level research questions: (1) What methods can
be utilized in the initial placement of data that do not impose expensive
costs on discovery, consumption and migration? (2) How can the placement of
data be optimized for the multi-tiered hierarchy expected in exascale
systems? (3) How can we migrate data throughout the layers of storage
without making the discovery process unbounded? (4) How does the additional
knowledge aid in preparing the data for reading in a timely manner? (5) How
do we make the decision on purging using this additional knowledge? 

 % Next we provide details on the challenges and proposed research through each
 % phase of the data life cycle.

% We address three main concerns for the data life-cycle in this work. First,
% we will {\it explore the initial placement} of data during the output
% process. Second, we will study techniques that can {\it enhance migration}
% without making the eventual lookup of data unbounded. And finally, we will
% study who to {\it manage each tier} in a scalable fashion. %FIXME 


% \subsubsection{Initial Placement of Data}
% \label{sec:init-plac-data}

% The generation of data from simulations or EOD sources will require the
% system to make initial placement decisions. These decisions will leverage
% the description and refactoring of data mentioned earlier, and also the
% tiered design of the storage system. The volume and velocity of data at
% exascale can be alleviated partially by utilizing in-situ and in-transit
% data staging to process the data before the storage layer. Eventually,
% however, the data, processed and transformed, has to be sent to storage. 

% % In-situ and in-transit data staging approaches can alleviate some of the
% % burden on storage, leveraging available compute and memory resources on high
% % end systems to process the data close to where it is generated. 
% % However, given current architectural trends, emerging systems will have a
% % vastly great gulf between the computing capability (the capability to
% % generate data) and the storage capability (the capability to store the
% % generated data). Moreover, the complexity and heterogeneity of the
% % architecture will introduce new tiers in the storage system. Instead of
% % being a flat, roughly homogeneous space in which data can be stored, the new
% % storage hierarchy will be multi-tiered and greatly heterogeneous. 
% %

% For the initial placement of data, we will utilize the faster layers of the storage
% hierarchy, such as DRAM and NVRAM as the target for the initial write from
% an application as much as possible. These targets can be on-node (such as in
% the design of the Summit system at ORNL) or on separate nodes (as in the
% Trinity system at LANL). Regardless, the space limitations might require
% some of the data to be directly transferred to the parallel file system.
% Partitioning of data to accomplish this has been described in
% ~\S~\ref{sec:data-refactor}. 


% To mitigate the space concerns, we will use the techniques
% described in \S~\ref{sec:data-refactor} to identify and partition the
% segments of data with the highest utility for the application and use case.
% The rest of the data will be stored on the lower layers of the file system,
% such as the parallel file system, as is done today. We will leverage the
% supported data migration techniques in \Sir~to determine the most
% appropriate location to move the data once its in stable storage. This
% migration will be guided by a combination of the utility function defined by
% the application and the resources available to the storage system. Knowing
% the specific structural and semantic information (including relationships
% between data segments) about the data will enable migration to partition the
% data further to optimize access to data for reader. For example, in
% Fig.~\ref{fig:ssio-bucket}, P3 is initially placed in Campaign storage,
% since it's size and utility allow our system to place data there, but as the
% utility of data allows this chunk to be migrated to the Long Term, and
% slower, storage layer to allow other data chunks from other users to take
% advantage of this space. Similar, P2 migrates from the Parallel FS to
% Campaign storage to clear room for other users. We will utilize this
% migration and eviction strategy for Checkpoint/Restart (C/R) data as well,
% where data can be initially placed on NVRAM and then purged after the next
% (or next two) C/R files are written to NVRAM.

% We will also explore policies that combine the knowledge of data, its
% utility to users, the cost of maintaining data in long term storage and
% other metrics to decide how to purge irrelevant data from the storage
% system. 

% Our research will ultimately address the following questions:\\
% 1) Can we do scalable migration without making the discovery process
%     unbounded;\\
% 2) What are the parameters and input for migration;
% 3)  Can we use the additional application level knowledge of data to purge
%     portions of the data from storage without making the data too much less
%     valuable?;\\
% 3)  With so many objects and no centralized directory how do we know what
%     is actually in the storage to be purged?;\\
% 4)  How do purge in a way that doesn't require a central authority,
%     doesn't interfere with other I/O and isn't bottlenecked by some sort of
%     global consistency?

\subsubsection{Initial Placement of Data}
\label{sec:init-plac-data}

\paragraph{Background:}
When an application outputs data the storage and middleware layer needs to
decide \textit{what} data is placed \textit{where} amongst the storage
resource available to the system. This decision has far reaching impact
throughout the life time of the data. Past work on data
staging\cite{tongipdps15,qiansc15}\cite{docan2012dataspaces}\cite{abbasi2010datastager}
in HPC systems has shown that different output techniques targetting
different layers of the storage hierarchy can have a significant impact on
the overhead observed by the application for I/O operations. The key
requirements here are application-driven runtime mechanisms for dynamically, and
deterministically, 
managing data placement across the layers of the distributed storage
hierarchy, coordinating data movement and data sharing between the
components of the application workflow so as to maximize its utility to the
application and reduce access costs. Our approach to data placement will
leverage the increased set of knowledge about the data available to the
middleware and storage system, potentially allowing for greater
optimizations as compared to other storage solutions.

\paragraph{Application Hints:}
As described in \S\ref{sec:data-refactor} one of the main components of our
proposed storage system is the ability to refactor and reduce data as it is
generated, and to reorgnized and regenerate the data as it is accessed. We
carry this principle into the placement and movement of data by allowing
applications to define hints and policies that guide \textbf{what} data is
placed \textbf{where}. We will explore the use of application hints in two
distinct areas. Firstly, we will study the challenges and tradeoffs of either
augmenting the I/O interface with hints or allowing the additional of an
external specification that defines the use case. Our experience with
developing modern I/O interfaces has shown that both techniques have value,
and we will investigate the set of hints that are embedded in the
application code vs. those that are described within an non-compiled
specification. Secondly, we will study how hints can guide the initial data
placement as data is handed off from application to storage ( during a
write) and from storage to application (during the read). In both cases we
will study what minimal set of annotations and hints can allow the storage
system to minimize data movement and optimize the resources consumed by I/O. 

\paragraph{Data Utility:} We will formulate the concept of utility (from an
application perspective) that is assigned data objects to quantify the
benefits of placement and/or movement actions to application. These
utilities may be based on the importance of the data objects, the frequency
with which they are accessed, etc., and are used during runtime data
management. For example, data objects will higher utility value may be
placed closer (e.g., in faster memory) to the accessing application.
Similarly, when it is time to evict data objects from a higher level storage
layer, the decision about which object to evict may take the utility into
account.

\paragraph{Autonomic in-transit data management:} 
Efficiently placing data objects
vertically across multiple memory hierarchy layers and storage levels as
well as horizontally across different storage nodes requires balancing
competing objectives. We will explore autonomic data
management strategies that can evaluate utility/cost tradeoffs and
appropriately place/move data objects at runtime. The autonomic approach
will leverage both user hints and information gathered at runtime (e.g.,
runtime data access history, network topology, etc.) to place data object.

\paragraph{ Runtime tracking and estimation:}
Autonomic data placement and/or
movement will leverage the information gathered at runtime together with
predictive analysis requirements. This information may include runtime
spatial-temporal data access patterns, physical network topology
information, and etc., which will be tracked and gathered at runtime, and
then be used to estimate and predict the behavior of coupled applications in
the rest of simulation-time. A key research aspect here will be to
anticipate accesses, for example using learning techniques or using
application knowledge, and use this information to prefetch data objects.
For example, we will identify and catalogue key phases and access patterns
in the target workflows and then use it to characterize patterns at runtime
and trigger appropriate place/movement actions.

% \paragraph{State of the art:} NAND flash-based devices are being
% increasingly used in HPC storage systems at different levels and for a
% variety of purposes. Several studies such as \cite{multitier}, \cite{sc10li}
% have investigated using compute node local SSDs as storage buffers, for
% example, to temporarily cache checkpoint data to support recovery from
% failures. Research efforts have also explored using deep memory devices for
% data analysis. Active Flash \cite{activeflash} proposes in-situ scientific
% data analysis by directly executing data analysis tasks on emerging storage
% devices. Minerva \cite{minerva} extends the conventional SSD architecture
% using a FPGA-based storage controller to offload data or I/O intensive
% application code to the SSD to accelerate data analysis. However, these
% solutions cannot deal with coupled application workflows, where the data
% exchange and access patterns can be both complex and dynamic. Furthermore,
% some of these solutions require hardware modifications or special access
% priorities to the HPC systems, which can limit their use to specific
% resources.

\subsubsection{Managing Data on Multiple Tiers}
\label{sec:manag-data-mult}

\paragraph{Background:} There is a spectrum of approaches on how
to leverage a memory and storage hierarchy: On one end of the
spectrum is to copy or move data from one tier to another, the
closer the tier to the hierarchy's top, the faster the access to
the data it contains. On the other end of the spectrum is to use
higher tiers exclusively for auxiliary data that to some extent
represents the actual data on the lowest tier. Examples of auxiliary
data are views, indices, lossy compressions, lower resolution data,
and summary data (sometimes also known as metadata). There are a
number of drivers that push approaches to one or the other extreme:
(1) if a high tier has sufficient room for an application's working
set, approaches that move the working set into that tier will perform
well; Conversely, if a high tier very rarely captures working sets,
it is better to fill it with auxiliary data that speeds up access
to the working set captured by a lower tier; working sets only
reduce capacity misses but the concept can be generalized to the
likelihood of having the right data in place, i.e. proactively
moving data to high tiers accessed in predictable patterns reduces
compulsory misses as well; (2) if the application generates parallel
read/write requests and strong consistency semantics are important,
approaches that use auxiliary data to speed up access to persistent
shared data might yield better performance than approaches that
require coherence overhead to keep all copies on higher tiers
consistent; (3) if the application's access patterns are mismatched
with low tier access characteristics, a combination of approaches
that use auxiliary data and move data between tiers in order to
convert access patterns into a better match will perform well.

% Any other drivers?
\paragraph{Thinning of Data:} 
This spectrum enables interesting strategies
to manage space pressure on high tiers: instead of just evicting
pieces of data (blocks, pages, objects, files), one can free space
by ``thinning'' data so that accesses within more limited views or
lower resolutions can still occur without misses while requests
outside of views or higher resolutions can leverage the information
stored in auxiliary data. Conversely, data on a high tier can be
``enriched'' (e.g. turned into a higher resolution) if there is
value in doing so.


\paragraph{Locality Properties:} The decision on when to thin and when to
enrich data on a particular level depends on \emph{access patterns}.
Approaches that leverage access patterns in applications at leadership facilities
 have shown to greatly improve performance and reduce the amount
of overhead required for data management~\cite{he:hpdc13}. The
challenge is for applications to communicate these access patterns
to the I/O stack layers that manages tiers outside of the address
space. In the proposed work we will develop an abstraction for
\emph{locality} that allow applications or runtime profilers to
describe locality properties of access patterns, and implement
services that leverage locality properties to dynamically determine
the value of thinning or enriching data at a particular tier.
Consider the example of a road navigation system: the locality
properties of each navigation client are based on physical constraints
of speed, direction, likely resolution of map, and the fact that
clients almost never go offroad.

\subsubsection{Migration}
\label{sec:migration}

\paragraph{Background:} One important research question in this proposal is
whether and how a liberated Hierarchical Storage Manager designed for online
use, such as Sirocco, might be employed in supporting the higher level
concepts discussed previously. Conceptually, in this effort, data is
accessed by the middleware I/O libraries at various tiers in the lower-level
storage hierarchy. While Sirocco does autonomously group the distributed set
of media into like groups and manage movement of data between these groups
the various motivations to do so include both the traditional ones found in
classical HSM (Hierarchical Storage Management) as well as the novel.

Sirocco groups like media into aggregated volumes on a service node, and like
service nodes with like media into pseudo-tiers. It accomplishes both using
attributes such as latency to first byte, bandwidth to and from the media, and
some function representing the resilience capabilities of the attached media.
In addition to these static attributes, when grouping servers into
pseudo-tiers, Sirocco also may employ more dynamic attributes such as CPU load
and the rate-of-change of the media use.

Grouping into pseudo-tiers is not a formal, nor rigid, concept within Sirocco,
at present. In classic HSM the tiering is often a formally expressed concept,
embodied both in the architecture and the implementation. In Sirocco, though,
it is not directly expressed. Instead, a client or server that attempts to make
a decision relative to migration and staging uses the attributes discussed
above to constrain the choice of candidates. For instance, a server acting as
an off-node writeback cache for a compute node might only have access to
volatile dynamic RAM for use as storage media and a client might deposit data
that is marked for persistent media as a resilience constraint. At some point,
then, such a server must copy or move the data to another Sirocco server with
media matching, or exceeding, the resilience constraint. At that point in time,
all servers managing appropriate persistent media are candidates but
second-order attributes, such as latency and bandwidth of a candidate's managed
media would tend to dominate the ranking. Thus, the concept of a ``tier''
becomes an artifact of decision rather than a formal concept.

Further complication arises when a Sirocco server is forced to make choices for
capacity reasons. The Sirocco architecture dictates that a server function as a
victim cache. As media managed by a server fills the server will begin to
manage for capacity. In order to do so, currently, it need only verify or make
a copy of the data on another server that meets the resiliency constraint of
the held data prior to removing the local copy and in the current
implementation the server makes no attempt either to remember it ever had the
data or to notify others about what it is doing.  others that it has removed
the local copy or remember that it has 

Taken together then, the above at least complicates the higher level research
questions discussed in this proposal, potentially even renders some
unachievable since a few involve determinism. We are forced, then, to consider
either or both mitigating management policies within and between Sirocco
servers as well as explicitly allowing clients to participate in capacity
management decisions, for a time at least, on the relevant Sirocco servers.

\paragraph{Bounded estimates for access:}
Our research intent involves short term and long term activities. In the short
term we will modify the relevant clients to provide hit and miss related data
and associated activities and explore an enhancement to our storage servers
that would provide some, hopefully tightly, bounded estimate about time to
first byte for a potential read operation. 

\paragraph{Migration Policies:}
In the long term we hope that an
augmented client-server API providing usage hints will allow us to modify local
server policy algorithms to more effectively manage it's media and communicate
to other servers holding copies of the relevant data how they might best manage
theirs. In the event that this approach proves ineffective at meeting the
overall approach described in this proposal we will explore modifying Sirocco
servers to provide notifications about locations of the relevant data as they
migrate and stage as well as supporting client-directed pinning of location,
for a period at least. Client-directed pinning could introduce potentially
serious negative impacts to Sirocco's overarching goals though, since the
fundamental design of Sirocco assumes much about opaque and unrestricted data
movement, so this is contemplated only as a last resort.

\subsubsection{Preliminary work and results} 
\label{sec:prework}

Our recent work \cite{tongipdps15}
explored a two-tiered staging method that spans both DRAM and solid state
disks (SSD). It allows us to support both code coupling and data management
for data intensive simulation workflows in a loosely coupled manner. In
addition, our application-aware adaptive data placement has demonstrated the
effectiveness of using user provided access information as ``hints'', and
runtime data access pattern history to improve data access efficiency and
overall end-to-end execution. Besides, additional data placement
optimization \cite{qiansc15} that leverages physical network topology along
with data access patterns has also demonstrated lower data access costs and
good scalability. Our work have been deployed and proven with real
applications, e.g., coupled combustion simulations (DNS-LES) that are part
of the ExaCT Exascale Co-design Center and coupled Fusion simulations
(XGC0-XGCa) that are part of the EPSI SciDAC on current high-end systems
such Titan at ORNL. 


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
