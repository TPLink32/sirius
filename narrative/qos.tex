\subsection{Resource Management and QoS}
\label{sec:qos}

\paragraph{Resource Management Background:} There is an important connection between finding the resources that can help fulfill a particular request and the predictability of the performance of that request, both in terms of latency and resolution. This connection is particular important for reads: in very large scale systems, finding a resource to read can be a significant, if not dominant, component of overall read latency. If the search for resources is unbounded, it will be difficult to make any performance predictions. 

Locating resources is a typical distributed systems problem. Central solutions are limited in their scalability, especially if they have to track every update. Traditional file systems are good examples for this: most updates to files require updates to their centrally maintained block allocation tables. The opposite of file systems are ``pile'' systems which instead of keeping track of updates search for resources at request time. Pile systems allow very efficient writes because clients can write whereever there is free space and there is very bookkeeping overhead. Furthermore, pile systems have great flexibility for moving resources around, for example to ensure durability. However, reads require a potentially unbounded search, especially if it is not clear whether the resource actually exists. Methods to make searches more efficient without reverting to centralized bookkeeping usually involves some kind of partitioning of the search space and a method to limit the search to a small number of partitions. 

Frequently this involves a hash function that is relatively stable against changes to its range, such as failures or updates to the hardware of the system (see for example consistent hashing~\cite{karger:stoc97}). Consistent hashing is scalable since it can be well-known throughout a system of any size and only requires minimal movement of data in the presence of failures or system changes. Consistent hash functions can also have sets of ordered but infinite lists as their range (which works well with primary replication)~\cite{honicky:ipdps04}, implement declustering (limiting overlap between partitions), and support placement policy rules~\cite{weil:sc06}.

In this proposal we will investigate the trade-off in terms of predictable performance between ``filing'' data, making an upfront investment to ensure finding data quickly, and ``piling'' data, pushing the cost of finding to the moment when the data is actually needed. This trade-off has a parallel in data management where there are two alternative paths to achieve knowledge: the analogue to filing is ``extract, transform, and load'' (ETL) data into a warehouse, paying an upfront ingest cost to achieve fast querying; the analogue to piling is to not pay any upfront cost and query the raw data directly by using data processing platforms such as Hadoop. Recent work has found that the combination of these two techniques can be optimized to perform better than any one technique alone~\cite{lefevre:sigmod14a}. We believe that a similar opportunity exists for combining filing and piling approaches to achieve performance predictability without sacrificing too much flexibility.

More specifically, the coarseness of a search space partitioning limits the search time within a partition, i.e. finely partitioned search spaces require little or no search within a partition but offer little placement flexibility. Partitioning can also be fine in one dimension and coarse in another to allow flexibility where it is most needed while still ensuring fast finding. For example, a hash function can finely partition a low storage tier but each partition can extend across multiple tiers so that a search still has to occur in order to find a resource on a particular tier. 

Specifically, we will address the following research questions:\\
  1) Can we make any guarantees in a scalable decentralized system?; \\
  2) What quality of service metrics can be make guarantees on in a scalable
    way?;\\
  3) What kind of error rates will we have in our estimation and what error
    rates are acceptable?;\\
  4) How much will the qos scheduler, admission control, manager cost in
    terms of resources and time?;\\
  5) How do we make the scheduler scalable with the size of the system?


\paragraph{QoS Background:} 
Quality of Service (QoS) refers to the
properties of the performance quality of a particular service, in
this case the storage system.
%Performance quality can be expressed in either relative or absolute
%terms. Examples of relative performance terms are ``fair'',
%``proportional'', or ``priority/class-based'' (``the higher the
%priority the better the service'' or ``1st class is better than
%2nd''). The key advantage of relative performance terms is that
%they are easy to implement. The key disadvantage is the difficulty
%of making any guarantees based on those implementations --
%even relative guarantees are mired by the well-known effect of
%priority inversion~\cite{lampson:cacm80}. Examples of absolute
%performance terms are ``rate-based'', ``soft real-time'', and
%``hard real-time''. The key advantage of absolute performance
%terms are that they enable the implementation of strong
%guarantees: absolute terms enable agreements between application
%tasks and storage that we call \emph{reservations} and that do not
%change meaning depending
%on the storage system's workload. The key challenge of absolute
%performance terms is that their implementation is non-trivial,
%especially in large-scale systems.
One of the key differentiating features this project offers is driving storage
decisions through QoS \emph{reservations}, rather than continuing the current model
of allowing applications to compete freely without restriction, for part or all of the storage
resources. This traditional approach leads to intereference effects
\cite{lofstead:2010:io-variability,liu_hotstorage} that can greatly impact IO performance predictability. 
%Further, with the introduction of additional tiers in the
%storage stack, the performance variability will increase with greater
%competiion for limited high performance resources. 
%% Temporaily commented out the above sentence, because it may not be true for the next DOE systems. 
Unfettered, this competition can yield a lower system and application performance 
than the case where applications attempt to reserve storage resources, thereby enabling the storage system
to reason over its performance in terms of reservations and actual
performance.
In the cases where an appication arrives that needs to run more urgently
than any of the other already running applications, the system
can rapidly respond to it by reducing the amount of resources taken 
by less urgent applications, allowing the urgent application to finish,
and later reverting back to the previous committed resource level for the rest.
This can be automatically enforced via user- (or adiminstrator-) specified \textit{policies}
(section \ref{}).

There are two fundamental causes that can impede the I/O performance
in an extreme-scale SSIO system. One is indirection, where layers
of storage tiers are employed either for performance and/or for
scalability reasons.  The direct consequence is that there could
be multiple traversing paths from application end to the rest place
of data, and more often than not, the I/O paths are not under control
under any single authority. The other cause is the shared use of
resources. The best effort I/O request/response nature and lack of
QoS mechanisms imply that there is little guarantee in terms of
expected performance. Both indirection and shared use of resource
contribute to a high probability of imbalanced use of resources
thus the occurrence of congestion and degraded performance.

As an example to demonstrate that this disconnection will hurt
system performance: We launch 4096 processes with each process doing
a single file I/O operation against half of the Spider II file
system. The traces of those files are analyzed to examine the
utilization distribution of different components. Figure (a), (b)
and (c) shows the resource usage distribution for OSTs, OSSes, and
LNETs, respectively. We observe that there exists a significant
variation in usage across components of any given type (e.g., OST,
OSS or LNET). For example, some OSTs are used more than 10 times
while some others are never used (corresponding to zero frequency
count). Similarly, OSSes and LNETs show significant imbalance in
usage under the default placement strategy. Consequently, imbalanced
resource utilization increases the contention at certain components
more than others.

% Don't know how to integrate this. Gary
%Given these insights, we advocate the idea that the infrastructure
%knowledge should find a way to relay to the upper layer for better
%and more effective use of resources. We think this is more pertinent
%and critical given the recent development of multi-tier storage and
%storage component heterogeneity.  There needs to be a way for
%application/middleware layer to gain more exposure of storage system
%for more intelligent processing logic.  One prime example of such
%exposed knowledge can be request/response time. To most applications,
%this is a black box. Profiling it at the upper layer is neither
%efficient nor effective, as it doesn't reflect cross-layer
%characteristics. However, Most of storage layer does keep a detailed
%profiling of such information. We therefore envision and propose a
%histogram-based request/response profiling API that application and
%middleware layer can leverage and make more informative decisions.


\begin{figure}[tbh]
  \centering
  \includegraphics[width=\columnwidth]{graphics/infrastructure.pdf}\vspace{-1.2in}
  \caption{Resource usage distribution for OST(a), OSS(b) and LNETs(c). }
\end{figure}


% This is about finding the relationshiop of workload-dependent to workload-independent metrics. -- Carlos
 %are driven from multiple different approaches. First, supporting
%capability runs is a priority for capability machines. We propose to address
%capability run performance by incorporating monitoring for prepartory runs at
%smaller scale to profile the application output characteristics both from a
%writing velocity and volume, but also for example read patterns for the data
%analytics required to generate scientific insights from the raw data. 

% Not sure how to reuse this ... -- Carlos
%By
%discovering approximate data proportions that will likely be the key subsets
%targeted by the simulation run, we can generate a policy to preserve a certain
%storage portion for high fidelity data storagae and use slower or perhaps lower
%fidelity or compressed data storage for less interesting data portions. We will
%support spreading data appropriately across the storage hierarchy. In
%particular, we will investigate how to place data across different layers to
%meet the performance requirements for output while maintaining system
%availability for other applications and offering the best possible performance
%for the data analytics that will ultimately process this data.


% Not sure how to reuse this... -- Carlos
%Second, once data has been placed for future performance requirements, policies
%must be able to address maintaining data on a priority basis in that location
%to deliver on the placement optimization generated by the capability run
%profilier.

% This is a matter of metering, not performance reservations. -- Carlos
%Third, most future NVM devices have limited write endurance prompting
%management to ensure fair use by all machine users. Technologies like
%NAND-flash and Phase Change Memory have limited write endurance. By
%incorporating policies about the proportion of write endurance a compute
%allocation is entitled to, data placement decisions can be made to ensure fair
%resource usage. While the offending application will suffer worse IO
%performance, other applications that more carefully address their data
%intensity on these limited devices will achieve higher overall performnce. Only
%by instituting such policies can we encourage application users to adopt
%technology to manage the limited resources.


%We propose to investigate both offering a policy mechanism and how to implement
%these kinds of policies in a way that addresses offering the shortest
%end-to-end time for scientific insights.

% Lifted the following paragraph to the beginning. Gary
%``Quality of Service'' (QoS) refers to the
%properties of the performance qualtiy of a particular service, in
%this case the storage system.
%Performance quality can be expressed in either relative or absolute
%terms. Examples of relative performance terms are ``fair'',
%``proportional'', or ``priority/class-based'' (``the higher the
%priority the better the service'' or ``1st class is better than
%2nd''). The key advantage of relative performance terms is that
%they are easy to implement. The key disadvantage is the difficulty
%of making any guarantees based on those implementations --
%even relative guarantees are mired by the well-known effect of
%priority inversion~\cite{lampson:cacm80}. Examples of absolute
%performance terms are ``rate-based'', ``soft real-time'', and
%``hard real-time''. The key advantage of absolute performance
%terms are that they enable the implementation of strong
%guarantees: absolute terms enable agreements between application
%tasks and storage that we call \emph{reservations} and that do not
%change meaning depending
%on the storage system's workload. The key challenge of absolute
%performance terms is that their implementation is non-trivial,
%especially in large-scale systems.

\paragraph{Approach:} The goals of this proposal require applications
to demand guarantees in terms of absolute performance qualities.
The first key enabler for any service to make absolute guarantees
is \emph{performance isolation}, i.e. the ability to shield the
performance impact of one task from another. The most straight-forward
but least flexible strategy is to simply cap the latency of each
task, regardless of its completeness (see for
example~\cite{decandia:sosp07}). This strategy is applicable when
incomplete tasks still have value and missing results are either
not important or can be easily retrieved in subsequent tasks. A
more generally applicable strategy is to control the admission of
tasks to the system: \emph{admission control} maintains a
\emph{utilization model} that can predict the utilization of system
resources given a new task, estimates the current utilization of
the system, and decides whether the task can be admitted without
overloading the system. Thus, admission control avoids system
overload conditions that lead to hard-to-model chaotic behaviors.
Consequently, utilization models can frequently be approximated by
automatically calibrated linear models (see for
example~\cite{skourtis:hpdc12}).

Another essential component of performance isolation is the
\emph{charging model} which determines which task(s) should be
charged how much for a request. Performance isolation is implemented
by the charging model by ensuring that performance of a task only
depends on its reservation and its workload behavior (but not on
other workloads in the system). Each task has a budget based on its
reservation and spends it based on the cost of each request determined
by the charging model. A charging model determines the cost based
on the utilization model and the interactions a request can have
with other requests (e.g. a random request to spinning media
interrupting a stream of sequential requests).

Performance isolation (including its utilization and charging models)
critically depends on \emph{workload-independent metrics}. Even
though throughput and latency are frequently the most meaningful
metrics for an application, they are not workload-independent and
therefore are inadequate for measuring the utilization of a device
across all workloads, unless one accepts worst-case estimates that
can lead to gross underutilization of resources (e.g. the throughput
of random and sequential workloads in spinning media differ by
orders of magnitudes). A common workload-independent metric is
\emph{time utilization}, i.e. the amount of time a resource is
utilized in a given time interval. Time utilization has the advantage
of having an always defined maximum and therefore makes resources
fully reservable. Even though a workload-independent metric might
at first not appear meaningful for an application, given a particular
workload an application can discover the relationship between its
workload-dependent metric and the workload-independent metric.
Because of performance isolation, the application has to discover
this relationship only once.
To achieve effective performance isolation, we seek to expand the resource management
capabilities in Sirocco to support additional utilization 
metrics to guide resource management
decisions. Currently, Sirocco offers automatic resource management capabilities,
based on resource capacity and data resilience requirements, and it needs to be extended
to offer data-centric metrics that help guide the forced QoS decisions.

As stated in the introduction, we seek to make a storage stack that works with
the user. Towards that end, we must incorporate data attributes into this
process. For example, particularly for a capability run, the storage system
should give priority to data subset tagged with high importance to stay in fast
storage tiers. Enabling this priority will be driven by other job
characteristics such as job size or a manually set priority inserted by system
administrators. It should also store the highest quality data possible given
the performance requirement. Ideally, the storage system will store the raw
data at full fidelity. When this tier approaches capacity, data should migrate
not just based on age, but also based on this priority. This will help ensure
the shortest time to insight by making sure the most important machine runs
have priority to use system resources.

\paragraph{Related Work:} On current HPC systems, absolute IO performance
guarantees are not possible. Previous work \cite{lofstead:2010:io-variability,liu_hotstorage} 
from this project team uses a client-assisted approach to mitigate IO variablity.
Existing efforts on scheduing resources~\cite{thapaliya:2014:io-cop,dorier:2014:calciom} attempt to offer
admission control to maximize storage performance. Unfortunately, these efforts
are limited to participating applications and only applications from a single
platform. For this approach to be effective, the storage system must offer an
approach that handles clients from all connected platforms and does not
require modification to use new storage access APIs. By using a priority
tagging system, we will offer defaults suitable for all applications that can
be informed by simple extensions to the job scheduler or more advanced
management through new APIs.

\paragraph{Challenges:} The deep and heterogenous memory and storage
hierarchy we are assuming for this proposal complicates the
relationship between workload-dependent and workload-independent
metrics: the performance of a task can significantly differ depending
on what levels of the hierarchy are involved. Furthermore, an
important goal of the proposed project is to enable applications
to reason about the trade-off between resolution and latency, adding
yet another dimension to how tasks are mapped to resources.

\begin{itemize}

\item A key challenge of admission control is scalability: the
decision of whether to admit a task could potentially depend on
global knowledge of the current utilization of every single resource
in the system. Scalability therefore depends on whether admission
control is able to accurately map a new task to a relatively small
set of resources which can quickly provide up-to-date utilization.
One approach might involve pseudo-random mapping that also load-balances
as a side-effect.

\item To offer latency/latency trade-offs, the system must be able
to quickly generate a number of data production plans, involving
different parts of the hierarchy and different data resolutions.
It will then use the utilization model to estimate the latency for
each production plan and the resolutions they can provide. Here
again, pseudo-random selection could reduce the number of resources
that would be involved, thereby increasing scalability.

\item The complexity of a utilization model involving the entire
storage hieararchy with 100,000s of devices is potentially daunting.
However, the utilization model can be simplified by modeling classes
of devices as well as classes of requests. In particular, each
device could restrict access to its content via a set of well-defined
methods with known, absolute performance properties.

\item Flash devices become inherently unpredictable when reads and
writes are mixed on the same device because of garbage collection.
While write latency can be easily hidden using asynchronous I/O,
hiding read latency is more difficult. By separating reads and
writes for each device, read latency becomes predictable. The
challenge is to minimize duplication of data, at least on fast
layers and leverage redundancy across layers.

\end{itemize}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:

