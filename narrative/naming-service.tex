\subsection{Naming and Discovery Service}

Given the well documented difficulties with POSIX-compliant naming and our
approach that decomposes what would be a single file into potentially large
number of objects, we must take a new approach to naming.  Data migration, as a
result of system pressures or user/system policies, further adds to the
complexity of managing naming and discovery. Our naming service must maintain
sufficient metadata information, track or be able to find objects as they move
across storage layers, and facilitates data discovery. Since we need to also
support access through POSIX APIs for existing application and system
integration, we must address how to bridge the drastic differences between the
decomposed objects of potentially varying fidelity with potentially multiple
version with a POSIX API-compatible interface.

Our research in this area will address the following questions:
(1) Since our metadata is distributed and partitioned, how do we respond to user
  requests within a specified time bound with accuracy? How do we estimate the
  completeness of the response?
(2) For POSIX API requests, how do we respond if only varying data fidelity levels
  are available for the requested data object?
(3) What level of metadata should we maintain and keep in sync to enhance response
  quality within time-bound requests?
(4) How much time should be allowed for searching for metadata given the user
  specified limits for data retrieval since the user will still have to spend
  the data retrieval time from different devices? How do we integrate the read
  time overhead into the metadata requests?
(5) How do we provide appropriate security while maintaining scalability,
  particularly with the distributed metadata stores?
% Which data block and what quality/type do we respond to a query with?;\\
% How will we scale a fully distributed storage system's metadata management;\\
% Can we accelerate the discovery process with the addition of an
%    application/user level metadata store that tracks where data was
%    originally written and how was it structured?;\\
% Can we provide time estimates for read/write calls in the presence of
%    an unbounded discovery system? If not how do we bound the discovery time?
% How do we amortize the time cost of discovery vs time cost of actually
%    reading the data (latency vs bandwidth essentially)?

Hierarchical Storage Management (HSM) systems~\cite{blaze:1992:hsm} offer a strict caching
approach for managing different storage capacities trading off performance for
capacity.  By maintaining a single namespace across all tiers, it is possible
to list a single directory view with files stored at different tiers. While
this approach to managing multi-tier storage works for whole files that fit on
single logial devices, it is far from ideal for scientific simulations.

Our goal with this proposal is to offer a similar capability but with a finer
granularity leveraging the inner structure of scientific data. It must also
handle data discovery efficiently for cases where data is spread across several
storage layers or some of those pieces may have migrated based on system
pressures or user or system policies. Instead of a single file, such as might
be used to store an entire timestep output for a simulation, we will
demonstrate a metadata service with naming at the science variable level, but
with an associated list of all of the variable pieces and potentially different
versions stored throughout the storage system. By shifting to a finer-grained
approach, we will enable more effective use of close/fast/small storage tiers
and prioritize objects with high importance.  Traiditional HSM stores an entire
file on a tier making room if insufficient space is available.  With this shift
to a partial variable granularity, a 1 PB output with 500 GB of ``high
interest'' data can limit this costly tier usage to just 500 GB greatly
improving storage availability in the close/fast/small tier and focus
performance/capacity costs to get the most science from from the platform.  By
only placing ``high interest'' data in the close/fast/small tier, we will
effectively reduce pressure forcing data migration hurting both writing
performance and the later data analytics required for generating scientific
insights.  Sirocco identifies a data segment using a
contaier/object/fork/address tuple that can have associated attributes. We will
both add a special attribute indicating that this record is of ``high
interest'' as well adapting the migration mechanisms to use this attribute, if
present, to strongly encouraging keeping the record in close/fast/small storage
as long as possible.

Meanwhile, we need to be able to locate or ``discover'' the data should it
move.  The challenge for discovery is that potentially, data will migrate from
where it is initially stored to a new location within the storage system.
Sirocco offers an ability to search for data that has moved as well as forcing
a particular resilience-based replica be the ``authoritative'' version
migrating the data to a particular location. We will investigate if the current
Sirocco functionality is capable of supporting the new operating modes we wish
to offer. Initial expectations suggest having bounded time guarantees for
finding data are critical for offering the quality of service guarantees we
wish to offer. Some potential approaches can build on CRUSH~\cite{weil:ceph} from
Ceph to offer a map of where to search for data sequentially. Given our
multiple storage tiers, we would extend this using some mechanism to shift to
the next tier to continue searching because the requested data never made it to
this location. This new work will be an expansion of Sirocco's currently
planned features.

The overriding theme for this proposal of providing a cooperative relationship
between the user and the storage system extends into this area as well.
Negotiating the data quality/retrieval time tradeoff reuqires additional
metadata.  Providing this functionality requries the naming service be able to
both return a list of data versions and an indication of data quality. This
will be combined with a retrieval time estimate for each verstion by the
middleware to determine what data version should be retrieved.  This estimate
will be disucssed elsewhere in this proposal.

Structurally, the traditional POSIX naming service offeriing a hierarchical
space consisting of directories and files may be maintained for backwards
compatibility. However, this view will not offer the same strict semantics
POSIX defines. We must break these strict semantics to address scalability
problems forced by the serialized access to a single source for creating and
accessing files.  Several efforts~\cite{patil:2007:giga+,carns:pvfs} have worked to
reduce this contention by doing things like reducing the serialized scope to a
single directory or subtree or allowing a single process in a collective file
operation talk with the metadata service and distribute a handle to other
participants. While these approaches help, they do not address this key
scalability limitation of serialized access--even if the metadata service is
spread across multiple nodes. Instead, we will offer a short duration to
consistency POSIX view to offer the performance and consistency requirements an
exascale application demands.

While pure object stores, such as those popular in the big data
domain~\cite{Fitzpatrick:2004:memcached,others} avoid this bottleneck by strictly offering an
object ID with the application required to manage how this ID maps to something
of interest. This approach of removing the metadata service from the system
level completely can work well for scale out applications where data is created
or consumed by a single process at a time or just in a work queue rather than
potentially O(1 million) processes all actively collectively for a single
``object''. To address this case, having some system integrated metadata
services to associate names with these object is a preferable solution. While
it will not have the same synchronous consistency semantics as POSIX, we will
offer something as close as possible to address legacy application and
command-line style data access and migration to other storage types using these
POSIX semantics.

We plan to address this scalability problem by continuing with the LWFS and
Sirocco model. LWFS and Sirocco have take an approach similar to the pure
object stores, but with a focus on the HPC setting. They have abandonded a
fully POSIX compliant metadata service as the default model in favor of a
container/object/fork/addrress tuple for identifying data similar to those used
for pure object stores. By having a service that addresses the object
collections that comprise a single thing, such as a variable or timestep
output, just enough metadata is maintained to make the storage system usable
without additional heavy lifting by clients.  LWFS demonstrated a POSIX-style
namespace on the side kept in sync using a transaction process like
D2T~\cite{lofstead:2012:txn} showing that this alternative approach can support traditional
POSIX API calls even though the underlying storage system uses a different
model. Because we are not requiring the entire storage space be addressible
from a single tree root, we can offer multiple independent roots using the
Sirocco object storage. We will investigate how to make the naming and
discovery service scalable under the circumstances outlined above.

In addition to the basic naming and data tracking operations, we will also need
to incorporate authorization capabilities. Sirocco currently integrates with a
Kerberos service for authentication. Given a capability ticket, a user can
access different objects as needed. This ticket structure offers protection
services typically offered on POSIX directories and files, but can do it at the
fork level instead. This allows a reduced quality data version to be available
for the general users while the high-quality version would be limited to the
data creator. This and other considerations for security must be incorporated
into the entire naming and metadata service.

While strongly encouraging data to stay in a particular type of storage tier,
ultimately it must be possible to flush this data to make room for other uses.
Sirocco offers a ``flush'' operation that forcibly migrates data to the
long-term resilience requirements freeing space in the scare resources. In
many, if not most cases, this will move data towards or actually on to a device
like tape intended for long-term storage.

The main challenge of incorporating the additional, rich metadata will be
joined by the additional challenge of coordinating with the other storage and
application-layer services to offer the best access times possible for data
stored in the system. The developed metadata services that drive data
compression and subsetting operations must have easy, consistent, and ideally
non-blocking or locking access to this metadata service. We must investigate
how to build such a metadata and naming service that also incorporates and
maintains the additional metadata required to support our advanced
functionality.

\begin{enumerate}

\item Milestone 1: Demonstrate a metadata service capable of serving both POSIX
clients and our clients, but without consideration for authentication,
authorization, or data migration.

\item Milestone 2: Demonstrate a time bounded search approach for finding data
within the storage system to identify data and the current location.  This will
update the cached information in the metadata service to short-circuit future
requests for the data presuming that it does not migrate again soon.

\item Milestone 3: Demonstrate incorporating A\&A and show scalability under
both application load as well as storage system pressures showing that we can
tolerate both loads and maintain quality of service.

\end{enumerate}

