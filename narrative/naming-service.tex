\subsection{Naming and Discovery Service}
\label{sec:naming-discovery}
\paragraph{Background:}
Given the well documented difficulties with POSIX-compliant naming and our
approach that decomposes what would be a single file into potentially large
number of objects, we must take a new approach to naming.  Data migration, as a
result of system pressures or user/system policies, further adds to the
complexity of managing naming and discovery. Our naming service must maintain
sufficient metadata information, track or be able to find objects as they move
across storage layers, and facilitates data discovery. Since we need to also
support access through POSIX APIs for existing application and system
integration, we must address how to bridge the drastic differences between the
decomposed objects of potentially varying fidelity with potentially multiple
versions with a POSIX API-compatible interface.

Hierarchical Storage Management (HSM) systems~\cite{blaze:1992:hsm} offer a
strict caching approach for managing different storage capacities trading off
performance for capacity.  By maintaining a single namespace across all tiers,
it is possible to list a single directory view with files stored at different
tiers. While this approach to managing multi-tier storage works for whole files
that fit on single logical devices, it is far from ideal for scientific
simulations.

The overriding theme for this proposal of providing a cooperative relationship
between the user and the storage system extends into this area as well.
Negotiating the data quality/retrieval time trade-off requires additional
metadata.  Providing this functionality requires the naming service be able to
both return a list of data versions and an indication of data quality. This
will be combined with a retrieval time estimate for each version by the
middleware to determine what data version should be retrieved.  This estimate
will be discussed elsewhere in this proposal.

In addition to the basic naming and data tracking operations, we will also need
to incorporate authorization capabilities. Sirocco currently integrates with a
Kerberos service for authentication. Given a capability ticket, a user can
access different objects as needed. This ticket structure offers protection
services typically offered on POSIX directories and files, but can do it at the
fork level instead. This allows a reduced quality data version to be available
for the general users while the high-quality version would be limited to the
data creator. This and other considerations for security must be incorporated
into the entire naming and metadata service.

While strongly encouraging data to stay in a particular type of storage tier,
ultimately it must be possible to flush this data to make room for other uses.
Sirocco offers a ``flush'' operation that forcibly migrates data to the
long-term resilience requirements, freeing space in the scarce resources. In
many, if not most cases, this will move data towards or actually on to a device
like tape, intended for long-term storage.

\paragraph{Approach:}
Our goal with this proposal is to offer a similar capability but with a finer
granularity leveraging the inner structure of scientific data. It must also
handle data discovery efficiently for cases where data is spread across several
storage layers or some of those pieces may have migrated based on system
pressures or user or system policies. Instead of a single file, such as might
be used to store an entire timestep output for a simulation, we will
demonstrate a metadata service with naming at the science variable level, but
with an associated list of all of the variable pieces and potentially different
versions stored throughout the storage system. By shifting to a finer-grained
approach, we will enable more effective use of close/fast/small storage tiers
and prioritize objects with high importance.  Traditional HSM stores an entire
file on a tier making room if insufficient space is available.  With this shift
to a partial variable granularity, a 1 PB output with 500 GB of ``high
interest'' data can limit this costly tier usage to just 500 GB, greatly
improving storage availability in the close/fast/small tier and focus
performance/capacity costs to get the most science from the platform.  By
only placing ``high interest'' data in the close/fast/small tier, we will
effectively reduce pressure forcing data migration hurting both writing
performance and the later data analytics required for generating scientific
insights.  Sirocco identifies a data segment using a
container/object/fork/address tuple that can have associated attributes. We will
add both a special attribute indicating that this record is of ``high
interest'' as well adapting a migration mechanisms to use this attribute, if
present, to strongly encourage keeping the record in  the highest storage tier as possible. 

Meanwhile, we need to be able to locate or ``discover'' the data should it
move.  The challenge for discovery is that potentially, data will migrate from
where it is initially stored to a new location within the storage system.
Sirocco offers an ability to search for data that has moved as well as forcing
a particular resilience-based replica be the ``authoritative'' version
migrating the data to a particular location. We will investigate if the current
Sirocco functionality is capable of supporting the new operating modes we wish
to offer. Initial expectations suggest having bounded time guarantees for
finding data are critical for offering the quality of service guarantees we
wish to offer. Some potential approaches can build on CRUSH~\cite{weil:ceph}
from Ceph to offer a map of where to search for data sequentially. Given our
multiple storage tiers, we would extend this using some mechanism to shift to
the next tier to continue searching because the requested data never made it to
this location. This new work will be an expansion of Sirocco's currently
planned features.

We plan to address scalability problems by continuing with the LWFS and Sirocco
model. LWFS and Sirocco have taken an approach similar to the pure object
stores, but with a focus on the HPC setting. They have abandoned a fully POSIX
compliant metadata service as the default model in favor of a
container/object/fork/address tuple for identifying data similar to those used
for pure object stores. By having a service that addresses the object
collections that comprise a single thing, such as a variable or timestep
output, just enough metadata is maintained to make the storage system usable
without additional heavy lifting by clients.  LWFS demonstrated a POSIX-style
namespace on the side kept in sync using a transaction process like
D2T~\cite{lofstead:2012:txn} showing that this alternative approach can support
traditional POSIX API calls even though the underlying storage system uses a
different model. Because we are not requiring the entire storage space be
addressable from a single tree root, we can offer multiple independent roots
using the Sirocco object storage. We will investigate how to make the naming
and discovery service scalable under the circumstances outlined above.

\paragraph{Related Work:}
Structurally, the traditional POSIX naming service offering a hierarchical
space consisting of directories and files may be maintained for backwards
compatibility. However, this view will not offer the same strict semantics
POSIX defines. We must break these strict semantics to address scalability
problems forced by the serialized access to a single source for creating and
accessing files.  Several efforts~\cite{patil:2007:giga+,carns:pvfs} have
worked to reduce this contention by doing things like reducing the serialized
scope to a single directory or subtree, or allowing a single process in a
collective file operation talk with the metadata service and distribute a
handle to other participants. While these approaches help, they do not address
this key scalability limitation of serialized access--even if the metadata
service is spread across multiple nodes. Instead, we will offer a short
duration to consistency POSIX view to offer the performance and consistency
requirements an exascale application demands.

While pure object stores, such as those popular in the big data
domain~\cite{Fitzpatrick:2004:memcached}, others avoid this bottleneck by
strictly offering an object ID with the application required to manage how this
ID maps to something of interest. This approach of removing the metadata
service from the system level completely can work well for scale out
applications where data is created or consumed by a single process at a time or
just in a work queue rather than potentially O(1 million) processes all
actively collectively for a single ``object''. To address this case, having
some system integrated metadata services to associate names with these object
is a preferable solution. While it will not have the same synchronous
consistency semantics as POSIX, we will offer something as close as possible to
address legacy application and command-line style data access and migration to
other storage types using these POSIX semantics.

\paragraph{Challenge:}
The main challenge of incorporating the additional, rich metadata will be
joined by the additional challenge of coordinating with the other storage and
application-layer services to offer the best access times possible for data
stored in the system. The developed metadata services that drive data
compression and subsetting operations must have easy, consistent, and ideally
non-blocking or locking access to this metadata service. We must investigate
how to build such a metadata and naming service that also incorporates and
maintains the additional metadata required to support our advanced
functionality.

Our research in this area will address the following questions:
(1) Since our metadata is distributed and partitioned, how do we respond to user
  requests within a specified time bound with accuracy? How do we estimate the
  completeness of the response?, 
(2) For POSIX API requests, how do we respond if only varying data fidelity
  levels are available for the requested data object?, 
(3) What level of metadata should we maintain and keep in sync to enhance
  response quality within time-bound requests?, 
(4) How much time should be allowed for searching for metadata given the user
  specified limits for data retrieval since the user will still have to spend
  the data retrieval time from different devices? How do we integrate the read
  time overhead into the metadata requests?, and
(5) How do we provide appropriate security while maintaining scalability,
  particularly with the distributed metadata stores?

In this project we will first 
%\begin{tightEnumerate}
%
%\item {\bf Milestone 1}: 
demonstrate a metadata service capable of serving both POSIX
clients and our clients, but without consideration for authentication,
authorization, or data migration.
Secondly we will 
%\item {\bf Milestone 2}: D
demonstrate a time bounded search approach for finding data
within the storage system to identify data and the current location.  This will
update the cached information in the metadata service to short-circuit future
requests for the data presuming that it does not migrate again soon.
Finally, we will 
%\item {\bf Milestone 3:} D
demonstrate admission control and show scalability under
both application load as well as storage system pressures, showing that we can
tolerate both loads and maintain quality of service.

%\end{tightEnumerate}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
