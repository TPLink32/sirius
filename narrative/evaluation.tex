\section{Evaluation}
\label{sec:evaluation}

Our guiding principles are the basis for the evaluation. These principles are
described in Table~\ref{table:eval}.  We will partition the problem along these
lines and evaluate how the knowledge centric system design we employ can
provide more predictable performance and data quality. 

%I dont think this table is right. Knowledge is our independent variable
%and predictability is our dependent variable
\begin{table}[ht]


\centering
\begin{tabular}{ | p{1.4in} | p{2in} | p{2.5in} | }
  \hline
  {\bf Principle/Metric}                   & {\bf User perspective metric}   & {\bf System perspective metric }\\ \hline
  {\bf Knowledge-centric system}           & More knowledge = faster access to higher quality data &  More knowledge = better resource utilization across the entire hierarchy \\ \hline
  {\bf Predictable Performance Accuracy}   & More predictability = better quality of data &  Accuracy of the time bounds across multiple layers of the storage hierarchy wrote/read in time constrained situations\\ \hline
\end{tabular}

\caption{Guiding Principles}
\label{table:eval}
\end{table}

In addition to benchmarking the various S2E2 methods and tools in isolation,
we will work with our science partners at Princeton University (J. Tromp),
PPPL (C. S. Chang), and U. C. Irvine (Z. Lin), along with the OLCF and SNL,
to evaluate the leading edge applications working with the storage system to
determine performance predictability and data quality.  Our evaluation 
will use the multiple storage layers available on systems at both SNL and the
OLCF.  For example, the sith cluster at the OLCF
(https://www.olcf.ornl.gov/computing-resources/sith/) has its own Lustre
storage system, SSDs on each node, and is connected to the center-wide Lustre
file system. We will perform tests in both the system level (on sith) and in
the user level (on titan), and evaluate our results using our application
knowledge.

The first part of our evaluation will cover the following characteristics: (1)
the accuracy of time bounds estimates for applications when data is on just one
level of the storage hierarchy as well as when the data chunks are split; (2)
data discovery speed when chunks are spread across the hierarchy informing how
many pieces data can be split into before it takes too long to access most of
the data; and (3) how well we can migrate data and update our server to
determine if data might get out-of-sync between data chunks.  We will need to
clearly understand how well this system works on current systems and we will
need to investigate this on the next generation OLCF system (Summitt) and the
SNL system (Trinity) which we will have early access along with all of the
applications we will be investigating.

The second part of our evaluation will cover data accuracy for data read when
there are time constraints due to users wanting to read in many time slices in
a given amount of time. For example, J. Tromp's visualization may contain
hundreds of time slices, but when we visualize this data, we would like very
fast access to the data ``first look.'' We will determine if the data will be
sufficiently representative of the overall data.  We will work with our
application partners to assess the visualization accuracy using different data
qualities including comparisons between a time-constrained test vs. a test
without any time constraints for data retrieval.

The third part of the evaluation will assess how well our data migration and
eviction policies work with user-knowledge. We will determine if the overall
system is able to use more of the storage hierarchy by understanding these
intentions and creating autonomic rules to self-optimize. Furthermore, we will
determine if users can give more accurate policies that lead to better system
resource utilization. We specifically will evaluate if more knowledge
translates into higher levels of system utilization.

The fourth part of the evaluation focuses on the additional user burden. One of
the risks in the proposed design is the unwillingness of users to add more
knowledge to the system. To better understand the user burden, we will attempt
to quantify the additional requirements to improve the automatic optimization.
We expect a substantial minimal burden to inform the automatic optimization.
Our evaluation will focus on the improvements gained through additional user
provided data and attempt to relate the pain/benefit trade-off. The initial
hypothesis from working with many LCF users suggests that once the initial
burden to facilitate the automatic optimization, the additional burden required
to increase benefits is small enough to encourage users to participate more
heavily for greater benefit.

The fifth part of the evalation will evaluate how well additional data
knowledge in the system will lead to more performance by placing the most
important pieces at the higher layers of the hierarchy. We will test different
knowledge levels and evaluate the system impact.  For example, if the user
doesn't want the data broken into priority chunks, then the data will be at
the mercy of what the system does when there is no knowledge. If the user then 
provides sufficient information to use ``generic'' rouitnes based on frequency
or accuracy, then that knowledge will be used to decompose the objects into
specially placed chunks. We will evaluate these no and minimal knowledge cases
against those that provide additional knowledge allowing application level data
binning (for C.S. Chang) or for level of features (for J. Tromp).

% How will we quantify the quality of data? We will look at errors after
% analytics.

% How well does the system utilize all of its resources. We feel the more
% knowledge means the better the system can utilize all of the resources.

%predictability ... are the errors bounded,

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../proposal"
%%% End:
