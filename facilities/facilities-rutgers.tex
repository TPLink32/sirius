\facilitiessection{Rutgers University}{RU}

\noindent The proposed research will be conducted at the {\bf Rutgers Discovery Informatics Institute  (RDI$^2$)} (see \url{rdi2.rutgers.edu}). RDI$^2$ was created in 2012 as New Jersey's Center for Advanced Computation, an institute with the overarching goal of establishing a comprehensive and internationally competitive Computational and Data-Enabled Science and Engineering (CDS\&E) effort at Rutgers University that will nurture the fundamental integration of research, education, and infrastructure. 

RDI$^2$ also hosts the Cloud and Autonomic Computing Center (CAC), a national research center that was established in January 2008 between Rutgers University, The University of Florida, and The University of Arizona.  The overall goal of the center is to combine resources from these universities, private companies, and federal government to make all kinds of computer systems and applications -- from humble desktop computers to complex air traffic control systems and scientific and engineering applications -- more reliable, more secure, and more efficient. 

The CAC center was funded by the Industry/University Cooperative Research Center program of the National Science Foundation, by its members from industry and government, and by university matching funds. CAC members are afforded access to leading edge developments in autonomic computing and to knowledge accumulated by academic researchers and other industry partners. Per NSF guidelines, industry and government contributions in the form of annual CAC memberships, coupled with baseline funds from NSF and university matching funds, directly support the Center's expenses for personnel, equipment, travel, and supplies. 

RDI$^2$ provides the Rutgers community with advanced computational resources as well as support, training and consulting services -- details can be found at \url{http://rdi2.rutgers.edu/}.

RDI$^2$ is home to Excalibur, Rutgers' IBM BlueGene/P HPC system installation.  Excalibur consists of two full racks of IBM BlueGene/P.  Each rack has 1024 quad-core processors, with each processor having 2 GB of dedicated RAM (2048 total processors, 8192 total cores, 4 TB total memory).  Excalibur is served by a 228 TB GPFS filesystem, with 1.2 TB of faster SSD storage.  Network interconnect to the storage subsystem is through multiple 10 Gigabit Ethernet links.  Excalibur is a University-wide resource.  All researchers and students at Rutgers can get access to Excalibur using a startup allocation; larger allocations are available via a proposal process. Jobs are scheduled round-the-clock so as to maximize utilization of the system.  

Other resources available at RDI$^2$/CAC include a 32 node Dell M610 blade cluster, consisting of two Dell M1000E Modular Blade Enclosures, necessary interconnect/management infrastructure, and a supervisory node. Each enclosure is maximally configured with sixteen blades, each blade having two Intel Xeon E5504 Nehalem family quad-core processors at 2.0 GHz, forming an eight core node. Each node has 24 GB RAM and 73 GB of local disk storage (10,000 RPM), twelve nodes have an additional 1 TB of local storage. The network infrastructure is comprised of an integrated 16-port Mellanox InfiniBand switch within each blade chassis, each switch linked to the switch in the other chassis. All blades have Mellanox Quad-Data-Rate (QDR) InfiniBand interface cards. There is also an integrated (redundant) 1 Gigabit Ethernet within each chassis, with two pairs of 10 Gigabit uplink capabilities in each chassis. In the aggregate, the cluster system consists of 32 nodes, 256 cores, 768 GB memory and ~14.5 TB disk capacity, with a 20 Gigabit InfiniBand network and two 1 Gigabit Ethernet networks. 

RDI$^2$ has recently deployed a NSF funded research instrument for ``Computational And data-enabled Platform for Energy efficiency Research (CAPER)''. 
This is an eight-node cluster based on Super Micro Computer's SYS-4027GR-TRT system, which is capable of housing concurrently, in one node: (i) eight general-purpose graphical processing units (GPGPU), or eight Intel many-integrated-core (MIC) coprocessors -- or any eight-card combination of the two;(ii) two Fusion-io IoDrive-2 non-volatile random access memory (NVRAM) storage cards, or two LSI 24-port disk controllers -- or any two-card combination of the two; (iii) a single-port fourteen-data-rate (FDR) InfiniBand networking card; and (iv) up to 48 hard disk drives (HDD), or solid-state drives (SSD). The initial configuration will tentatively feature servers two Intel Xeon Ivybridge E5-2650v2 (16 cores/32 hardware threads), 128GB of DRAM, 1TB of Flash-based NVRAM, 2TB of SSD and 4TB of hard disk, one Intel Xeon Phi 7120P, one NVIDIA K40, and Infiniband FDR and 10G Ethernet network connectivity. This hardware is unprecedented in its flexibility and adaptability.  The platform will be instrumented with both coarse- and fine-grained power metering. The focus of the research in this proposal is a perfect fit to this hardware platform, and vice-versa.  

RDI$^2$ has established a distributed computational infrastructure composed of clusters at each site that are linked together. Current facilities include multiple clusters, multiple compute servers and GPU resources. These systems are housed in a dedicated machine room that has been recently upgraded to increase cooling capabilities as well as UPS capabilities. 

The state-of-the-art RDI$^2$ machine room features proper refrigeration and necessary security. In order to optimize cooling, the machine room follows a variant of the (accepted best practice) host aisle/cold aisle cabinet layout. The cold air is pushed from ceiling to the front of the racks. The exhaust air is routed directly into computer room air conditioner (CRAC) units.

RDI$^2$ with support from the State of New Jersey (see  \url{http://nj.gov/governor/news/news/552013/approved/20130429i.html}) is working on establishing a balanced advanced research computational and data environment composed of large-scale high-end compute engines and clusters, as well as significant co-located storage with embedded analytics capabilities. The platform will provide researchers, students, industry and government across the state with access to ACI capabilities for research and instruction.

This project will be deployed in three phases. The first phase provides an Inbiniband-based high-performance computing cluster with the following configuration: 120 baseline compute nodes, 18 medium memory, 2 large memory nodes, 24 single NVidia GPU based nodes, 2 multi NVidia GPU nodes, 2 Intel Xeon Phi nodes, along with 2 head nodes, 3 login nodes and 2 service nodes. All the compute is supported with an Infiniband FDR compute fabric (2:1 oversubscribed) a Gigabit Ethernet management fabric, a 10 Gigabit network for connection to public networks. The baseline and medium memory nodes are built leveraging the Dell M1000e blade environment high density, integrated switching and ease of deployment. The nodes themselves are M630 half height blades with dual Intel Xeon E5-2680v3 2.5GHz, 12-Core, 120W processors, 256G of 2133 MHz RAM (10.7GB RAM per core), single 300G 10K hard drive, dual 10 Gigabit Ethernet ports and a Mellanox FDR Infiniband adapter. The rest of nodes are in a rack-based configuration with the same basic configuration and specialized capabilities (i.e., NVidia GPU, Intel Xeon Phi or large memory size). The first phase also includes a scalable high-performance parallel file system based on GPFS (General Parallel File System), which provides 1PB of usable storage capacity.

The second phase is focused on a self-contained, flexible modular data center, connected with high-speed networking. The modular data center will be able to house up to 17 racks and will provide at least 350KW of electrical power for computer systems.

The third phase will provide a supercomputing system, which will deliver a peak performance of hundreds of Teraflops, making this system one of the top academic computing systems in the nation. This system will be featured with tens of thousands of CPU cores, a low-latency high-bandwidth network and innovative non-volatile memory solutions.
